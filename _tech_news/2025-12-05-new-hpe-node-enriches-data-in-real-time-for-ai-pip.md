---
author: Marisa Aigen
category: ai hardware
companies:
- HPE
date: '2025-12-05 17:58:00'
description: Na konferenci HPE Discover Barcelona 2025 spoleƒçnost HPE p≈ôedstavila
  uzly datov√© inteligence Alletra Storage MP X10000, kter√© obohacuj√≠, organizuj√≠ a
  p≈ôipravuj√≠ data pro pokroƒçilou anal√Ωzu a aplikace strojov√©ho uƒçen√≠.
importance: 3
layout: tech_news_article
original_title: New HPE node enriches data in real time for AI pipelines
publishedAt: '2025-12-05T17:58:00+00:00'
slug: new-hpe-node-enriches-data-in-real-time-for-ai-pip
source:
  emoji: üì∞
  id: null
  name: Techtarget.com
title: Nov√Ω uzel HPE obohacuje data v re√°ln√©m ƒçase pro AI pipeline
url: https://www.techtarget.com/searchstorage/news/366636052/New-HPE-node-enriches-data-in-real-time-for-AI-pipelines
urlToImage: https://www.techtarget.com/visuals/German/article/artificial-intelligence-brain-2-adobe.jpg
urlToImageBackup: https://www.techtarget.com/visuals/German/article/artificial-intelligence-brain-2-adobe.jpg
---

## Souhrn
Spoleƒçnost Hewlett Packard Enterprise (HPE) na konferenci HPE Discover Barcelona 2025 ozn√°mila uzly datov√© inteligence HPE Alletra Storage MP X10000. Tyto uzly, poh√°nƒõn√© grafikami Nvidia L40S GPU, slou≈æ√≠ k obohacen√≠ metadat a vektorov√Ωm embedding≈Øm p≈ô√≠mo v √∫lo≈æi≈°ti, co≈æ urychluje p≈ô√≠pravu dat pro AI aplikace jako RAG, velk√© jazykov√© modely (LLM) a anal√Ωzu. ≈òe≈°√≠ tak hlavn√≠ p≈ôek√°≈æku on-premises AI iniciativ, kterou je p≈ô√≠prava a p≈ôesun dat k v√Ωpoƒçetn√≠m zdroj≈Øm.

## Kl√≠ƒçov√© body
- Uzly Alletra Storage MP X10000 funguj√≠ jako vrstva mezi √∫lo≈æi≈°tƒõm a v√Ωpoƒçty, automaticky extrahuj√≠ a obohacuj√≠ metadata z objekt≈Ø X10000.
- Pou≈æ√≠vaj√≠ Nvidia L40S GPU pro inline zpracov√°n√≠ dat v re√°ln√©m ƒçase, co≈æ zvy≈°uje rychlost, kvalitu a relevanci dat pro AI.
- Podle zpr√°vy Omdia jsou hlavn√≠ v√Ωzvy identifikace datov√Ωch sad, jejich p≈ô√≠prava a p≈ôesun na spr√°vnou infrastrukturu.
- Eliminuj√≠ pot≈ôebu samostatn√Ωch n√°stroj≈Ø pro p≈ô√≠pravu dat, co≈æ zkracuje ƒças na nasazen√≠ AI model≈Ø.
- Podpora pro RAG (Retrieval-Augmented Generation), LLM a anal√Ωzu, vƒçetnƒõ ukl√°d√°n√≠ vektorov√Ωch embedding≈Ø.

## Podrobnosti
Hewlett Packard Enterprise, kter√° se specializuje na enterprise √∫lo≈æi≈°tƒõ, servery a hybridn√≠ cloudov√° ≈ôe≈°en√≠, p≈ôedstavila uzly datov√© inteligence jako odpovƒõƒè na rostouc√≠ pot≈ôeby firem s AI projekty. Podle pr≈Øzkumu Omdia, kter√Ω analyzuje trh s IT infrastrukturou, ƒçel√≠ podniky p≈ôi on-premises AI iniciativ√°ch t≈ôem kl√≠ƒçov√Ωm probl√©m≈Øm: nalezen√≠ vhodn√Ωch datov√Ωch sad, podpora t√Ωm≈Ø p≈ôi jejich p≈ô√≠pravƒõ a p≈ôesun dat na GPU clustery. Scott Sinclair, ≈ôeditel praxe v Omdia, zd≈Øraznil, ≈æe kvalita dat je kl√≠ƒçov√° pro √∫spƒõch AI, ale ≈°k√°la modern√≠ch datov√Ωch prost≈ôed√≠ a jejich distribuce komplikuj√≠ lokalizaci relevantn√≠ch zdroj≈Ø. Bez ≈ô√°dn√© p≈ô√≠pravy kles√° v√Ωkon GPU syst√©m≈Ø, co≈æ vede k neefektivn√≠mu vyu≈æit√≠ drah√© hardware.

Uzly HPE Alletra Storage MP X10000 ≈ôe≈°√≠ tento bottleneck vlo≈æen√≠m vrstvy zpracov√°n√≠ p≈ô√≠mo do √∫lo≈æi≈°tƒõ. Integrovan√Ω inline engine pro obohacen√≠ metadat automaticky extrahuje informace z objekt≈Ø ulo≈æen√Ωch v syst√©mu X10000, generuje vektorov√© embeddingy a ukl√°d√° je pro okam≈æit√© pou≈æit√≠. Nvidia L40S GPU zaji≈°≈•uje vysok√Ω v√Ωkon pro tyto operace, p≈ôibli≈æuje data k v√Ωpoƒçt≈Øm a sni≈æuje latenci. Fidelma Russo, v√Ωkonn√° viceprezidentka pro hybridn√≠ cloud a CTO v HPE, uvedla, ≈æe bottleneck AI nen√≠ kapacita GPU, ale p≈ô√≠prava dat pro nƒõ. Tento p≈ô√≠stup umo≈æ≈àuje firm√°m integrovat data p≈ô√≠mo do AI workflow bez nutnosti v√≠ce n√°stroj≈Ø, jako jsou separ√°tn√≠ ETL (Extract, Transform, Load) syst√©my nebo manu√°ln√≠ skripty.

Pro praktick√© pou≈æit√≠ slou≈æ√≠ uzly k p≈ô√≠pravƒõ dat pro RAG, kde se relevantn√≠ dokumenty vyhled√°vaj√≠ a kombinuj√≠ s LLM pro p≈ôesnƒõj≈°√≠ odpovƒõdi; pro tr√©nink LLM, kde kvalitn√≠ embeddingy zlep≈°uj√≠ p≈ôesnost model≈Ø; a pro anal√Ωzu, kde rychl√© metadata umo≈æ≈àuj√≠ explorativn√≠ dotazy. HPE tak posiluje svou pozici v ekosyst√©mu Nvidia, kde se GPU st√°vaj√≠ standardem pro AI akceleraci.

## Proƒç je to d≈Øle≈æit√©
Tato novinka p≈ôisp√≠v√° k demokratizaci on-premises AI t√≠m, ≈æe sni≈æuje z√°vislost na cloudov√Ωch slu≈æb√°ch pro datovou p≈ô√≠pravu a umo≈æ≈àuje firm√°m s citliv√Ωmi daty zpracov√°vat je lok√°lnƒõ. V ≈°ir≈°√≠m kontextu enterprise AI, kde podle Omdia 70 % firem pl√°nuje hybridn√≠ nasazen√≠, ≈ôe≈°√≠ kl√≠ƒçov√Ω bod selh√°n√≠ ‚Äì data pipeline. Pro pr≈Ømysl znamen√° m√©nƒõ slo≈æitost√≠ v infrastruktu≈ôe, rychlej≈°√≠ ƒças na hodnotu z AI investic a lep≈°√≠ vyu≈æit√≠ existuj√≠c√≠ch √∫lo≈æi≈°≈• HPE. Kriticky lze poznamenat, ≈æe zat√≠mco hardware ≈ôe≈°√≠ ≈°k√°lovatelnost, √∫spƒõch z√°vis√≠ na integraci s otev≈ôen√Ωmi frameworky jako LangChain nebo Hugging Face, co≈æ HPE zat√≠m podrobnƒõ nespecifikovalo. Celkovƒõ posiluje trend smƒõ≈ôuj√≠c√≠ k edge AI zpracov√°n√≠ v enterprise prost≈ôed√≠ch.

---

[ƒå√≠st p≈Øvodn√≠ ƒçl√°nek](https://www.techtarget.com/searchstorage/news/366636052/New-HPE-node-enriches-data-in-real-time-for-AI-pipelines)

**Zdroj:** üì∞ Techtarget.com
