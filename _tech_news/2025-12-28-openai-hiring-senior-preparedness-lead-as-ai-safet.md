---
author: Marisa Aigen
category: bezpeÄnost ai
companies:
- OpenAI
date: '2025-12-28 23:33:00'
description: OpenAI Group PBC nabÃ­zÃ­ pozici vedoucÃ­ho pÅ™Ã­pravy, seniornÃ­ role zamÄ›Å™enou
  na predikci potenciÃ¡lnÃ­ch Å¡kod z umÄ›lÃ½ch inteligencÃ­ modelÅ¯ a navrhovÃ¡nÃ­ opatÅ™enÃ­
  k jejich zmÃ­rnÄ›nÃ­ pÅ™i pokraÄujÃ­cÃ­m vÃ½voji schopnostÃ­. Role vede technickou strategii
  a provÃ¡dÄ›nÃ­ Preparedness Framework pro sledovÃ¡nÃ­ hraniÄnÃ­ch schopnostÃ­.
importance: 4
layout: tech_news_article
original_title: OpenAI hiring senior preparedness lead as AI safety scrutiny grows
publishedAt: '2025-12-28T23:33:00+00:00'
slug: openai-hiring-senior-preparedness-lead-as-ai-safet
source:
  emoji: ğŸ“°
  id: null
  name: SiliconANGLE News
title: OpenAI hledÃ¡ seniornÃ­ho vedoucÃ­ho pÅ™Ã­pravy na rizika, jak roste dohled nad
  bezpeÄnostÃ­ AI
url: https://siliconangle.com/2025/12/28/openai-hiring-senior-preparedness-lead-ai-safety-scrutiny-grows/
urlToImage: https://d15shllkswkct0.cloudfront.net/wp-content/blogs.dir/1/files/2025/12/openaiheadofprepardness.png
urlToImageBackup: https://d15shllkswkct0.cloudfront.net/wp-content/blogs.dir/1/files/2025/12/openaiheadofprepardness.png
---

## Souhrn
OpenAI Group PBC vyhlÃ¡silo vÃ½bÄ›r na pozici vedoucÃ­ho pÅ™Ã­pravy (head of preparedness), coÅ¾ je seniornÃ­ bezpeÄnostnÃ­ role odpovÄ›dnÃ¡ za predikci rizik z pokroÄilÃ½ch AI modelÅ¯ a koordinaci jejich mitigace. Tato pozice vede Preparedness Framework, rÃ¡mec pro hodnocenÃ­ hraniÄnÃ­ch schopnostÃ­, kterÃ© mohou vÃ©st k novÃ½m nebo zÃ¡vaÅ¾nÃ½m rizikÅ¯m, jako je zneuÅ¾itÃ­ nebo kybernetickÃ© hrozby. NabÃ­dka pÅ™ichÃ¡zÃ­ po turbulentnÃ­ch zmÄ›nÃ¡ch v bezpeÄnostnÃ­m leadershipu spoleÄnosti.

## KlÃ­ÄovÃ© body
- Role vede technickou strategii Preparedness Framework, vÄetnÄ› vÃ½voje modelÅ¯ hrozeb, hodnocenÃ­ schopnostÃ­ a nastavovÃ¡nÃ­ prahÅ¯ rizik.
- Monitoruje rizika jako zneuÅ¾itÃ­ modelÅ¯ (misuse scenarios), kybernetickÃ© Ãºtoky a dalÅ¡Ã­ dopady rostoucÃ­ch schopnostÃ­ AI.
- Pozice spadÃ¡ pod Safety Systems a spolupracuje s vÃ½zkumnÃ½mi, politickÃ½mi a produktovÃ½mi tÃ½my, ovlivÅˆuje rozhodnutÃ­ o vydÃ¡vÃ¡nÃ­ modelÅ¯.
- ZÃ¡kladnÃ­ plat 550 000 USD plus podÃ­ly, vyÅ¾aduje zkuÅ¡enosti s velkÃ½mi technickÃ½mi systÃ©my, bezpeÄnostÃ­ nebo rizikovou analÃ½zou.
- Nahrazuje doÄasnÃ© Å™eÅ¡enÃ­ po odchodÃ­ch Aleksandra Madryho, Lilian Weng a pÅ™esunu JoaquÃ­na QuiÃ±onera Candely.

## Podrobnosti
OpenAI, spoleÄnost zamÄ›Å™enÃ¡ na vÃ½voj pokroÄilÃ½ch modelÅ¯ umÄ›lÃ© inteligence jako GPT sÃ©rie, nynÃ­ hledÃ¡ stÃ¡lÃ©ho vedoucÃ­ho pÅ™Ã­pravy v rÃ¡mci svÃ© Safety Systems organizace. Preparedness Framework slouÅ¾Ã­ k systematickÃ©mu sledovÃ¡nÃ­ a hodnocenÃ­ tzv. frontier capabilities â€“ hraniÄnÃ­ch schopnostÃ­ modelÅ¯, kterÃ© pÅ™ekraÄujÃ­ souÄasnÃ© limity a mohou pÅ™inÃ©st novÃ¡ rizika. Mezi tyto rizika patÅ™Ã­ napÅ™Ã­klad zneuÅ¾itÃ­ modelÅ¯ k tvorbÄ› Å¡kodlivÃ©ho obsahu, jako dezinformace nebo biologickÃ© zbranÄ›, kybernetickÃ© hrozby prostÅ™ednictvÃ­m automatizovanÃ½ch ÃºtokÅ¯ Äi neoÄekÃ¡vanÃ© dopady na spoleÄnost pÅ™i rostoucÃ­ autonomii systÃ©mÅ¯.

VedoucÃ­ role bude vyvÃ­jet threat models, coÅ¾ jsou modely predikujÃ­cÃ­ moÅ¾nÃ© Ãºtoky nebo selhÃ¡nÃ­, provÃ¡dÄ›t capability evaluations â€“ testy, kterÃ© mÄ›Å™Ã­, zda model dosahuje nebezpeÄnÃ½ch ÃºrovnÃ­ vÃ½konu â€“ a nastavovat risk thresholds, prahovÃ© hodnoty, pÅ™i jejichÅ¾ pÅ™ekroÄenÃ­ se aktivujÃ­ dalÅ¡Ã­ bezpeÄnostnÃ­ opatÅ™enÃ­, jako omezenÃ­ nasazenÃ­ nebo dodateÄnÃ© kontroly. Tyto vÃ½stupy pÅ™Ã­mo ovlivÅˆujÃ­ rozhodnutÃ­ o vydÃ¡vÃ¡nÃ­ novÃ½ch modelÅ¯ a funkcÃ­, coÅ¾ je klÃ­ÄovÃ© v dobÄ›, kdy regulaÄnÃ­ orgÃ¡ny jako evropskÃ¡ AI Act nebo americkÃ© federÃ¡lnÃ­ agentury zesilujÃ­ dohled.

Tato nabÃ­dka reflektuje nedÃ¡vnÃ© zmÄ›ny v bezpeÄnostnÃ­m tÃ½mu OpenAI. V polovinÄ› roku 2024 byl bÃ½valÃ½ head of preparedness Aleksander Madry, expert na robustnÃ­ AI, pÅ™eÅ™azen na jinou pozici. DoÄasnÄ› pÅ™evzali odpovÄ›dnost seniornÃ­ manaÅ¾eÅ™i Joaquin QuiÃ±onero Candela a Lilian Weng, specialistka na bezpeÄnostnÃ­ systÃ©my. Weng pozdÄ›ji odeÅ¡la a QuiÃ±onero Candela pÅ™eÅ¡el do nÃ¡boru personÃ¡lu, coÅ¾ zanechalo oblast bez dedikovanÃ©ho lÃ­dra. CEO Sam Altman dÅ™Ã­ve zdÅ¯raznil pÅ™Ã­pravu na rizika jako klÃ­ÄovÃ½ pilÃ­Å™, avÅ¡ak opakovanÃ© zmÄ›ny v tÃ½mu vyvolÃ¡vajÃ­ otÃ¡zky o stabilitÄ› zÃ¡vazkÅ¯ k bezpeÄnosti, zvlÃ¡Å¡tÄ› po sporech o superalignment tÃ½m v roce 2024.

PoÅ¾adavky na kandidÃ¡ta jsou vysokÃ©: zkuÅ¡enosti s velkÃ½mi systÃ©my, bezpeÄnostÃ­, rizikovou analÃ½zou nebo governance, schopnost pÅ™evÃ¡dÄ›t vÃ½zkum do provoznÃ­ch kontrol. NabÃ­dka s platem 550 000 USD plus equity signalizuje prioritu, ale kritici poukazujÃ­, Å¾e vysokÃ© odmÄ›ny mohou pÅ™itahovat talenty spÃ­Å¡e z korporÃ¡tnÃ­ho sektoru neÅ¾ z akademickÃ© sfÃ©ry bezpeÄnosti AI.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tento krok nastupuje v dobÄ› rostoucÃ­ho tlaku na AI firmy kvÅ¯li potenciÃ¡lnÃ­m rizikÅ¯m superinteligentnÃ­ch systÃ©mÅ¯. OpenAI jako lÃ­dr v LLM modelech ovlivÅˆuje celÃ½ prÅ¯mysl â€“ jejich Preparedness Framework slouÅ¾Ã­ jako benchmark pro ostatnÃ­, napÅ™Ã­klad Anthropic nebo Google DeepMind. StabilnÃ­ leadership v tÃ©to oblasti mÅ¯Å¾e urychlit vÃ½voj bezpeÄnostnÃ­ch standardÅ¯, ovlivnit regulaÄnÃ­ debaty a zabrÃ¡nit incidentÅ¯m, jako byly minulÃ© Ãºniky dat nebo jailbreaky modelÅ¯. Pro uÅ¾ivatele a firmy znamenÃ¡ lepÅ¡Ã­ predikce rizik bezpeÄnÄ›jÅ¡Ã­ nasazenÃ­ AI v produkci, zatÃ­mco absence takovÃ© role by mohla vÃ©st k opoÅ¾dÄ›nÃ½m releasÃ­m nebo eskalaci veÅ™ejnÃ©ho skepticizmu. V Å¡irÅ¡Ã­m kontextu podtrhuje to nutnost integrace bezpeÄnosti do jÃ¡dra vÃ½voje AI, coÅ¾ je podmÃ­nkou udrÅ¾itelnÃ©ho pokroku smÄ›rem k AGI.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://siliconangle.com/2025/12/28/openai-hiring-senior-preparedness-lead-ai-safety-scrutiny-grows/)

**Zdroj:** ğŸ“° SiliconANGLE News
