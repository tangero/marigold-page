---
author: Marisa Aigen
category: regulace ai
date: '2025-12-31 17:37:16'
description: Bez federÃ¡lnÃ­ch pravidel stÃ¡tnÃ­ zÃ¡konodÃ¡rci v Kalifornii pÅ™ebÃ­rajÃ­ iniciativu
  a regulujÃ­ bezpeÄnost AI sami.
importance: 4
layout: tech_news_article
original_title: Can one state save us from AI disaster? Inside California's new legislative
  crackdown
publishedAt: '2025-12-31T17:37:16+00:00'
slug: can-one-state-save-us-from-ai-disaster-inside-cali
source:
  emoji: ğŸ“°
  id: null
  name: ZDNet
title: MÅ¯Å¾e jeden stÃ¡t nÃ¡s zachrÃ¡nit pÅ™ed katastrofou AI? PozitivnÃ­ pohled na novou
  kalifornskou legislativnÃ­ regulaci
url: https://www.zdnet.com/article/california-ai-law-crackdown-on-risks/
urlToImage: https://www.zdnet.com/a/img/resize/f5b6b184d94bead8ea72118f655176013a89f896/2025/12/31/35fe502b-9f26-4b7a-9e25-8e2865d08638/gettyimages-186704207.jpg?auto=webp&fit=crop&height=675&width=1200
urlToImageBackup: https://www.zdnet.com/a/img/resize/f5b6b184d94bead8ea72118f655176013a89f896/2025/12/31/35fe502b-9f26-4b7a-9e25-8e2865d08638/gettyimages-186704207.jpg?auto=webp&fit=crop&height=675&width=1200
---

### Souhrn
NovÃ½ kalifornskÃ½ zÃ¡kon o bezpeÄnosti AI vstupuje v platnost 1. ledna a zavÃ¡dÃ­ povinnou transparentnost pro vÃ½vojÃ¡Å™e pokroÄilÃ½ch modelÅ¯ AI. Firmy musÃ­ zveÅ™ejÅˆovat plÃ¡ny na Å™eÅ¡enÃ­ katastrofickÃ½ch rizik a hlÃ¡sit kritickÃ© bezpeÄnostnÃ­ incidenty stÃ¡tnÃ­m ÃºÅ™adÅ¯m do 15 dnÅ¯. ZÃ¡kon navÃ­c chrÃ¡nÃ­ whistleblowery v tomto odvÄ›tvÃ­.

### KlÃ­ÄovÃ© body
- Povinnost zveÅ™ejnit na webu plÃ¡ny a politiky pro reakci na "katastrofickÃ© riziko", jako je smrt vÃ­ce neÅ¾ 50 lidÃ­ nebo Å¡kody pÅ™es 1 miliardu dolarÅ¯.
- HlÃ¡Å¡enÃ­ "kritickÃ½ch bezpeÄnostnÃ­ch incidentÅ¯" do 15 dnÅ¯ pod hrozbou pokut aÅ¾ 1 milion dolarÅ¯ za poruÅ¡enÃ­.
- Ochrana zamÄ›stnancÅ¯-whistleblowerÅ¯ v firmÃ¡ch vyvÃ­jejÃ­cÃ­ch AI modely.
- ZÃ¡kon navrhl demokratickÃ½ senÃ¡tor Scott Wiener a zamÄ›Å™uje se na frontier AI modely, tedy ty nejpokroÄilejÅ¡Ã­ systÃ©my.
- Definice katastrofickÃ©ho rizika zahrnuje napÅ™Ã­klad instrukce k vÃ½robÄ› chemickÃ½ch, biologickÃ½ch nebo jadernÃ½ch zbranÃ­.

### Podrobnosti
Kalifornie, jako centrum technologickÃ©ho prÅ¯myslu s firmami jako OpenAI, Google DeepMind nebo Anthropic, se stÃ¡vÃ¡ prvnÃ­m stÃ¡tem USA, kterÃ½ zavÃ¡dÃ­ specifickou regulaci pro bezpeÄnost AI v dobÄ› absence federÃ¡lnÃ­ch norem. ZÃ¡kon, oznaÄovanÃ½ jako SB 1047, vyÅ¾aduje od spoleÄnostÃ­ vyvÃ­jejÃ­cÃ­ch frontier AI modely â€“ tedy systÃ©my s vysokÃ½mi vÃ½poÄetnÃ­mi nÃ¡roky a potenciÃ¡lnÄ› neÄekanÃ½mi schopnostmi â€“ aby na svÃ½ch webech uveÅ™ejnily podrobnÃ© informace o svÃ½ch bezpeÄnostnÃ­ch postupech. Tyto plÃ¡ny musÃ­ pokrÃ½vat scÃ©nÃ¡Å™e, kde AI mÅ¯Å¾e zpÅ¯sobit masivnÃ­ Å¡kody, aÅ¥ uÅ¾ zÃ¡mÄ›rnÄ› zneuÅ¾itÃ­m (malicious use) nebo chybou systÃ©mu (malfunction).

NapÅ™Ã­klad pokud model poskytne pokyny k vÃ½voji zbranÃ­ hromadnÃ©ho niÄenÃ­ nebo umoÅ¾nÃ­ kybernetickÃ½ Ãºtok s obrovskÃ½mi Å¡kodami, firma musÃ­ incident nahlÃ¡sit kalifornskÃ©mu generÃ¡lnÃ­mu prokurÃ¡torovi. Pokuty za nedodrÅ¾enÃ­ dosahujÃ­ aÅ¾ 1 milionu dolarÅ¯ za kaÅ¾dÃ½ pÅ™Ã­pad, coÅ¾ mÃ¡ motivovat dodrÅ¾ovÃ¡nÃ­. ZÃ¡kon rovnÄ›Å¾ zavÃ¡dÃ­ prÃ¡vnÃ­ ochranu pro zamÄ›stnance, kteÅ™Ã­ odhalÃ­ bezpeÄnostnÃ­ nedostatky; chrÃ¡nÃ­ je pÅ™ed odvetou zamÄ›stnavatele, coÅ¾ je klÃ­ÄovÃ© v odvÄ›tvÃ­, kde loajalita vÅ¯Äi firmÄ› Äasto pÅ™evaÅ¾uje nad veÅ™ejnÃ½m zÃ¡jmem.

Tento krok pÅ™ichÃ¡zÃ­ v dobÄ›, kdy experti na AI bezpeÄnost, jako ti z Center for AI Safety, varujÃ­ pÅ™ed rychlou evolucÃ­ technologiÃ­. Modely jako GPT-4o nebo Claude 3.5 vykazujÃ­ schopnosti komplexnÃ­ho uvaÅ¾ovÃ¡nÃ­, kterÃ© mohou vÃ©st k neoÄekÃ¡vanÃ½m rizikÅ¯m, napÅ™Ã­klad v generovÃ¡nÃ­ Å¡kodlivÃ©ho kÃ³du nebo dezinformacÃ­ ve velkÃ©m mÄ›Å™Ã­tku. ZÃ¡kon nenÃ­ zamÄ›Å™enÃ½ na bÄ›Å¾nÃ© AI nÃ¡stroje jako chatboti pro zÃ¡kaznickou podporu, ale vÃ½hradnÄ› na ty nejpokroÄilejÅ¡Ã­, kterÃ© spotÅ™ebovÃ¡vajÃ­ obrovskÃ© zdroje GPU a trÃ©nujÃ­ na datech v Å™Ã¡dech bilionÅ¯ tokenÅ¯. Pro prÅ¯mysl to znamenÃ¡ nutnost revidovat internÃ­ procesy: firmy budou muset implementovat systÃ©my pro detekci rizik, jako red teaming (simulace ÃºtokÅ¯) nebo evaluace na bezpeÄnostnÃ­ benchmarky typu MLCommons.

### ProÄ je to dÅ¯leÅ¾itÃ©
Tato regulace nastavuje precedens v USA, kde federÃ¡lnÃ­ legislativa zaostÃ¡vÃ¡ â€“ napÅ™Ã­klad Bidenova vÃ½konnÃ¡ smÄ›rnice z roku 2023 je dobrovolnÃ¡. Kalifornie ovlivnÃ­ celÃ½ ekosystÃ©m, protoÅ¾e mnoho firem sÃ­dlÃ­ prÃ¡vÄ› zde a zÃ¡kon se vztahuje na globÃ¡lnÃ­ modely distribuovanÃ© v USA. Pro uÅ¾ivatele to znamenÃ¡ vÄ›tÅ¡Ã­ dÅ¯vÄ›ru v AI systÃ©my, mÃ©nÄ› rizik z neÅ™Ã­zenÃ©ho vÃ½voje a tlak na etickÃ© standardy. Kriticky Å™eÄeno, zÃ¡kon nenÃ­ dokonalÃ½: postihuje aÅ¾ po incidentu a neÅ™eÅ¡Ã­ prevenci na Ãºrovni trÃ©ninkovÃ½ch dat nebo architektury modelÅ¯. PÅ™esto posiluje odpovÄ›dnost velkÃ½ch hrÃ¡ÄÅ¯ a mÅ¯Å¾e inspirovat Evropu (AI Act) nebo ÄŒÃ­nu k podobnÃ½m krokÅ¯m, ÄÃ­mÅ¾ pÅ™ispÃ­vÃ¡ k globÃ¡lnÃ­ harmonizaci bezpeÄnostnÃ­ch norem v Ã©Å™e, kdy AI pÅ™ekraÄuje hranice stÃ¡tÅ¯.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.zdnet.com/article/california-ai-law-crackdown-on-risks/)

**Zdroj:** ğŸ“° ZDNet
