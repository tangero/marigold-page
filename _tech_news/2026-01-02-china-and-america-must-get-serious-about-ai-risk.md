---
author: Marisa Aigen
category: ai rizika
date: '2026-01-02 10:00:36'
description: ZÃ¡vod o vedenÃ­ v vojenskÃ©, zpravodajskÃ© a komerÄnÃ­ AI â€“ a o globÃ¡lnÃ­
  adopci americkÃ½ch nebo ÄÃ­nskÃ½ch modelÅ¯ â€“ se jen zintenzivnÃ­. Jako nÃ¡rodnÃ­ bezpeÄnostnÃ­
  poradce jsem pracoval na pÅ™Ã­pravÄ› USA na kaÅ¾dÃ½ scÃ©nÃ¡Å™ v nejistÃ©m spektru rizik.
importance: 4
layout: tech_news_article
original_title: China and America must get serious about AI risk
publishedAt: '2026-01-02T10:00:36+00:00'
slug: china-and-america-must-get-serious-about-ai-risk
source:
  emoji: ğŸ“°
  id: null
  name: Livemint
title: ÄŒÃ­na a Amerika se musÃ­ vÃ¡Å¾nÄ› zabÃ½vat riziky AI
url: https://www.livemint.com/opinion/columns/us-china-ai-risks-national-security-diplomacy-technology-global-stability-11767179324869.html
urlToImage: https://www.livemint.com/lm-img/img/2025/12/31/1600x900/logo/China_US_PS_AFP_1767179428162_1767179438515.jpg
urlToImageBackup: https://www.livemint.com/lm-img/img/2025/12/31/1600x900/logo/China_US_PS_AFP_1767179428162_1767179438515.jpg
---

### Souhrn
Jake Sullivan, bÃ½valÃ½ nÃ¡rodnÃ­ bezpeÄnostnÃ­ poradce USA, vyzÃ½vÃ¡ ÄŒÃ­nu a SpojenÃ© stÃ¡ty k rozvoji trvalÃ© diplomacie na vysokÃ© Ãºrovni ohlednÄ› rizik umÄ›lÃ© inteligence. Navazuje na spoleÄnÃ© prohlÃ¡Å¡enÃ­ prezidentÅ¯ Bidena a Si Å¤in-pchinga z listopadu 2024, kterÃ© zdÅ¯razÅˆuje nutnost udrÅ¾et lidskou kontrolu nad rozhodovÃ¡nÃ­m o pouÅ¾itÃ­ jadernÃ½ch zbranÃ­. Tento krok ukazuje, Å¾e i pÅ™i intenzivnÃ­ soutÄ›Å¾i o globÃ¡lnÃ­ vedenÃ­ v AI lze dosÃ¡hnout pokroku v Å™Ã­zenÃ­ rizik.

### KlÃ­ÄovÃ© body
- SpoleÄnÃ© prohlÃ¡Å¡enÃ­ USA a ÄŒÃ­ny z listopadu 2024 o lidskÃ© kontrole nad jadernÃ½mi zbranÄ›mi v Ã©Å™e AI.
- Rok vyjednÃ¡vÃ¡nÃ­ pÅ™ekonal skepticismus ÄÃ­nskÃ© strany vÅ¯Äi americkÃ½m nÃ¡vrhÅ¯m na snÃ­Å¾enÃ­ rizik.
- SetkÃ¡nÃ­ diplomatÅ¯ a expertÅ¯ v Å½enevÄ› na zaÄÃ¡tku roku 2024.
- VÃ½zva k pokraÄujÃ­cÃ­mu dialogu na Ãºrovni vrcholovÃ½ch pÅ™edstavitelÅ¯ pÅ™i soubÄ›Å¾nÃ© soutÄ›Å¾i o AI vedenÃ­.
- PotenciÃ¡l pro dalÅ¡Ã­ kroky v Å™Ã­zenÃ­ nÃ¡rodnÄ› bezpeÄnostnÃ­ch rizik spojenÃ½ch s AI.

### Podrobnosti
ÄŒlÃ¡nek vychÃ¡zÃ­ z autorovy zkuÅ¡enosti jako nÃ¡rodnÃ­ho bezpeÄnostnÃ­ho poradce, kde koordinoval pÅ™Ã­pravu USA na Å¡irokÃ© spektrum hrozeb vÄetnÄ› tÄ›ch souvisejÃ­cÃ­ch s rychlÃ½m vÃ½vojem AI v civilnÃ­ch i vojenskÃ½ch aplikacÃ­ch. KlÃ­ÄovÃ½m milnÃ­kem je prohlÃ¡Å¡enÃ­ z listopadu 2024, kdy Biden a Si Å¤in-pching poprvÃ© spoleÄnÄ› uznaly potÅ™ebu zachovat lidskou kontrolu nad jadernÃ½mi zbranÄ›mi. Tento bod, i kdyÅ¾ zdÃ¡nlivÄ› samozÅ™ejmÃ½, vyÅ¾adoval vÃ­ce neÅ¾ rok intenzivnÃ­ch jednÃ¡nÃ­. ÄŒÃ­nskÃ¡ strana je tradiÄnÄ› skeptickÃ¡ vÅ¯Äi nÃ¡vrhÅ¯m USA na snÃ­Å¾enÃ­ rizik, zejmÃ©na kdyÅ¾ Rusko podobnÃ© formulace blokovalo v multilaterÃ¡lnÃ­ch fÃ³rech. ÃšspÄ›ch bilaterÃ¡lnÃ­ch rozhovorÅ¯ tak vytvÃ¡Å™Ã­ trhlinu mezi PekÃ­ngem a Moskvou, coÅ¾ posiluje pozici USA.

DÅ™Ã­ve v roce 2024 se v Å½enevÄ› konala prodlouÅ¾enÃ¡ setkÃ¡nÃ­ expertÅ¯ obou zemÃ­, kterÃ¡ poloÅ¾ila zÃ¡klady pro tento dialog. Sullivan zdÅ¯razÅˆuje, Å¾e rychlost vÃ½voje AI â€“ od velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) jako GPT nebo ÄÃ­nskÃ½ch ekvivalentÅ¯ po autonomnÃ­ systÃ©my pro zpravodajstvÃ­ a vojenskÃ© operace â€“ zvyÅ¡uje rizika nechtÄ›nÃ© eskalace. NapÅ™Ã­klad AI by mohla ovlivnit rozhodovÃ¡nÃ­ v kritickÃ½ch situacÃ­ch, jako je detekce hrozeb nebo cÃ­lenÃ­ zbranÃ­, coÅ¾ by mohlo vÃ©st k chybÃ¡m s katastrofÃ¡lnÃ­mi nÃ¡sledky. Pro ÄŒÃ­nu i USA jde o strategickou soutÄ›Å¾: kdo ovlÃ¡dne globÃ¡lnÃ­ standardy AI modelÅ¯, ten ovlivnÃ­ ekonomiku, bezpeÄnost i geopolitiku. Sullivan proto navrhuje udrÅ¾et dialog i pÅ™i maximalizaci nÃ¡rodnÃ­ch vÃ½hod, podobnÄ› jako v jinÃ½ch oblastech zbrojenÃ­.

Tento pÅ™Ã­stup kontrastuje s absencÃ­ multilaterÃ¡lnÃ­ch dohod, kde ÄŒÃ­na a Rusko Äasto kooperujÃ­ proti ZÃ¡padu. Pro prÅ¯mysl znamenÃ¡, Å¾e firmy jako OpenAI, Google DeepMind nebo ÄÃ­nskÃ© Baidu a Tencent musÃ­ poÄÃ­tat s rostoucÃ­ regulacÃ­, kterÃ¡ by mohla omezit export technologiÃ­ nebo sdÃ­lenÃ­ dat.

### ProÄ je to dÅ¯leÅ¾itÃ©
Tento diplomatickÃ½ pokrok nastavuje precedens pro globÃ¡lnÃ­ governance AI, kde absence dohod hrozÃ­ zÃ¡vodnÃ½m zbranÄ›nÃ­m v autonomnÃ­ch systÃ©mech. Pro uÅ¾ivatele a prÅ¯mysl to znamenÃ¡ potenciÃ¡lnÃ­ stabilizaci: mÃ©nÄ› rizik neoÄekÃ¡vanÃ½ch incidentÅ¯, jako je chyba AI v kritickÃ© infrastruktuÅ™e, ale zÃ¡roveÅˆ zpomalenÃ­ inovacÃ­ kvÅ¯li bezpeÄnostnÃ­m omezenÃ­m. V Å¡irÅ¡Ã­m kontextu posiluje to pozici USA v soutÄ›Å¾i s ÄŒÃ­nou, kde americkÃ© modely (napÅ™. od OpenAI) dominujÃ­ v komerÄnÃ­m sektoru, zatÃ­mco ÄÃ­nskÃ© vedou v masovÃ© adopci. Bez takovÃ©ho dialogu by rostoucÃ­ integrace AI do vojenskÃ½ch systÃ©mÅ¯ mohla vÃ©st k nestabilitÄ›, podobnÄ› jako v minulosti u jadernÃ½ch zbranÃ­. CelkovÄ› to podtrhuje nutnost expertÅ¯ na AI bezpeÄnost, aby ovlivÅˆovali politiku a zabraÅˆovali scÃ©nÃ¡Å™Å¯m, kde algoritmy pÅ™ebÃ­rajÃ­ klÃ­ÄovÃ¡ rozhodnutÃ­.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.livemint.com/opinion/columns/us-china-ai-risks-national-security-diplomacy-technology-global-stability-11767179324869.html)

**Zdroj:** ğŸ“° Livemint
