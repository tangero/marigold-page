---
author: Marisa Aigen
category: ai
date: '2025-12-12 19:16:00'
description: ÃšÅ™ad pro management a rozpoÄet upÅ™esnil kroky, kterÃ© federÃ¡lnÃ­ agentury
  musÃ­ podniknout, aby jejich kontraktovanÃ© velkÃ© jazykovÃ© modely nevyrÃ¡bÄ›ly 'woke'
  vÃ½stupy. Memo navazuje na prezidentskÃ½ pÅ™Ã­kaz proti ideologicky zkreslenÃ© umÄ›lÃ©
  inteligenci.
importance: 4
layout: tech_news_article
original_title: White House instructs agencies to stop using â€˜biasedâ€™ AI
publishedAt: '2025-12-12T19:16:00+00:00'
slug: white-house-instructs-agencies-to-stop-using-biase
source:
  emoji: ğŸ“°
  id: null
  name: Nextgov
title: BÃ­lÃ½ dÅ¯m naÅ™izuje agenturÃ¡m pÅ™estat pouÅ¾Ã­vat 'zkreslenou' AI
url: https://www.nextgov.com/artificial-intelligence/2025/12/white-house-instructs-agencies-stop-using-biased-ai/410135/
urlToImage: https://cdn.nextgov.com/media/img/cd/2025/12/12/121225EisenhowerNG/open-graph.jpg
urlToImageBackup: https://cdn.nextgov.com/media/img/cd/2025/12/12/121225EisenhowerNG/open-graph.jpg
---

### Souhrn
ÃšÅ™ad pro management a rozpoÄet (OMB) vydal 12. prosince 2025 memorandum, kterÃ© federÃ¡lnÃ­m agenturÃ¡m naÅ™izuje zajistit, aby umÄ›lÃ¡ inteligence, pÅ™edevÅ¡Ã­m velkÃ© jazykovÃ© modely (LLM), produkovala pravdivÃ© vÃ½stupy bez manipulace ve prospÄ›ch ideologickÃ½ch dogmat. Tento krok vychÃ¡zÃ­ z prezidentskÃ©ho vÃ½konnÃ©ho pÅ™Ã­kazu Donalda Trumpa z Äervence, kterÃ½ mÄ›l eradikovat tzv. 'woke' AI v federÃ¡lnÃ­ sprÃ¡vÄ›.

### KlÃ­ÄovÃ© body
- FederÃ¡lnÃ­ agentury musÃ­ vyhodnotit stÃ¡vajÃ­cÃ­ kontrakty na LLM a upravit je tak, aby odpovÃ­daly principÅ¯m nestrannÃ© AI.
- Do 11. bÅ™ezna 2026 aktualizovat internÃ­ politiky a postupy vÄetnÄ› mechanismu pro hlÃ¡Å¡enÃ­ LLM, kterÃ© poruÅ¡ujÃ­ novÃ¡ pravidla.
- PÅ™i zadÃ¡vÃ¡nÃ­ novÃ½ch zakÃ¡zek na LLM zÃ­skÃ¡vat od dodavatelÅ¯ dostateÄnÃ© informace o souladu s principy nestrannÃ© AI.
- LLM musÃ­ prioritizovat historickou pÅ™esnost, vÄ›deckÃ½ pÅ™Ã­stup, objektivitu a uznÃ¡vat nejistoty v pÅ™Ã­padech nedostateÄnÃ½ch nebo protichÅ¯dnÃ½ch dat.
- Memo zdÅ¯razÅˆuje, Å¾e AI nesmÃ­ bÃ½t 'truth-seeking' jen formÃ¡lnÄ›, ale musÃ­ se vyhÃ½bat ideologickÃ½m zkreslenÃ­m.

### Podrobnosti
ÃšÅ™ad pro management a rozpoÄet (OMB), kterÃ½ spadÃ¡ pod BÃ­lÃ½ dÅ¯m a Å™Ã­dÃ­ federÃ¡lnÃ­ rozpoÄty a administrativu, vydal toto memorandum jako reakci na Å¡irÅ¡Ã­ debatu o zkreslenÃ­ v umÄ›lÃ© inteligenci. VelkÃ© jazykovÃ© modely, jako jsou ty od OpenAI (GPT sÃ©rie), Anthropic (Claude) nebo Meta (Llama), slouÅ¾Ã­ k generovÃ¡nÃ­ textu, analÃ½ze dat a automatizaci ÃºkolÅ¯ v oblastiach jako je sprÃ¡va dokumentÅ¯, prÃ¡vnÃ­ analÃ½zy nebo veÅ™ejnÃ¡ komunikace. ProblÃ©m spoÄÃ­vÃ¡ v tom, Å¾e mnohÃ© z tÄ›chto modelÅ¯ jsou trÃ©novÃ¡ny na datech obsahujÃ­cÃ­ch sociÃ¡lnÃ­ a politickÃ© biasy, coÅ¾ vede k vÃ½stupÅ¯m, kterÃ© OMB oznaÄuje za 'woke' â€“ tedy pÅ™Ã­liÅ¡ liberÃ¡lnÄ› nebo ideologicky zamÄ›Å™enÃ©.

Memo explicitnÄ› uvÃ¡dÃ­, Å¾e LLM v federÃ¡lnÃ­ sprÃ¡vÄ› musÃ­ bÃ½t 'truth-seeking', coÅ¾ znamenÃ¡ zamÄ›Å™enÃ­ na historickou pÅ™esnost (napÅ™. objektivnÃ­ popis udÃ¡lostÃ­ bez modernÃ­ch interpretacÃ­), vÄ›deckou metodiku (overitelnÃ¡ data pÅ™ed spekulacemi) a objektivitu (vyvÃ¡Å¾enÃ© pohledy bez upÅ™ednostÅˆovÃ¡nÃ­ jednÃ© ideologie). Agentury majÃ­ do 11. bÅ™ezna 2026 pÅ™izpÅ¯sobit svÃ© postupy, vÄetnÄ› vytvoÅ™enÃ­ kanÃ¡lÅ¯ pro stÃ­Å¾nosti na nespolehlivÃ© modely. PÅ™i novÃ½ch zakÃ¡zkÃ¡ch musÃ­ dodavatelÃ© prokÃ¡zat soulad, coÅ¾ mÅ¯Å¾e zahrnovat audity trÃ©novacÃ­ch dat nebo testy na bias.

Tento pÅ™Ã­stup navazuje na TrumpÅ¯v vÃ½konnÃ½ pÅ™Ã­kaz z Äervence 2025, kterÃ½ kritizoval existujÃ­cÃ­ AI systÃ©my za Å¡Ã­Å™enÃ­ 'ideologickÃ½ch dogmat'. V praxi to znamenÃ¡, Å¾e firmy jako Microsoft (partner OpenAI) nebo Google, kterÃ© dodÃ¡vajÃ­ AI federÃ¡lnÃ­m agenturÃ¡m, budou muset upravit svÃ© modely â€“ napÅ™Ã­klad pÅ™idÃ¡nÃ­m 'safety layers' nebo fine-tuningem na neutrÃ¡lnÄ›jÅ¡Ã­ datasety. OMB neupÅ™esÅˆuje technickÃ© detaily, jako je mÄ›Å™enÃ­ biasu (napÅ™. metriky jako WEAT nebo toxicity scores), ale zdÅ¯razÅˆuje zodpovÄ›dnost agentur za dodrÅ¾ovÃ¡nÃ­.

Jako expert na AI vidÃ­m zde rizika: definice 'woke' je subjektivnÃ­ a mÅ¯Å¾e vÃ©st k opaÄnÃ©mu biasu ve prospÄ›ch konzervativnÃ­ch narativÅ¯. NavÃ­c, dosaÅ¾enÃ­ skuteÄnÃ© objektivity v LLM je technicky nÃ¡roÄnÃ©, protoÅ¾e modely odrÃ¡Å¾ejÃ­ data, na kterÃ½ch jsou trÃ©novÃ¡ny â€“ Äasto z internetu plnÃ©ho polarizace. FederÃ¡lnÃ­ vlÃ¡da utratila v poslednÃ­ch letech stovky milionÅ¯ dolarÅ¯ na AI kontrakty, takÅ¾e toto memo pÅ™Ã­mo ovlivnÃ­ trh.

### ProÄ je to dÅ¯leÅ¾itÃ©
Tato regulace nastavuje precedens pro vlÃ¡dnÃ­ nÃ¡kupy AI v USA, coÅ¾ je nejvÄ›tÅ¡Ã­ svÄ›tovÃ½ trh s rozpoÄtem pÅ™es 20 miliard dolarÅ¯ roÄnÄ› na IT. Firmy vyvÃ­jejÃ­cÃ­ LLM budou muset investovat do 'debiasing' technik, jako je reinforcement learning from human feedback (RLHF) s diverznÃ­mi recenzenty nebo syntetickÃ¡ data. Pro prÅ¯mysl to znamenÃ¡ vyÅ¡Å¡Ã­ nÃ¡klady, ale i pÅ™Ã­leÅ¾itost pro specializovanÃ© 'government-grade' modely.

V Å¡irÅ¡Ã­m kontextu posiluje debatu o AI etice: zatÃ­mco EU mÃ¡ AI Act s dÅ¯razem na rizika a transparentnost, USA se zamÄ›Å™uje na ideologickou neutralitu. To mÅ¯Å¾e ovlivnit globÃ¡lnÃ­ standardy, protoÅ¾e americkÃ© firmy dominujÃ­ trhu. Pro uÅ¾ivatele v ÄŒR nebo EU to znamenÃ¡ nepÅ™Ã­mÃ½ tlak na mezinÃ¡rodnÃ­ dodavatele, aby upravili modely pro podobnÃ© poÅ¾adavky. DlouhodobÄ› by to mohlo zlepÅ¡it spolehlivost AI v kritickÃ½ch oblastech, ale zÃ¡roveÅˆ omezit kreativitu modelÅ¯ v sociÃ¡lnÃ­ch tÃ©matech.

(Celkem cca 550 slov)

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.nextgov.com/artificial-intelligence/2025/12/white-house-instructs-agencies-stop-using-biased-ai/410135/)

**Zdroj:** ğŸ“° Nextgov
