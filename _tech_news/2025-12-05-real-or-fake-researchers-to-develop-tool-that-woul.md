---
author: Marisa Aigen
category: umÄ›lÃ¡ inteligence
date: '2025-12-05 20:07:21'
description: KanadÅ¡tÃ­ vÃ½zkumnÃ­ci z oblastÃ­ technologie a prÃ¡va pracujÃ­ na open-source
  nÃ¡stroji, kterÃ½ mÃ¡ pomoci soudÅ¯m rozliÅ¡it autentickÃ© dÅ¯kazy od tÄ›ch generovanÃ½ch
  umÄ›lou inteligencÃ­. Projekt by mÄ›l trvat dva roky a nÃ¡stroj bude zdarma a snadno
  pouÅ¾itelnÃ½.
importance: 3
layout: tech_news_article
original_title: Real or fake? Researchers to develop tool that would help courts spot
  AI evidence
publishedAt: '2025-12-05T20:07:21+00:00'
slug: real-or-fake-researchers-to-develop-tool-that-woul
source:
  emoji: ğŸ“°
  id: financial-post
  name: Financial Post
title: SkuteÄnÃ© nebo faleÅ¡nÃ©? KanadÅ¡tÃ­ vÄ›dci vyvÃ­jejÃ­ nÃ¡stroj pro soudy k odhalenÃ­
  dÅ¯kazÅ¯ z AI
url: https://financialpost.com/pmn/real-or-fake-researchers-to-develop-tool-that-would-help-courts-spot-ai-evidence
---

## Souhrn
KanadÅ¡tÃ­ vÃ½zkumnÃ­ci z univerzit v Ontariu a BritskÃ© Kolumbii spustili projekt na vÃ½voj open-source nÃ¡stroje pro detekci obsahu vytvoÅ™enÃ©ho umÄ›lou inteligencÃ­, jako jsou upravovanÃ¡ videa nebo fotografie. NÃ¡stroj je urÄen soudÅ¯m, ÃºÄastnÃ­kÅ¯m soudnÃ­ch sporÅ¯ a dalÅ¡Ã­m, aby pomohl oddÄ›lit skuteÄnÃ© dÅ¯kazy od falÅ¡ovanÃ½ch. VÃ½voj potrvÃ¡ dva roky a bude zdarma k dispozici.

## KlÃ­ÄovÃ© body
- TÃ½m zahrnuje technologickÃ© odbornÃ­ky a prÃ¡vnÃ­ky z University of Waterloo, York University a univerzit v BritskÃ© Kolumbii.
- SouÄasnÃ© komerÄnÃ­ detektory AI obsahu jsou neprÅ¯hlednÃ©, nespolehlivÃ©, produkujÃ­ faleÅ¡nÃ© pozitiva a vykazujÃ­ zkreslenÃ­ vÅ¯Äi mluvÄÃ­m, kteÅ™Ã­ nemajÃ­ rodilou angliÄtinu.
- ZamÄ›stnÃ¡vÃ¡nÃ­ expertÅ¯ na ovÄ›Å™enÃ­ dÅ¯kazÅ¯ je nÃ¡kladnÃ© a prodluÅ¾uje soudnÃ­ procesy.
- Projekt vede Maura Grossman, profesorka informatiky na University of Waterloo a pedagoÅ¾ka na prÃ¡vnickÃ© fakultÄ› Osgoode Hall.
- V soudnÃ­m prostÅ™edÃ­ jsou rizika obzvlÃ¡Å¡tÄ› vysokÃ¡, protoÅ¾e faleÅ¡nÃ© dÅ¯kazy mohou ovlivnit verdikty.

## Podrobnosti
Skupina kanadskÃ½ch vÄ›dcÅ¯ reaguje na rostoucÃ­ dostupnost nÃ¡strojÅ¯ umÄ›lÃ© inteligence, kterÃ© umoÅ¾ÅˆujÃ­ i bÄ›Å¾nÃ½m uÅ¾ivatelÅ¯m snadno upravovat nebo zcela vytvÃ¡Å™et videa, fotografie a dalÅ¡Ã­ digitÃ¡lnÃ­ dÅ¯kazy. NapÅ™Ã­klad modely jako Stable Diffusion nebo DALL-E generujÃ­ realistickÃ© obrÃ¡zky, zatÃ­mco nÃ¡stroje pro deepfake videa, jako jsou varianty FaceSwap Äi Roop, dokÃ¡Å¾ou nahradit tvÃ¡Å™e v nahrÃ¡vkÃ¡ch s vysokou pÅ™esnostÃ­. Tyto technologie jsou dnes volnÄ› dostupnÃ© online a nevyÅ¾adujÃ­ pokroÄilÃ© znalosti. VÃ½zkumnÃ­ci plÃ¡nujÃ­ vyvinout nÃ¡stroj, kterÃ½ bude analyzovat stopy typickÃ© pro AI-generaci, jako jsou anomÃ¡lie v pixelech, frekvencÃ­ch signÃ¡lu nebo statistickÃ½ch vzorcÃ­ch v datech.

Projekt je financovÃ¡n a bude open-source, coÅ¾ znamenÃ¡, Å¾e zdrojovÃ½ kÃ³d bude veÅ™ejnÄ› dostupnÃ½ na platformÃ¡ch jako GitHub. To umoÅ¾nÃ­ komunitÄ› vÃ½vojÃ¡Å™Å¯ pÅ™ispÃ­vat, testovat a zlepÅ¡ovat ho. Na rozdÃ­l od komerÄnÃ­ch Å™eÅ¡enÃ­, jako jsou detektory od spoleÄnostÃ­ Hive Moderation nebo Reality Defender, kterÃ© Äasto fungujÃ­ jako uzavÅ™enÃ© "black box" systÃ©my, tento nÃ¡stroj mÃ¡ bÃ½t prÅ¯hlednÃ½. Maura Grossman upozorÅˆuje, Å¾e souÄasnÃ© nÃ¡stroje selhÃ¡vajÃ­ v detekci obsahu v jinÃ½ch jazycÃ­ch nebo dialektech, coÅ¾ vede k nespravedlivÃ½m vÃ½sledkÅ¯m. NapÅ™Ã­klad algoritmy trÃ©novanÃ© pÅ™evÃ¡Å¾nÄ› na anglickÃ©m obsahu mohou oznaÄit autentickÃ½ materiÃ¡l z neanglicky mluvÃ­cÃ­ch zemÃ­ za faleÅ¡nÃ½.

VÃ½vojÃ¡Å™i zdÅ¯razÅˆujÃ­, Å¾e soudy nejsou pÅ™ipraveny na pÅ™Ã­liv AI-generovanÃ©ho materiÃ¡lu. V USA a EvropÄ› jiÅ¾ doÅ¡lo k pÅ™Ã­padÅ¯m, kde deepfakes ovlivnily soudnÃ­ spory, napÅ™Ã­klad v kauzÃ¡ch tÃ½kajÃ­cÃ­ch se sexuÃ¡lnÃ­ho obtÄ›Å¾ovÃ¡nÃ­ nebo politickÃ½ch Å¡kandÃ¡lÅ¯. ZamÄ›stnÃ¡vÃ¡nÃ­ forenznÃ­ch expertÅ¯, kteÅ™Ã­ manuÃ¡lnÄ› analyzujÃ­ metadata, kompresi souborÅ¯ nebo artefakty v obrazech, stojÃ­ tisÃ­ce dolarÅ¯ a trvÃ¡ tÃ½dny, coÅ¾ zpomaluje spravedlnost. NovÃ½ nÃ¡stroj by mÄ›l bÃ½t rychlÃ½, intuitivnÃ­ a integrovatelnÃ½ do stÃ¡vajÃ­cÃ­ch soudnÃ­ch systÃ©mÅ¯, jako je elektronickÃ© podÃ¡vÃ¡nÃ­ dÅ¯kazÅ¯.

## ProÄ je to dÅ¯leÅ¾itÃ©
V Ã©Å™e, kdy AI modely jako Midjourney nebo Sora produkujÃ­ video s rozliÅ¡enÃ­m 1080p v reÃ¡lnÃ©m Äase, se dÅ¯vÄ›ryhodnost digitÃ¡lnÃ­ch dÅ¯kazÅ¯ stÃ¡vÃ¡ klÃ­ÄovÃ½m problÃ©mem pro prÃ¡vnÃ­ systÃ©my. Open-source pÅ™Ã­stup mÅ¯Å¾e demokratizovat detekci a snÃ­Å¾it zÃ¡vislost na proprietÃ¡rnÃ­ch sluÅ¾bÃ¡ch, kterÃ© mohou bÃ½t ovlivnÄ›ny korporÃ¡tnÃ­mi zÃ¡jmy. Pro prÅ¯mysl to znamenÃ¡ lepÅ¡Ã­ nÃ¡stroje pro novinÃ¡Å™e, bezpeÄnostnÃ­ firmy a sociÃ¡lnÃ­ sÃ­tÄ›, kde deepfakes Å¡Ã­Å™Ã­ dezinformace. NicmÃ©nÄ› ÃºspÄ›ch zÃ¡visÃ­ na kontinuÃ¡lnÃ­m trÃ©ninku proti novÃ½m AI modelÅ¯m, jinak rychle zastarÃ¡vÃ¡. Tento projekt pÅ™ispÃ­vÃ¡ k Å¡irÅ¡Ã­mu ÃºsilÃ­ o AI governance, podobnÄ› jako iniciativy EU AI Act, kterÃ© vyÅ¾adujÃ­ watermarking pro generovanÃ½ obsah.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://financialpost.com/pmn/real-or-fake-researchers-to-develop-tool-that-would-help-courts-spot-ai-evidence)

**Zdroj:** ğŸ“° Financial Post
