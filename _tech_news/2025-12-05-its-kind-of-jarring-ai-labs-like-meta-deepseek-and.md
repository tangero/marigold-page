---
author: Marisa Aigen
category: bezpeÄnost ai
companies:
- Meta
- Deepseek
- Xai
- Anthropic
- OpenAI
date: '2025-12-05 22:18:35'
description: Anthropic, OpenAI a Google DeepMind obsadily prvnÃ­ tÅ™i mÃ­sta s celkovou
  znÃ¡mkou C+ nebo C. ZprÃ¡va od Future of Life Institute ukazuje slabiny v bezpeÄnostnÃ­ch
  postupech velkÃ½ch AI firem.
importance: 4
layout: tech_news_article
original_title: 'Itâ€™s â€˜kind of jarringâ€™: AI labs like Meta, Deepseek, and Xai earned
  some of the worst grades possible on an existential safety index'
publishedAt: '2025-12-05T22:18:35+00:00'
slug: its-kind-of-jarring-ai-labs-like-meta-deepseek-and
source:
  emoji: ğŸ“°
  id: fortune
  name: Fortune
title: 'Je to â€šnÄ›jak Å¡okujÃ­cÃ­â€˜: AI laboratoÅ™e jako Meta, Deepseek a Xai zÃ­skaly nÄ›kterÃ©
  z nejhorÅ¡Ã­ch znÃ¡mek v indexu existenciÃ¡lnÃ­ bezpeÄnosti'
url: https://fortune.com/2025/12/05/ai-labs-meta-deepseek-xai-bad-grades-existential-safety-index/
urlToImage: https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2168198379-e1764973088304.jpg?resize=1200,600
urlToImageBackup: https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2168198379-e1764973088304.jpg?resize=1200,600
---

### Souhrn
Institute Future of Life (FLI) vydal nejnovÄ›jÅ¡Ã­ index bezpeÄnosti AI, kterÃ½ hodnotÃ­ osm velkÃ½ch AI laboratoÅ™Ã­. Anthropic, OpenAI a Google DeepMind dosÃ¡hly nejlepÅ¡Ã­ch vÃ½sledkÅ¯ s celkovou znÃ¡mkou C+ nebo C, zatÃ­mco Meta, Deepseek, Xai, Z.ai a Alibaba zÃ­skaly D nebo D-. NejhorÅ¡Ã­ vÃ½sledky firmy vykÃ¡zaly v kategorii existenciÃ¡lnÃ­ bezpeÄnosti, kde vÅ¡echny obdrÅ¾ely D nebo F.

### KlÃ­ÄovÃ© body
- HodnocenÃ­ probÄ›hlo na zÃ¡kladÄ› veÅ™ejnÃ½ch materiÃ¡lÅ¯ a odpovÄ›dÃ­ na prÅ¯zkumy pÄ›ti firem; recenzenty byli AI akademici a experti na governance.
- Kategorie zahrnovaly bezpeÄnostnÃ­ rÃ¡mce, hodnocenÃ­ rizik a souÄasnÃ© Å¡kody zpÅ¯sobenÃ© AI.
- ExistenciÃ¡lnÃ­ bezpeÄnost, tedy pÅ™Ã­prava na rizika superinteligence, selhala u vÅ¡ech firem.
- Max Tegmark z MIT, prezident FLI, kritizuje absenci regulacÃ­, kterÃ¡ vede k prioritizaci rychlosti pÅ™ed bezpeÄnostÃ­.
- ÄŒtyÅ™i z pÄ›ti americkÃ½ch firem nynÃ­ odpovÃ­dajÃ­ na prÅ¯zkumy FLI, Meta je jedinÃ¡ vÃ½jimka.

### Podrobnosti
Future of Life Institute, neziskovÃ¡ organizace zamÄ›Å™enÃ¡ na mitigaci globÃ¡lnÃ­ch rizik vÄetnÄ› tÄ›ch spojenÃ½ch s AI, zveÅ™ejnil tento index jako nÃ¡stroj pro hodnocenÃ­ odpovÄ›dnosti AI firem. HodnocenÃ­ se opÃ­ralo o analÃ½zu veÅ™ejnÄ› dostupnÃ½ch dokumentÅ¯, jako jsou bezpeÄnostnÃ­ protokoly a zprÃ¡vy o rizicÃ­ch, doplnÄ›nou o odpovÄ›di na standardizovanÃ½ prÅ¯zkum, kterÃ½ podalo pÄ›t z osmi firem. Recenzenti, panel AI akademikÅ¯ a odbornÃ­kÅ¯ na regulaci, hodnotili firmy v klÃ­ÄovÃ½ch oblastech: vÃ½voj bezpeÄnostnÃ­ch rÃ¡mcÅ¯ pro kontrolu AI systÃ©mÅ¯, systematickÃ© hodnocenÃ­ potenciÃ¡lnÃ­ch rizik vÄetnÄ› katastrofickÃ½ch scÃ©nÃ¡Å™Å¯ a opatÅ™enÃ­ proti aktuÃ¡lnÃ­m Å¡kodÃ¡m, jako jsou biasy v modelech nebo zneuÅ¾itÃ­ pro dezinformace.

NejproblematiÄtÄ›jÅ¡Ã­ kategoriÃ­ se stala existenciÃ¡lnÃ­ bezpeÄnost, kterÃ¡ se zabÃ½vÃ¡ dlouhodobÃ½mi riziky, jako je vÃ½voj superinteligence â€“ AI pÅ™ekonÃ¡vajÃ­cÃ­ lidskou inteligenci ve vÅ¡ech oblastech. Mnoho tÄ›chto firem, vÄetnÄ› OpenAI, Anthropic nebo Xai (spoleÄnost Elona Muska zamÄ›Å™enÃ¡ na bezpeÄnÃ© AGI), veÅ™ejnÄ› deklaruje cÃ­l dosÃ¡hnout superinteligence, ale podle recenzentÅ¯ chybÃ­ konkrÃ©tnÃ­ plÃ¡ny na jejÃ­ bezpeÄnÃ© Å™Ã­zenÃ­. Max Tegmark popsal tento rozpor jako â€Å¡okujÃ­cÃ­â€œ, protoÅ¾e firmy investujÃ­ miliardy do vÃ½voje, ale ignorujÃ­ zÃ¡kladnÃ­ bezpeÄnostnÃ­ protokoly.

PoÅ™adÃ­ firem: prvnÃ­ tÅ™i â€“ Anthropic (C+), OpenAI (C), Google DeepMind (C). NÃ¡sledovaly Xai (D), Z.ai (D), Meta (D-), Deepseek (D-) a Alibaba (D-). Deepseek je ÄÃ­nskÃ¡ AI laboratoÅ™ specializujÃ­cÃ­ se na velkÃ© jazykovÃ© modely (LLM) jako DeepSeek-V2, kterÃ© konkurujÃ­ GPT modelÅ¯m v efektivitÄ›. Z.ai je mÃ©nÄ› znÃ¡mÃ¡ firma zamÄ›Å™enÃ¡ na AI governance. Slabiny odrÃ¡Å¾ejÃ­ Å¡irÅ¡Ã­ trend: v absenci federÃ¡lnÃ­ch regulacÃ­ v USA (navzdory pokusÅ¯m v Kalifornii a New Yorku) konkurence v AI zÃ¡vodÄ› nutÃ­ firmy urychlovat vydÃ¡vÃ¡nÃ­ modelÅ¯ na Ãºkor bezpeÄnosti. FLI proto vydÃ¡vÃ¡ tyto indexy pravidelnÄ›, aby tlaÄily na prÅ¯mysl â€“ nynÃ­ ÄtyÅ™i americkÃ© firmy (kromÄ› MetÃ¡) spolupracujÃ­.

### ProÄ je to dÅ¯leÅ¾itÃ©
Tento index odhaluje systÃ©movÃ© selhÃ¡nÃ­ v AI prÅ¯myslu, kde i lÃ­dÅ™i jako OpenAI dosahujÃ­ jen prÅ¯mÄ›rnÃ½ch znÃ¡mek. V kontextu rychlÃ©ho pokroku smÄ›rem k AGI to zvyÅ¡uje rizika nekontrolovanÃ©ho nasazenÃ­ systÃ©mÅ¯ schopnÃ½ch autonomnÃ­ho rozhodovÃ¡nÃ­. Pro uÅ¾ivatele to znamenÃ¡ vyÅ¡Å¡Ã­ pravdÄ›podobnost incidentÅ¯, jako jsou halucinace v LLM nebo zneuÅ¾itÃ­ pro kybernetickÃ© Ãºtoky. Pro prÅ¯mysl to podtrhuje potÅ™ebu samoregulace nebo legislativy â€“ Kalifornie nedÃ¡vno zavÃ¡dÃ­ povinnÃ© reportovÃ¡nÃ­ katastrofickÃ½ch rizik, coÅ¾ by mohlo stanovit standard. Bez zmÄ›n hrozÃ­, Å¾e zÃ¡vod na superinteligenci skonÄÃ­ bez bezpeÄnostnÃ­ch pojistek, coÅ¾ ohrozÃ­ globÃ¡lnÃ­ stabilitu.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://fortune.com/2025/12/05/ai-labs-meta-deepseek-xai-bad-grades-existential-safety-index/)

**Zdroj:** ğŸ“° Fortune
