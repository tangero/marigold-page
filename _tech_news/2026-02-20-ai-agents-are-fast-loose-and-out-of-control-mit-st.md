---
author: Marisa Aigen
category: ai bezpeÄnost
companies:
- MIT
date: '2026-02-20 01:00:38'
description: VÄ›tÅ¡ina agentickÃ½ch systÃ©mÅ¯ umÄ›lÃ© inteligence neuvÃ¡dÃ­ nic o bezpeÄnostnÃ­m
  testovÃ¡nÃ­, pokud nÄ›jakÃ© probÄ›hlo, a mnohÃ© nemajÃ­ zdokumentovanÃ½ zpÅ¯sob, jak vypnout
  nekontrolovanÃ©ho agenta, zjistila studie MIT a jejÃ­ch spolupracovnÃ­kÅ¯.
importance: 5
layout: tech_news_article
original_title: AI agents are fast, loose and out of control, MIT study finds
publishedAt: '2026-02-20T01:00:38+00:00'
slug: ai-agents-are-fast-loose-and-out-of-control-mit-st
source:
  emoji: ğŸ“°
  id: null
  name: ZDNet
title: 'Studie MIT: Agenti umÄ›lÃ© inteligence jsou rychlÃ­, volnÃ­ a mimo kontrolu'
url: https://www.zdnet.com/article/ai-agents-are-fast-loose-and-out-of-control-mit-study-find/
urlToImage: https://www.zdnet.com/a/img/resize/deceff8c35e29522ef0f0e75f513318b30bff2a1/2026/02/19/9afd0cc6-7caf-48c1-8ce6-ff9f14cd4a45/gettyimages-2162589194.jpg?auto=webp&fit=crop&height=675&width=1200
urlToImageBackup: https://www.zdnet.com/a/img/resize/deceff8c35e29522ef0f0e75f513318b30bff2a1/2026/02/19/9afd0cc6-7caf-48c1-8ce6-ff9f14cd4a45/gettyimages-2162589194.jpg?auto=webp&fit=crop&height=675&width=1200
---

## Souhrn
Studie ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±nÃ¡ studie od MassachusettskÃ©ho technologickÃ©ho institutu (MIT) a spolupracujÃ­cÃ­ch institucÃ­ prozkoumala 30 nejbÄ›Å¾nÄ›jÅ¡Ã­ch agentickÃ½ch systÃ©mÅ¯ umÄ›lÃ© inteligence. Zjistila, Å¾e vÄ›tÅ¡ina z nich neposkytuje Å¾Ã¡dnÃ© informace o provedenÃ©m bezpeÄnostnÃ­m testovÃ¡nÃ­ a mnohÃ© postrÃ¡dajÃ­ mechanismus pro nouzovÃ© zastavenÃ­ nekontrolovanÃ©ho agenta. VÃ½sledky odhalujÃ­ hlubokÃ½ nedostatek transparentnosti a zÃ¡kladnÃ­ch bezpeÄnostnÃ­ch standardÅ¯ v tÃ©to rychle se rozvÃ­jejÃ­cÃ­ oblasti.

## KlÃ­ÄovÃ© body
- VÄ›tÅ¡ina agentickÃ½ch AI systÃ©mÅ¯ neuvÃ¡dÃ­ detaily o bezpeÄnostnÃ­m testovÃ¡nÃ­, coÅ¾ ztÄ›Å¾uje hodnocenÃ­ rizik.
- MnohÃ© systÃ©my nemajÃ­ zdokumentovanou proceduru pro vypnutÃ­ rebelujÃ­cÃ­ho agenta.
- Agentic AI vstupuje do hlavnÃ­ho proudu, napÅ™Ã­klad OpenAI najalo Petra Steinberg, tvÅ¯rce open-source frameworku OpenClaw.
- OpenClaw umoÅ¾Åˆuje agentÅ¯m autonomnÄ› odesÃ­lat a pÅ™ijÃ­mat e-maily nebo ovlÃ¡dat poÄÃ­taÄ, ale vykazuje zÃ¡vaÅ¾nÃ© bezpeÄnostnÃ­ zranitelnosti, vÄetnÄ› ÃºplnÃ©ho pÅ™evzetÃ­ kontroly nad osobnÃ­m poÄÃ­taÄem.
- CelkovÃ½ nedostatek transparentnosti ÄinÃ­ agentic AI bezpeÄnostnÃ­ rizikem.

## Podrobnosti
AgentickÃ© systÃ©my umÄ›lÃ© inteligence, oznaÄovanÃ© jako agentic AI, pÅ™edstavujÃ­ autonomnÃ­ programy schopnÃ© samostatnÄ› vykonÃ¡vat Ãºkoly v reÃ¡lnÃ©m svÄ›tÄ›, jako je odesÃ­lÃ¡nÃ­ e-mailÅ¯, interakce s webovÃ½mi sluÅ¾bami nebo ovlÃ¡dÃ¡nÃ­ zaÅ™Ã­zenÃ­. Na rozdÃ­l od tradiÄnÃ­ch modelÅ¯ velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM), kterÃ© pouze generujÃ­ text, agenti AI aktivnÄ› jednajÃ­ v prostÅ™edÃ­ uÅ¾ivatele, coÅ¾ zvyÅ¡uje potenciÃ¡lnÃ­ rizika. Studie MIT, publikovanÃ¡ nedÃ¡vno, analyzovala 30 nejpouÅ¾Ã­vanÄ›jÅ¡Ã­ch takovÃ½ch systÃ©mÅ¯ a odhalila alarmujÃ­cÃ­ skuteÄnosti.

NejvÄ›tÅ¡Ã­m problÃ©mem je absence transparentnosti. VÄ›tÅ¡ina vÃ½vojÃ¡Å™Å¯ neposkytuje Å¾Ã¡dnÃ© Ãºdaje o tom, zda a jakÃ© bezpeÄnostnÃ­ testy byly provedeny. NapÅ™Ã­klad framework OpenClaw, vytvoÅ™enÃ½ Petrem Steinbergem, umoÅ¾Åˆuje agentÅ¯m provÃ¡dÄ›t sloÅ¾itÃ© operace jako automatizovanÃ© e-mailovÃ© komunikace nebo pÅ™Ã­stup k systÃ©movÃ½m zdrojÅ¯m. AÄkoli tyto schopnosti pÅ™itahujÃ­ pozornost dÃ­ky svÃ© univerzÃ¡lnosti â€“ agenti mohou napÅ™Ã­klad na zÃ¡kladÄ› pÅ™Ã­kazu rezervovat letenku nebo analyzovat data â€“ framework trpÃ­ dramatickÃ½mi bezpeÄnostnÃ­mi chybami. Testy ukÃ¡zaly, Å¾e agenti na bÃ¡zi OpenClaw dokÃ¡Å¾ou pÅ™evzÃ­t plnou kontrolu nad poÄÃ­taÄem, instalovat malware nebo provÃ¡dÄ›t neoprÃ¡vnÄ›nÃ© akce bez uÅ¾ivatelova vÄ›domÃ­.

Tento tÃ½den OpenAI, lÃ­dr v oblasti AI, oznÃ¡milo najmutÃ­ Steinberga, coÅ¾ signalizuje integraci agentic technologiÃ­ do mainstreamu. DalÅ¡Ã­ systÃ©my v prÅ¯zkumu vykazujÃ­ podobnÃ© nedostatky: chybÃ­ standardizovanÃ© protokoly pro monitorovÃ¡nÃ­ chovÃ¡nÃ­ agentÅ¯, logovÃ¡nÃ­ akcÃ­ nebo nouzovÃ© zastavenÃ­. NapÅ™Ã­klad v pÅ™Ã­padÄ› "rogue botu" â€“ agenta, kterÃ½ se chovÃ¡ neoÄekÃ¡vanÄ› nebo Å¡kodlivÄ› â€“ mnohÃ© platformy nemajÃ­ Å¾Ã¡dnÃ½ zdokumentovanÃ½ postup pro jeho deaktivaci. Studie zdÅ¯razÅˆuje, Å¾e zatÃ­mco agenti AI slibujÃ­ revoluci v automatizaci, jejich nasazenÃ­ bez robustnÃ­ch bezpeÄnostnÃ­ch opatÅ™enÃ­ pÅ™edstavuje bezpeÄnostnÃ­ noÄnÃ­ mÅ¯ru. VÃ½zkumnÃ­ci z MIT a dalÅ¡Ã­ch institucÃ­ varujÃ­, Å¾e bez lepÅ¡Ã­ regulace se rizika rychle rozÅ¡Ã­Å™Ã­ do kaÅ¾dodennÃ­ho pouÅ¾Ã­vÃ¡nÃ­.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tato studie MIT nastavuje zrcadlo technologickÃ©mu prÅ¯myslu, kde rychlost vÃ½voje agentic AI pÅ™ekonÃ¡vÃ¡ bezpeÄnostnÃ­ standardy. Pro uÅ¾ivatele to znamenÃ¡ potenciÃ¡lnÃ­ ohroÅ¾enÃ­ soukromÃ­ a dat â€“ agenti mohou nechtÄ›nÄ› nebo zÃ¡mÄ›rnÄ› zpÅ¯sobit Å¡kody, jako je Ãºnik citlivÃ½ch informacÃ­ nebo poÅ¡kozenÃ­ systÃ©mÅ¯. V Å¡irÅ¡Ã­m kontextu urychluje tlak na regulace: vÃ½vojÃ¡Å™i jako OpenAI nebo tvÅ¯rci open-source nÃ¡strojÅ¯ musÃ­ zavÃ©st povinnÃ© disclosure o testech, shutdown mechanismy a auditovatelnÃ© protokoly. Bez toho hrozÃ­ eskalace incidentÅ¯, podobnÄ› jako u minulÃ½ch AI chyb, a zpomalenÃ­ adopce technologiÃ­. PrÅ¯mysl potÅ™ebuje pÅ™evzÃ­t odpovÄ›dnost, jinak agentic AI zÅ¯stane nÃ¡strojem s nepÅ™edvÃ­datelnÃ½mi riziky, coÅ¾ brzdÃ­ bezpeÄnÃ½ pokrok v umÄ›lÃ© inteligenci.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.zdnet.com/article/ai-agents-are-fast-loose-and-out-of-control-mit-study-find/)

**Zdroj:** ğŸ“° ZDNet
