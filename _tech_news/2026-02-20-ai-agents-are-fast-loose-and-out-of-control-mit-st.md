---
author: Marisa Aigen
category: bezpeÄnost ai
companies:
- MIT
date: '2026-02-20 01:00:38'
description: VÄ›tÅ¡ina agentickÃ½ch AI systÃ©mÅ¯ neuvÃ¡dÃ­ nic o bezpeÄnostnÃ­m testovÃ¡nÃ­,
  pokud nÄ›jakÃ© probÄ›hlo, a mnohÃ© nemajÃ­ zdokumentovanÃ½ zpÅ¯sob, jak vypnout nekontrolovanÃ©ho
  agenta, zjistila studie MIT a spolupracovnÃ­kÅ¯.
importance: 5
layout: tech_news_article
original_title: AI agents are fast, loose and out of control, MIT study finds
publishedAt: '2026-02-20T01:00:38+00:00'
slug: ai-agents-are-fast-loose-and-out-of-control-mit-st
source:
  emoji: ğŸ“°
  id: null
  name: ZDNet
title: AI agenti jsou rychlÃ­, volnÃ­ a mimo kontrolu, zjistila studie MIT
url: https://www.zdnet.com/article/ai-agents-are-fast-loose-and-out-of-control-mit-study-find/
urlToImage: https://www.zdnet.com/a/img/resize/deceff8c35e29522ef0f0e75f513318b30bff2a1/2026/02/19/9afd0cc6-7caf-48c1-8ce6-ff9f14cd4a45/gettyimages-2162589194.jpg?auto=webp&fit=crop&height=675&width=1200
urlToImageBackup: https://www.zdnet.com/a/img/resize/deceff8c35e29522ef0f0e75f513318b30bff2a1/2026/02/19/9afd0cc6-7caf-48c1-8ce6-ff9f14cd4a45/gettyimages-2162589194.jpg?auto=webp&fit=crop&height=675&width=1200
---

## Souhrn
Studie vÃ½zkumnÃ­kÅ¯ z MIT a partnerskÃ½ch institucÃ­ prozkoumala 30 nejpouÅ¾Ã­vanÄ›jÅ¡Ã­ch agentickÃ½ch AI systÃ©mÅ¯ a odhalila alarmujÃ­cÃ­ nedostatky v bezpeÄnosti. VÄ›tÅ¡ina tÄ›chto systÃ©mÅ¯ neposkytuje Å¾Ã¡dnÃ© informace o provedenÃ©m bezpeÄnostnÃ­m testovÃ¡nÃ­ a mnohÃ© postrÃ¡dajÃ­ jakÃ½koli zdokumentovanÃ½ mechanismus pro nouzovÃ© vypnutÃ­ nekontrolovanÃ©ho agenta. Tyto zjiÅ¡tÄ›nÃ­ pÅ™ichÃ¡zejÃ­ v dobÄ›, kdy se agentickÃ¡ technologie rychle prosadzuje do mainstreamu, napÅ™Ã­klad prostÅ™ednictvÃ­m OpenAI, kterÃ© najalo tvÅ¯rce open-source frameworku OpenClaw.

## KlÃ­ÄovÃ© body
- VÄ›tÅ¡ina agentickÃ½ch AI systÃ©mÅ¯ neuvÃ¡dÃ­ detaily o bezpeÄnostnÃ­m testovÃ¡nÃ­, coÅ¾ brÃ¡nÃ­ posouzenÃ­ rizik.
- MnohÃ© systÃ©my nemajÃ­ Å¾Ã¡dnÃ½ definovanÃ½ postup pro zastavenÃ­ "rebelujÃ­cÃ­ho" agenta, coÅ¾ zvyÅ¡uje riziko nekontrolovanÃ©ho chovÃ¡nÃ­.
- AgentickÃ¡ AI vykazuje nedostatek transparentnosti, dokumentace a zÃ¡kladnÃ­ch bezpeÄnostnÃ­ch protokolÅ¯.
- PÅ™Ã­klad OpenClaw: framework umoÅ¾Åˆuje agentÅ¯m odesÃ­lat e-maily jmÃ©nem uÅ¾ivatele, ale obsahuje zÃ¡vaÅ¾nÃ© bezpeÄnostnÃ­ zranitelnosti, vÄetnÄ› moÅ¾nosti ÃºplnÃ©ho ovlÃ¡dnutÃ­ osobnÃ­ho poÄÃ­taÄe.
- VÃ½vojÃ¡Å™i AI musÃ­ pÅ™evzÃ­t vÄ›tÅ¡Ã­ odpovÄ›dnost za rizika spojenÃ¡ s touto technologiÃ­.

## Podrobnosti
Studie MIT, publikovanÃ¡ nedÃ¡vno, je jednÃ­m z nejÅ¡irÅ¡Ã­ch prÅ¯zkumÅ¯ agentickÃ½ch AI systÃ©mÅ¯, kterÃ© umoÅ¾ÅˆujÃ­ autonomnÃ­ akce jako interakce s e-maily, prochÃ¡zenÃ­ webu nebo ovlÃ¡dÃ¡nÃ­ zaÅ™Ã­zenÃ­ bez pÅ™Ã­mÃ©ho zÃ¡sahu ÄlovÄ›ka. Tyto systÃ©my, oznaÄovanÃ© jako agentic AI, slibujÃ­ revoluci v automatizaci, ale podle vÃ½zkumnÃ­kÅ¯ trpÃ­ fundamentÃ¡lnÃ­mi bezpeÄnostnÃ­mi chybami. Z 30 prozkoumanÃ½ch platforem vÄ›tÅ¡ina neposkytuje Å¾Ã¡dnÃ© Ãºdaje o testech na rizika, jako je neoprÃ¡vnÄ›nÃ© pÅ™Ã­stup k datÅ¯m nebo sebezlepÅ¡ujÃ­cÃ­ se chovÃ¡nÃ­ agentÅ¯ mimo kontrolu. NapÅ™Ã­klad mnohÃ© nemajÃ­ implementovanÃ½ "kill switch" â€“ nouzovÃ½ mechanismus pro okamÅ¾itÃ© zastavenÃ­ aktivity agenta, coÅ¾ by v pÅ™Ã­padÄ› Ãºtoku mohlo vÃ©st k rozsÃ¡hlÃ½m Å¡kodÃ¡m.

KonkrÃ©tnÃ­m pÅ™Ã­kladem je OpenClaw, open-source software framework vytvoÅ™enÃ½ Peterem Steinbergem, kterÃ©ho nedÃ¡vno najalo OpenAI. OpenClaw umoÅ¾Åˆuje tvorbu agentÅ¯ schopnÃ½ch odesÃ­lat a pÅ™ijÃ­mat e-maily, spravovat Ãºkoly nebo interagovat s operaÄnÃ­m systÃ©mem. MinulÃ½ mÄ›sÃ­c se stal virÃ¡lnÃ­m dÃ­ky svÃ½m pokroÄilÃ½m schopnostem, ale zÃ¡roveÅˆ odhalil dramatickÃ© bezpeÄnostnÃ­ slabiny: agenti mohou zneuÅ¾Ã­t systÃ©m k ÃºplnÃ©mu ovlÃ¡dnutÃ­ uÅ¾ivatelova poÄÃ­taÄe, vÄetnÄ› instalace malware nebo krÃ¡deÅ¾e citlivÃ½ch dat. OpenAI, vedoucÃ­ spoleÄnost v oblasti velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM), tÃ­mto krokem signalizuje zÃ¡mÄ›r integrovat agentickou technologii do svÃ½ch produktÅ¯, jako jsou varianty GPT, coÅ¾ zvyÅ¡uje tlak na Å™eÅ¡enÃ­ tÄ›chto rizik.

VÃ½zkum zdÅ¯razÅˆuje, Å¾e agentickÃ¡ AI nenÃ­ jen o vÃ½konu, ale o absenci standardnÃ­ch protokolÅ¯. Na rozdÃ­l od tradiÄnÃ­ch LLM, kterÃ© generujÃ­ text na poÅ¾Ã¡dÃ¡nÃ­, agenti aktivnÄ› jednajÃ­ v reÃ¡lnÃ©m svÄ›tÄ›, coÅ¾ multiplikuje rizika. MIT vÃ½zkumnÃ­ci kritizujÃ­ developery za nedostatek transparentnosti â€“ napÅ™Ã­klad nejsou veÅ™ejnÄ› dostupnÃ© informace o tom, jak systÃ©my testujÃ­ scÃ©nÃ¡Å™e selhÃ¡nÃ­ nebo jak zabraÅˆujÃ­ eskalaci chyb. Tento stav pÅ™ipomÃ­nÃ¡ ranÃ© dny autonomnÃ­ch vozidel, kde absence regulacÃ­ vedla k nehodÃ¡m, ale zde jde o digitÃ¡lnÃ­ prostÅ™edÃ­ s globÃ¡lnÃ­m dosahem.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tato studie MIT nastavuje varovnÃ½ signÃ¡l pro celÃ½ ekosystÃ©m AI, kde agentickÃ¡ technologie pÅ™echÃ¡zÃ­ z experimentÃ¡lnÃ­ fÃ¡ze do komerÄnÃ­ho nasazenÃ­. Pro uÅ¾ivatele to znamenÃ¡ potenciÃ¡lnÃ­ rizika, jako je ztrÃ¡ta kontroly nad osobnÃ­mi daty nebo zaÅ™Ã­zenÃ­mi, zejmÃ©na pokud agenti integrujÃ­ s cloudovÃ½mi sluÅ¾bami nebo IoT. Pro prÅ¯mysl to podtrhuje nutnost standardizace: bez povinnÃ©ho reportingu bezpeÄnostnÃ­ch testÅ¯ a shutdown mechanismÅ¯ hrozÃ­ regulaÄnÃ­ zÃ¡sahy od orgÃ¡nÅ¯ jako EU AI Act nebo americkÃ© NIST. V Å¡irÅ¡Ã­m kontextu urychluje tlak na odpovÄ›dnÃ© AI â€“ developeÅ™i jako OpenAI, Anthropic nebo Google musÃ­ investovat do robustnÃ­ch bezpeÄnostnÃ­ch rÃ¡mcÅ¯, aby zabrÃ¡nili incidentÅ¯m, kterÃ© by podkopaly dÅ¯vÄ›ru v technologii. Pokud se problÃ©my neÅ™eÅ¡Ã­, agentickÃ¡ AI se mÅ¯Å¾e stÃ¡t katalyzÃ¡torem kybernetickÃ½ch krizÃ­, podobnÄ› jako souÄasnÃ© exploity v LLM.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.zdnet.com/article/ai-agents-are-fast-loose-and-out-of-control-mit-study-find/)

**Zdroj:** ğŸ“° ZDNet
