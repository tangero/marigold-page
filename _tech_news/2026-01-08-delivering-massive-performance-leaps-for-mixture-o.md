---
author: Marisa Aigen
category: ai hardware
companies:
- NVIDIA
date: '2026-01-08 02:43:29'
description: NVIDIA p≈ôedstavuje aktualizace sv√©ho softwaru pro inference na architektu≈ôe
  Blackwell, kter√© v√Ωraznƒõ zvy≈°uj√≠ propustnost token≈Ø u model≈Ø typu smƒõs expert≈Ø (MoE),
  jako je DeepSeek-R1. Tato optimalizace kombinuje hardwareov√© inovace s softwarem
  TensorRT-LLM pro ni≈æ≈°√≠ n√°klady na zpracov√°n√≠.
importance: 4
layout: tech_news_article
original_title: Delivering Massive Performance Leaps for Mixture of Experts Inference
  on NVIDIA Blackwell
publishedAt: '2026-01-08T02:43:29+00:00'
slug: delivering-massive-performance-leaps-for-mixture-o
source:
  emoji: üì∞
  id: null
  name: Nvidia.com
title: Masivn√≠ zlep≈°en√≠ v√Ωkonu inference smƒõsi expert≈Ø na NVIDIA Blackwell
url: https://developer.nvidia.com/blog/delivering-massive-performance-leaps-for-mixture-of-experts-inference-on-nvidia-blackwell/
urlToImage: https://developer-blogs.nvidia.com/wp-content/uploads/2026/01/gb200-nvl-rack-gtc24-press-1920x1080-1.jpg
urlToImageBackup: https://developer-blogs.nvidia.com/wp-content/uploads/2026/01/gb200-nvl-rack-gtc24-press-1920x1080-1.jpg
---

## Souhrn
NVIDIA optimalizuje inference model≈Ø smƒõsi expert≈Ø (MoE) na nov√© architektu≈ôe Blackwell pomoc√≠ aktualizac√≠ softwaru TensorRT-LLM a pln√©ho vyu≈æit√≠ hardwarov√Ωch funkc√≠. Tato ≈ôe≈°en√≠ zvy≈°uj√≠ propustnost token≈Ø na watt v racku GB200 NVL72 s 72 GPU, co≈æ sni≈æuje n√°klady na generov√°n√≠ token≈Ø. Zamƒõ≈ôen√≠ je na modely jako DeepSeek-R1, kter√Ω pat≈ô√≠ mezi ≈°piƒçkov√© ≈ô√≠dic√≠ MoE modely.

## Kl√≠ƒçov√© body
- Rack GB200 NVL72 spojuje 72 Blackwell GPU p≈ôes p√°tou generaci NVLink s propustnost√≠ 1 800 GB/s mezi v≈°emi ƒçipy.
- Hardwareov√° podpora form√°tu NVFP4, ƒçty≈ôbitov√©ho plovouc√≠ho bodu optimalizovan√©ho pro p≈ôesnost.
- Aktualizace TensorRT-LLM zlep≈°uj√≠ v√Ωkon inference pro MoE modely s ƒçast√Ωmi v√Ωmƒõnami dat mezi experty.
- Disagregovan√© zpracov√°n√≠ oddƒõluje prefill od dek√≥dov√°n√≠ pro lep≈°√≠ ≈°k√°lovatelnost.
- Optimalizace prodlu≈æuj√≠ ≈æivotnost st√°vaj√≠c√≠ch NVIDIA GPU v cloudu a enterprise prost≈ôed√≠ch.

## Podrobnosti
Architektura NVIDIA Blackwell, ztƒõlesnƒõn√° v platformƒõ GB200 NVL72, je navr≈æena pro velk√© ≈°k√°ly AI v√Ωpoƒçt≈Ø, zejm√©na pro sparse MoE modely. Tyto modely, jako DeepSeek-R1, rozdƒõluj√≠ zpracov√°n√≠ na specializovan√© experty, co≈æ vy≈æaduje intenzivn√≠ komunikaci mezi nimi p≈ôi generov√°n√≠ token≈Ø. Rack spojuje 72 GPU p≈ôes NVLink Switch ƒçipy p√°t√© generace, kter√© zaji≈°≈•uj√≠ 1 800 GB/s obousmƒõrnou propustnost. Tato vysok√° rychlost komunikace minimalizuje zpo≈ædƒõn√≠ p≈ôi v√Ωmƒõn√°ch dat, co≈æ je kl√≠ƒçov√© pro MoE architektury, kde experti na r≈Øzn√Ωch GPU mus√≠ synchronizovat sv√© v√Ωstupy.

Software TensorRT-LLM, knihovna pro optimalizaci inference velk√Ωch jazykov√Ωch model≈Ø (LLM), nyn√≠ plnƒõ vyu≈æ√≠v√° Blackwellovy schopnosti. Mezi novinky pat≈ô√≠ podpora NVFP4, NVIDIA navr≈æen√©ho form√°tu s ƒçty≈ômi bity pro plovouc√≠ ƒç√≠sla, kter√Ω lep≈°√≠ zachov√°v√° p≈ôesnost oproti standardn√≠m FP4 variant√°m. To umo≈æ≈àuje efektivnƒõj≈°√≠ kvantizaci model≈Ø bez v√Ωrazn√© ztr√°ty kvality. Dal≈°√≠ optimalizace zahrnuj√≠ disaggregated serving, kde f√°ze prefill (zpracov√°n√≠ vstupn√≠ho textu) prob√≠h√° oddƒõlenƒõ od dek√≥dov√°n√≠ (generov√°n√≠ token≈Ø). Tento p≈ô√≠stup umo≈æ≈àuje paralelizaci na vƒõt≈°√≠m poƒçtu GPU a zvy≈°uje celkovou propustnost.

NVIDIA zd≈Øraz≈àuje ko-design nap≈ô√≠ƒç GPU, CPU, s√≠tƒõmi, nap√°jen√≠m a chlazen√≠m, co≈æ zvy≈°uje tokeny na watt. To p≈ô√≠mo sni≈æuje cenu za milion token≈Ø, co≈æ je krit√©rium pro cloudov√© poskytovatele (CSP), jako jsou hyperscale≈ôi. Aktualizace softwaru nav√≠c zlep≈°uj√≠ v√Ωkon i na star≈°√≠m hardware, ƒç√≠m≈æ prodlu≈æuj√≠ jeho vyu≈æitelnost v existuj√≠c√≠ch datacentrech modelov√Ωch tv≈Ørc≈Ø a firem. DeepSeek-R1 slou≈æ√≠ jako testovac√≠ model ‚Äì sparse MoE s pokroƒçil√Ωm uva≈æov√°n√≠m, kde tyto zmƒõny p≈ôin√°≈°ej√≠ v√Ωrazn√© zlep≈°en√≠ oproti p≈ôedchoz√≠m generac√≠m.

## Proƒç je to d≈Øle≈æit√©
Tyto pokroky posiluj√≠ dominanci NVIDIA v AI inference, kde MoE modely rostou v popularitƒõ d√≠ky vy≈°≈°√≠ efektivitƒõ oproti hust√Ωm model≈Øm. Pro pr≈Ømysl znamen√° ni≈æ≈°√≠ provozn√≠ n√°klady a vy≈°≈°√≠ ≈°k√°lovatelnost, co≈æ umo≈æ≈àuje del≈°√≠ provoz st√°vaj√≠c√≠ infrastruktury p≈ôi rostouc√≠ popt√°vce po AI slu≈æb√°ch. U≈æivatel√© od spot≈ôebitel≈Ø po enterprise z√≠skaj√≠ rychlej≈°√≠ odpovƒõdi za ni≈æ≈°√≠ cenu, ale z√°vislost na NVIDIA ekosyst√©mu omezuje volby. V ≈°ir≈°√≠m kontextu urychluj√≠ p≈ôechod k efektivnƒõj≈°√≠m AI syst√©m≈Øm, kde energie a throughput rozhoduj√≠ o konkurenceschopnosti, a posouvaj√≠ hranice toho, co je mo≈æn√© v re√°ln√©m ƒçase s velk√Ωmi modely.

---

[ƒå√≠st p≈Øvodn√≠ ƒçl√°nek](https://developer.nvidia.com/blog/delivering-massive-performance-leaps-for-mixture-of-experts-inference-on-nvidia-blackwell/)

**Zdroj:** üì∞ Nvidia.com
