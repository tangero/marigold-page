---
author: Marisa Aigen
category: ai bezpeÄnost
companies:
- OpenAI
date: '2026-01-04 19:20:36'
description: SpoleÄnost OpenAI Sama Altmana, tvÅ¯rce ChatGPT, vyhlaÅ¡uje novou pozici
  vrcholovÃ©ho manaÅ¾era pro pÅ™ipravenost, kterÃ½ povede ÃºsilÃ­ o studium a mitigaci bezpeÄnostnÃ­ch
  rizik spojenÃ½ch s pokroÄilÃ½mi AI modely. Pozice nabÃ­zÃ­ plat 555 000 dolarÅ¯ plus
  podÃ­ly a zamÄ›Å™uje se na pÅ™Ã­pravu na hraniÄnÃ­ schopnosti AI, kterÃ© mohou vÃ©st k vÃ¡Å¾nÃ½m
  Å¡kodÃ¡m.
importance: 4
layout: tech_news_article
original_title: OpenAI Seeks 'Head of Preparedness' to Tackle Emerging AI Safety Risks
people:
- Sam Altman
publishedAt: '2026-01-04T19:20:36+00:00'
slug: openai-seeks-head-of-preparedness-to-tackle-emergi
source:
  emoji: ğŸ“°
  id: breitbart-news
  name: Breitbart News
title: OpenAI hledÃ¡ 'Head of Preparedness' na Å™eÅ¡enÃ­ novÃ½ch rizik bezpeÄnosti AI
url: https://www.breitbart.com/tech/2026/01/04/openai-seeks-head-of-preparedness-to-tackle-emerging-ai-safety-risks/
urlToImage: https://media.breitbart.com/media/2025/12/Sam-Altman-speaks-on-screen-640x335.jpg
urlToImageBackup: https://media.breitbart.com/media/2025/12/Sam-Altman-speaks-on-screen-640x335.jpg
---

## Souhrn
OpenAI vyhlaÅ¡uje hledÃ¡nÃ­ kandidÃ¡ta na pozici Head of Preparedness, kterÃ½ mÃ¡ vÃ©st firemnÃ­ rÃ¡mec pÅ™ipravenosti na rizika umÄ›lÃ© inteligence. Sam Altman na platformÄ› X zdÅ¯raznil vÃ½zvy jako vliv na mentÃ¡lnÃ­ zdravÃ­ a objevovÃ¡nÃ­ zranitelnostÃ­ v kyberbezpeÄnosti. Tento krok pÅ™ichÃ¡zÃ­ po zmÄ›nÃ¡ch v leadershipu bezpeÄnostnÃ­ho tÃ½mu.

## KlÃ­ÄovÃ© body
- OpenAI nabÃ­zÃ­ plat 555 000 USD plus podÃ­ly za roli Head of Preparedness.
- Ãškol: ProvÃ¡dÄ›t rÃ¡mec pÅ™ipravenosti pro sledovÃ¡nÃ­ hraniÄnÃ­ch schopnostÃ­ AI a pÅ™Ã­pravu na rizika tÄ›Å¾kÃ½ch Å¡kod.
- Altman zdÅ¯razÅˆuje rovnovÃ¡hu mezi posÃ­lenÃ­m obrÃ¡ncÅ¯ kyberbezpeÄnosti a prevencÃ­ zneuÅ¾itÃ­ ÃºtoÄnÃ­ky.
- TÃ½m preparedness byl zaloÅ¾en v roce 2023 pro mitigaci rizik od phishingu po jadernÃ© hrozby.
- BÃ½valÃ½ Å¡Ã©f Aleksander Madry pÅ™eÅ¡el do role zamÄ›Å™enÃ© na uvaÅ¾ovÃ¡nÃ­ AI, dalÅ¡Ã­ bezpeÄnostnÃ­ lÃ­dÅ™i odeÅ¡li.

## Podrobnosti
OpenAI, spoleÄnost za ChatGPT a dalÅ¡Ã­mi velkÃ½mi jazykovÃ½mi modely (LLM), ÄelÃ­ rostoucÃ­m obavÃ¡m z bezpeÄnosti svÃ½ch systÃ©mÅ¯. V pÅ™Ã­spÄ›vku na X Sam Altman popsal, jak pokroÄilÃ© AI modely pÅ™inÃ¡Å¡ejÃ­ rizika v oblastech jako mentÃ¡lnÃ­ zdravÃ­ uÅ¾ivatelÅ¯ â€“ napÅ™Ã­klad prostÅ™ednictvÃ­m manipulativnÃ­ho obsahu generovanÃ©ho AI â€“ nebo schopnost AI odhalovat kritickÃ© zranitelnosti v poÄÃ­taÄovÃ½ch systÃ©mech, coÅ¾ by mohlo urychlit kyberÃºtoky. NovÃ¡ pozice Head of Preparedness mÃ¡ tyto hrozby systematicky sledovat a pÅ™ipravovat na nÄ›, s dÅ¯razem na 'frontier capabilities' â€“ hraniÄnÃ­ schopnosti AI, kterÃ© pÅ™ekraÄujÃ­ souÄasnÃ© limity.

RÃ¡mec pÅ™ipravenosti OpenAI funguje jako strukturovanÃ½ pÅ™Ã­stup k hodnocenÃ­ rizik. Zahrnuje monitorovÃ¡nÃ­ vÃ½voje modelÅ¯, testovÃ¡nÃ­ na potenciÃ¡lnÃ­ katastrofickÃ© scÃ©nÃ¡Å™e a vÃ½voj opatÅ™enÃ­. TÃ½m byl zaloÅ¾en v roce 2023 s cÃ­lem pokrÃ½vat Å¡irokÃ© spektrum rizik: od okamÅ¾itÃ½ch jako phishingovÃ© Ãºtoky generovanÃ© AI aÅ¾ po spekulativnÃ­ jako biologickÃ© zbranÄ› navrÅ¾enÃ© algoritmy nebo eskalace jadernÃ½ch konfliktÅ¯ dÃ­ky AI rozhodovÃ¡nÃ­. NicmÃ©nÄ› leadership proÅ¡el zmÄ›nami â€“ Aleksander Madry, pÅ¯vodnÃ­ Head of Preparedness a expert na robustnÃ­ AI, byl pÅ™eÅ™azen do oblasti AI reasoning, coÅ¾ zahrnuje zlepÅ¡ovÃ¡nÃ­ logickÃ©ho uvaÅ¾ovÃ¡nÃ­ modelÅ¯. DalÅ¡Ã­ bezpeÄnostnÃ­ manaÅ¾eÅ™i buÄ opustili firmu, nebo zmÄ›nili role, coÅ¾ signalizuje nestabilitu v tÃ©to klÃ­ÄovÃ© oblasti.

IdealnÃ­ kandidÃ¡t podle Altmana musÃ­ zvlÃ¡dat rovnovÃ¡hu: poskytovat AI nÃ¡stroje pro kyberbezpeÄnostnÃ­ obrÃ¡nce, jako detekci zranitelnostÃ­ rychlejÅ¡Ã­ neÅ¾ u ÄlovÄ›ka, zatÃ­mco brÃ¡nit ÃºtoÄnÃ­kÅ¯m v zneuÅ¾itÃ­ stejnÃ½ch technologiÃ­. Pozice je vypsÃ¡na s atraktivnÃ­m balÃ­Äkem, coÅ¾ odrÃ¡Å¾Ã­ prioritu OpenAI na pÅ™ilÃ¡kÃ¡nÃ­ top talentu v dobÄ›, kdy konkurence jako Anthropic nebo Google DeepMind posilujÃ­ svÃ© bezpeÄnostnÃ­ tÃ½my.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tato personÃ¡lnÃ­ zmÄ›na podtrhuje eskalaci tlaku na AI firmy v oblasti bezpeÄnosti. OpenAI, jako lÃ­dr v LLM, ovlivÅˆuje miliardy uÅ¾ivatelÅ¯ pÅ™es ChatGPT, a selhÃ¡nÃ­ v preparedness by mohlo vÃ©st k regulacÃ­m jako EU AI Act nebo zÃ¡konÅ¯m v USA. Pro prÅ¯mysl znamenÃ¡ posÃ­lenÃ­ duality: AI urychluje kyberbezpeÄnost (napÅ™. automatizovanÃ¡ detekce exploitÅ¯), ale zvyÅ¡uje rizika pro ÃºtoÄnÃ­ky s pÅ™Ã­stupem k otevÅ™enÃ½m modelÅ¯m. V Å¡irÅ¡Ã­m kontextu, kde AGI blÃ­Å¾Ã­, je stabilnÃ­ leadership klÃ­Äem k prevenci 'catastrophic risks'. Kriticky: ÄastÃ© odchody z OpenAI safety tÃ½mu zpochybÅˆujÃ­ dlouhodobou zÃ¡vazek firmy k bezpeÄnosti oproti rychlÃ©mu nasazenÃ­ modelÅ¯, coÅ¾ mÅ¯Å¾e oslabit dÅ¯vÄ›ru investorÅ¯ a veÅ™ejnosti.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.breitbart.com/tech/2026/01/04/openai-seeks-head-of-preparedness-to-tackle-emerging-ai-safety-risks/)

**Zdroj:** ğŸ“° Breitbart News
