---
author: Marisa Aigen
category: bezpeÄnost ai
companies:
- OpenAI
date: '2025-12-29 19:29:02'
description: SpoleÄnost OpenAI vyhlaÅ¡uje vÃ½bÄ›r na pozici Å¡Ã©fa pÅ™ipravenosti s roÄnÃ­m
  platem 555 000 dolarÅ¯ plus podÃ­ly, aby Å™eÅ¡ila rizika spojenÃ¡ s AI, jako je duÅ¡evnÃ­
  zdravÃ­ uÅ¾ivatelÅ¯ a kyberbezpeÄnost. Tento krok pÅ™ichÃ¡zÃ­ v dobÄ› rostoucÃ­ch obav z
  reputaÄnÃ­ch a operaÄnÃ­ch rizik AI u stovek firem.
importance: 4
layout: tech_news_article
original_title: OpenAI is hiring a â€˜head of preparednessâ€™ with a $550,000 salary to
  mitigate AI dangers that CEO Sam Altman warns will be â€˜stressfulâ€™
people:
- Sam Altman
publishedAt: '2025-12-29T19:29:02+00:00'
slug: openai-is-hiring-a-head-of-preparedness-with-a-550
source:
  emoji: ğŸ“°
  id: fortune
  name: Fortune
title: OpenAI hledÃ¡ Å¡Ã©fa pÅ™ipravenosti s platem 550 000 dolarÅ¯ na zmÃ­rnÄ›nÃ­ nebezpeÄÃ­
  AI, kterÃ¡ CEO Sam Altman oznaÄuje za stresujÃ­cÃ­
url: https://fortune.com/2025/12/29/openai-hiring-head-of-preparedness-550000-salary-ai-safety-risks-sam-altman/
urlToImage: https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2236543888_895bdb-e1767034730660.jpg?resize=1200,600
urlToImageBackup: https://fortune.com/img-assets/wp-content/uploads/2025/12/GettyImages-2236543888_895bdb-e1767034730660.jpg?resize=1200,600
---

## Souhrn
OpenAI hledÃ¡ kandidÃ¡ta na pozici Å¡Ã©fa pÅ™ipravenosti, kterÃ½ se zamÄ›Å™Ã­ na minimalizaci Å¡kodlivÃ½ch dopadÅ¯ AI, vÄetnÄ› rizik pro kyberbezpeÄnost a duÅ¡evnÃ­ zdravÃ­ uÅ¾ivatelÅ¯. CEO Sam Altman tento post popsal jako vysoce stresujÃ­cÃ­ s okamÅ¾itÃ½m nÃ¡stupem do praxe a nabÃ­zÃ­ plat 555 000 dolarÅ¯ roÄnÄ› plus podÃ­ly. Tento krok reaguje na rychlÃ½ pokrok modelÅ¯ AI a souvisejÃ­cÃ­ vÃ½zvy.

## KlÃ­ÄovÃ© body
- Plat: 555 000 USD roÄnÄ› plus equity.
- ZamÄ›Å™enÃ­: KyberbezpeÄnost, biologickÃ© schopnosti, sebezlepÅ¡ujÃ­cÃ­ se systÃ©my a duÅ¡evnÃ­ zdravÃ­.
- Kontext: NÃ¡stupce Aleksandra Madryho, kterÃ½ byl pÅ™eÅ™azen na roli spojenou s uvaÅ¾ovÃ¡nÃ­m AI.
- AltmanÅ¯v komentÃ¡Å™: Role je stresujÃ­cÃ­ a vyÅ¾aduje okamÅ¾itÃ© ponoÅ™enÃ­ do problÃ©mÅ¯.
- Å irÅ¡Ã­ trend: 418 firem s hodnotou nad 1 miliardu USD zmÃ­nilo AI rizika v SEC zprÃ¡vÃ¡ch, nÃ¡rÅ¯st o 46 % oproti roku 2024.

## Podrobnosti
OpenAI, zaloÅ¾enÃ¡ v roce 2015 jako neziskovÃ¡ organizace s cÃ­lem vyuÅ¾Ã­t AI ve prospÄ›ch lidstva, nynÃ­ intenzivnÄ› Å™eÅ¡Ã­ bezpeÄnostnÃ­ vÃ½zvy spojenÃ© s rychlÃ½m vÃ½vojem svÃ½ch modelÅ¯. Pozice Å¡Ã©fa pÅ™ipravenosti (head of preparedness) mÃ¡ koordinovat pÅ™Ã­stupy k rizikÅ¯m, jako je zneuÅ¾itÃ­ AI pro kybernetickÃ© Ãºtoky, kde by ÃºtoÄnÃ­ci mohli vyuÅ¾Ã­t pokroÄilÃ© schopnosti modelÅ¯ k prolomenÃ­ bezpeÄnostnÃ­ch systÃ©mÅ¯, zatÃ­mco obrÃ¡nci by mÄ›li zÃ­skat nÃ¡stroje pro lepÅ¡Ã­ detekci a obranu. DalÅ¡Ã­m klÃ­ÄovÃ½m bodem je uvolÅˆovÃ¡nÃ­ biologickÃ½ch schopnostÃ­, coÅ¾ zahrnuje prevenci zneuÅ¾itÃ­ AI v tvorbÄ› biologickÃ½ch zbranÃ­ nebo nebezpeÄnÃ½ch patogenÅ¯, a posouzenÃ­ bezpeÄnosti systÃ©mÅ¯ schopnÃ½ch sebezlepÅ¡ovÃ¡nÃ­, kterÃ© by mohly pÅ™ekroÄit kontrolu lidskÃ½ch operÃ¡torÅ¯.

Sam Altman ve svÃ©m pÅ™Ã­spÄ›vku na platformÄ› X zdÅ¯raznil, Å¾e modely AI rychle zlepÅ¡ujÃ­ svÃ© schopnosti, coÅ¾ pÅ™inÃ¡Å¡Ã­ nejen vÃ½hody, ale i reÃ¡lnÃ© vÃ½zvy. PÅ™edchozÃ­ Å¡Ã©f pÅ™ipravenosti Aleksander Madry, expert na robustnÃ­ AI systÃ©my, byl loni pÅ™eÅ™azen na roli zamÄ›Å™enou na uvaÅ¾ovÃ¡nÃ­ AI (AI reasoning), kde bezpeÄnost zÅ¯stÃ¡vÃ¡ souÄÃ¡stÃ­ ÃºkolÅ¯. AnalÃ½za spoleÄnosti AlphaSense z listopadu ukÃ¡zala, Å¾e v prvnÃ­ch 11 mÄ›sÃ­cÃ­ch roku 2024 zmÃ­nilo 418 firem s trÅ¾nÃ­ kapitalizacÃ­ nad 1 miliardu USD v SEC zprÃ¡vÃ¡ch reputaÄnÃ­ rizika spojenÃ¡ s AI, jako jsou zkreslenÃ© datasety vedoucÃ­ k diskriminaci nebo bezpeÄnostnÃ­ propusti. Tento nÃ¡rÅ¯st o 46 % oproti pÅ™edchozÃ­mu roku signalizuje Å¡Ã­Å™Ã­cÃ­ se obavy v korporÃ¡tnÃ­m sektoru.

Pro uÅ¾ivatele to znamenÃ¡ potÅ™ebu robustnÄ›jÅ¡Ã­ch bezpeÄnostnÃ­ch mechanismÅ¯ v nÃ¡strojÃ­ch jako ChatGPT, kde by AI mÄ›la detekovat a blokovat Å¡kodlivÃ© poÅ¾adavky. V prÅ¯myslu to podtrhuje posun od rychlÃ©ho nasazovÃ¡nÃ­ k prioritizaci bezpeÄnosti, coÅ¾ mÅ¯Å¾e zpÅ¯sobit zpomalenÃ­ vÃ½voje novÃ½ch verzÃ­ modelÅ¯, jako je GPT-5.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tato nabÃ­dka odrÃ¡Å¾Ã­ eskalaci v AI bezpeÄnostnÃ­m diskurzu, kde OpenAI pÅ™iznÃ¡vÃ¡, Å¾e pokrok v modelech jako GPT pÅ™inÃ¡Å¡Ã­ nejen produktivitu, ale i existenciÃ¡lnÃ­ rizika. V Å¡irÅ¡Ã­m ekosystÃ©mu to ovlivnÃ­ regulace, jako nadchÃ¡zejÃ­cÃ­ EU AI Act, a donutÃ­ konkurenty jako Anthropic nebo Google DeepMind posÃ­lit svÃ© bezpeÄnostnÃ­ tÃ½my. Pro prÅ¯mysl to znamenÃ¡ vyÅ¡Å¡Ã­ nÃ¡klady na compliance a potenciÃ¡lnÃ­ zpomalenÃ­ inovacÃ­, zatÃ­mco pro uÅ¾ivatele posÃ­lÃ­ dÅ¯vÄ›ru v AI nÃ¡stroje, pokud bude pÅ™ipravenost efektivnÃ­. Kriticky Å™eÄeno, absence takovÃ½ch rolÃ­ by mohla vÃ©st k opakovanÃ½m incidentÅ¯m, jako byly minulÃ© Ãºniky dat nebo halucinace modelÅ¯ vedoucÃ­ k dezinformacÃ­m.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://fortune.com/2025/12/29/openai-hiring-head-of-preparedness-550000-salary-ai-safety-risks-sam-altman/)

**Zdroj:** ğŸ“° Fortune
