---
author: Marisa Aigen
category: umÄ›lÃ¡ inteligence
companies:
- Google
date: '2025-11-18 05:02:14'
description: Sundar Pichai, generÃ¡lnÃ­ Å™editel Alphabetu, upozornil, Å¾e aktuÃ¡lnÃ­ modely
  umÄ›lÃ© inteligence jsou nÃ¡chylnÃ© k chybÃ¡m, a vyzval uÅ¾ivatele, aby je nepouÅ¾Ã­vali
  jako jedinÃ½ zdroj informacÃ­.
importance: 3
layout: tech_news_article
original_title: Don't blindly trust what AI tells you, says Google's Sundar Pichai
people:
- Sundar Pichai
publishedAt: '2025-11-18T05:02:14+00:00'
slug: dont-blindly-trust-what-ai-tells-you-says-googles-
source:
  emoji: ğŸ“°
  id: null
  name: BBC News
title: NespolÃ©hejte slepÄ› na to, co vÃ¡m Å™Ã­kÃ¡ umÄ›lÃ¡ inteligence, varuje Å¡Ã©f Googlu
  Sundar Pichai
url: https://www.bbc.com/news/articles/c8drzv37z4jo
urlToImage: https://ichef.bbci.co.uk/news/1024/branded_news/a501/live/d35ed1f0-c3e1-11f0-bbb8-f506e20a7d44.jpg
urlToImageBackup: https://ichef.bbci.co.uk/news/1024/branded_news/a501/live/d35ed1f0-c3e1-11f0-bbb8-f506e20a7d44.jpg
---

## Souhrn
Sundar Pichai, Å¡Ã©f mateÅ™skÃ© spoleÄnosti Google Alphabet, veÅ™ejnÄ› pÅ™iznal, Å¾e nÃ¡stroje umÄ›lÃ© inteligence (AI) mohou poskytovat nepÅ™esnÃ© nebo zcela faleÅ¡nÃ© informace. V rozhovoru pro BBC doporuÄil uÅ¾ivatelÅ¯m nespolÃ©hat se na AI slepÄ› a mÃ­sto toho ji kombinovat s jinÃ½mi, ovÄ›Å™enÃ½mi zdroji, jako je klasickÃ© vyhledÃ¡vÃ¡nÃ­ na Google.

## KlÃ­ÄovÃ© body
- AI modely jsou nÃ¡chylnÃ© k chybÃ¡m a mohou â€vymÃ½Å¡letâ€œ odpovÄ›di, aby uspokojily uÅ¾ivatele.
- Google doporuÄuje pouÅ¾Ã­vat AI jako doplnÄ›k, nikoli nÃ¡hradu pro tradiÄnÃ­ vyhledÃ¡vÃ¡nÃ­.
- SpoleÄnost zobrazuje u svÃ½ch AI nÃ¡strojÅ¯ upozornÄ›nÃ­ na moÅ¾nost chyb.
- NedÃ¡vnÃ½ zavÃ¡hÃ¡nÃ­ funkce AI Overviews v Google Search vyvolalo kritiku kvÅ¯li absurdnÃ­m odpovÄ›dÃ­m.
- OdbornÃ­ci kritizujÃ­, Å¾e mÃ­sto vÃ½zvy k manuÃ¡lnÃ­mu ovÄ›Å™ovÃ¡nÃ­ by mÄ›ly firmy zlepÅ¡ovat spolehlivost svÃ½ch systÃ©mÅ¯.

## Podrobnosti
Pichai v rozhovoru pro BBC zdÅ¯raznil, Å¾e i pÅ™es rozsÃ¡hlÃ© ÃºsilÃ­ o pÅ™esnost jsou souÄasnÃ© generativnÃ­ modely AI stÃ¡le â€nÃ¡chylnÃ© k chybÃ¡mâ€œ. KonkrÃ©tnÄ› zmÃ­nil, Å¾e AI Äasto â€vymÃ½Å¡lÃ­â€œ odpovÄ›di, aby znÄ›ly pÅ™esvÄ›dÄivÄ› a uspokojily uÅ¾ivatele â€“ jev znÃ¡mÃ½ jako â€halucinaceâ€œ. Tento problÃ©m se projevil napÅ™Ã­klad pÅ™i spuÅ¡tÄ›nÃ­ funkce AI Overviews v Google Search, kde systÃ©m doporuÄoval uÅ¾ivatelÅ¯m nebezpeÄnÃ© nebo absurdnÃ­ tipy, jako je pitÃ­ lepidla nebo jÃ­dlo desek. Google nÃ¡slednÄ› funkci upravil, ale incident ukÃ¡zal limity dneÅ¡nÃ­ch LLM (large language models).

SpoleÄnost proto na svÃ½ch AI nÃ¡strojÃ­ch, jako je Gemini, zobrazuje disclaimery upozorÅˆujÃ­cÃ­ na moÅ¾nost nepÅ™esnostÃ­. Pichai zÃ¡roveÅˆ obhajuje tradiÄnÃ­ vyhledÃ¡vaÄ Google Search jako â€vÃ­ce zakotvenÃ½â€œ v ovÄ›Å™enÃ½ch datech. NicmÃ©nÄ› kritici, jako profesorka Gina Neff z Oxfordu, namÃ­tajÃ­, Å¾e zodpovÄ›dnost za ovÄ›Å™ovÃ¡nÃ­ informacÃ­ by nemÄ›la bÃ½t pÅ™esouvÃ¡na na uÅ¾ivatele â€“ mÃ­sto toho by mÄ›ly technologickÃ© firmy investovat do spolehlivÄ›jÅ¡Ã­ch architektur a lepÅ¡Ã­ho trÃ©novÃ¡nÃ­ modelÅ¯.

## ProÄ je to dÅ¯leÅ¾itÃ©
Pichaiho vÃ½rok odhaluje zÃ¡sadnÃ­ napÄ›tÃ­ v souÄasnÃ© fÃ¡zi rozvoje AI: technologie je Å¡iroce dostupnÃ¡, ale stÃ¡le nezralÃ¡ pro kritickÃ© aplikace. ZatÃ­mco firmy jako Google propagujÃ­ AI jako revoluÄnÃ­ nÃ¡stroj, zÃ¡roveÅˆ pÅ™iznÃ¡vajÃ­ jejÃ­ zÃ¡vaÅ¾nÃ© nedostatky. Tento rozpor mÃ¡ dopad na dÅ¯vÄ›ru uÅ¾ivatelÅ¯ i na regulaci â€“ EvropskÃ¡ unie i USA zvaÅ¾ujÃ­ pÅ™Ã­snÄ›jÅ¡Ã­ pravidla pro transparentnost a odpovÄ›dnost poskytovatelÅ¯ AI. Pro bÄ›Å¾nÃ© uÅ¾ivatele to znamenÃ¡, Å¾e by mÄ›li AI pouÅ¾Ã­vat spÃ­Å¡e jako inspiraÄnÃ­ nebo kreativnÃ­ nÃ¡stroj, nikoli jako autoritu v otÃ¡zkÃ¡ch faktÅ¯, zdravÃ­ nebo bezpeÄnosti.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.bbc.com/news/articles/c8drzv37z4jo)

**Zdroj:** ğŸ“° BBC News
