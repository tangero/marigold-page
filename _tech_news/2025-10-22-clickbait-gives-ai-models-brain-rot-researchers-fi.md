---
category: ai
date: '2025-10-22 12:55:57'
description: Výzkum ukazuje, že trénování velkých jazykových modelů na nekvalitních
  datech ze sociálních sítí vede k měřitelnému poklesu jejich kognitivních schopností.
importance: 3
layout: tech_news_article
original_title: Clickbait Gives AI Models ‘Brain Rot,’ Researchers Find - Gizmodo
publishedAt: '2025-10-22T12:55:57+00:00'
slug: clickbait-gives-ai-models-brain-rot-researchers-fi
source:
  emoji: 📰
  id: null
  name: Gizmodo.com
title: Clickbait a spam způsobují AI modelům 'degradaci schopností', zjistili vědci
url: https://gizmodo.com/clickbait-gives-ai-models-brain-rot-researchers-find-2000675101
urlToImage: https://gizmodo.com/app/uploads/2025/10/llm_AI_brain_rot-1200x675.jpg
urlToImageBackup: https://gizmodo.com/app/uploads/2025/10/llm_AI_brain_rot-1200x675.jpg
---

## Souhrn

Výzkumný tým z amerických univerzit prokázal, že trénování velkých jazykových modelů (LLM) na nekvalitních datech ze sociálních sítí vede k výraznému zhoršení jejich výkonu. Studie nazvaná "LLM Brain Rot Hypothesis" testovala čtyři různé modely na datech z platformy X a zjistila pokles v logickém uvažování, porozumění kontextu i dodržování bezpečnostních standardů.

## Klíčové body

- Výzkumníci z Texas A&M University, University of Texas at Austin a Purdue University identifikovali dva typy "nekvalitních" dat: krátké příspěvky ze sociálních sítí s vysokou mírou interakcí a delší obsah s clickbaitovými titulky
- Testovány byly čtyři modely: Llama3 8B, Qwen2.5 7B/0.5B a Qwen3 4B na vzorku jednoho milionu příspěvků z platformy X
- Meta Llama3 prokázal nejvyšší citlivost na nekvalitní data, zatímco menší model Qwen3 4B byl odolnější
- Vyšší podíl nekvalitních dat vedl k častějšímu "no thinking" režimu, kdy model neposkytuje zdůvodnění své odpovědi
- Všechny testované modely zaznamenaly měřitelný pokles kognitivních schopností

## Podrobnosti

Výzkumníci definovali dva základní typy problematických trénovacích dat. První kategorii tvoří krátké příspěvky ze sociálních sítí, které mají vysokou míru zapojení uživatelů formou lajků a sdílení, ale nízkou informační hodnotu. Druhou kategorii představuje delší obsah charakteristický senzacechtivými titulky, přehnanou prezentací a povrchní úrovní skutečných informací.

Pro experiment výzkumníci sesbírali vzorek jednoho milionu příspěvků z platformy X a trénovali čtyři různé jazykové modely na různých poměrech kontrolních dat a nekvalitního obsahu. Výsledky jednoznačně prokázaly negativní dopad na všechny testované modely.

Nejvýraznější pokles zaznamenal model Llama3 8B od společnosti Meta, který vykazoval zhoršení v oblasti logického uvažování, porozumění kontextu a dodržování bezpečnostních protokolů. Překvapivě se jako odolnější ukázal menší model Qwen3 4B, i když ani ten nebyl imunní vůči degradaci schopností.

Zajímavým zjištěním je, že s rostoucím podílem nekvalitních dat se modely častěji dostávaly do režimu, kdy neposkytovaly žádné zdůvodnění svých odpovědí. Tyto nezdůvodněné odpovědi byly zároveň s vyšší pravděpodobností nesprávné, což naznačuje přímou souvislost mezi kvalitou trénovacích dat a spolehlivostí výstupů.

## Proč je to důležité

Tato studie přichází v době, kdy se velké jazykové modely stále více spoléhají na automatizované sběry dat z internetu pro své trénování. S rostoucím množstvím AI-generovaného obsahu a clickbaitových materiálů na webu se problém nekvalitních trénovacích dat může v budoucnu ještě zhoršovat.

Výzkum má přímé důsledky pro vývoj AI systémů a ukazuje na nutnost pečlivější kurátorské práce s trénovacími daty. Firmy vyvíjející jazykové modely budou muset investovat více prostředků do filtrování a ověřování kvality zdrojových dat, pokud chtějí udržet nebo zlepšit výkon svých modelů. Zjištění také vyvolává otázky o dlouhodobé udržitelnosti současného přístupu k trénování AI, kdy se modely učí z čím dál většího množství nekvalitního obsahu generovaného jinými AI systémy.

---

[Číst původní článek](https://gizmodo.com/clickbait-gives-ai-models-brain-rot-researchers-find-2000675101)

**Zdroj:** 📰 Gizmodo.com
