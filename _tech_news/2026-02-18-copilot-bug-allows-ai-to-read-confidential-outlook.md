---
author: Marisa Aigen
category: kyberbezpeÄnost
companies:
- Microsoft
date: '2026-02-18 16:22:12'
description: Asistent Microsoft Copilot v Microsoft 365 mÃ¡ kritickou chybu, kterÃ¡
  umoÅ¾Åˆuje umÄ›lÃ© inteligenci skenovat a shrnovat e-maily oznaÄenÃ© jako dÅ¯vÄ›rnÃ© v Outlooku,
  vÄetnÄ› sloÅ¾ek OdeslanÃ© a Koncepty. Microsoft zavÃ¡dÃ­ opravu, ale nenÃ­ jasnÃ½ ÄasovÃ½
  rÃ¡mec, coÅ¾ zvyÅ¡uje obavy z ochrany soukromÃ­ a spolehlivosti AI.
importance: 5
layout: tech_news_article
original_title: Copilot bug allows â€˜AIâ€™ to read confidential Outlook emails
publishedAt: '2026-02-18T16:22:12+00:00'
slug: copilot-bug-allows-ai-to-read-confidential-outlook
source:
  emoji: ğŸ“°
  id: null
  name: PCWorld
title: Chyba v Copilot umoÅ¾Åˆuje AI ÄÃ­st dÅ¯vÄ›rnÃ© e-maily v Outlooku
url: https://www.pcworld.com/article/3064782/copilot-bug-allows-ai-to-read-confidential-outlook-emails.html
urlToImage: https://www.pcworld.com/wp-content/uploads/2026/02/image_930daa.png?w=1024
urlToImageBackup: https://www.pcworld.com/wp-content/uploads/2026/02/image_930daa.png?w=1024
---

## Souhrn
Microsoft Copilot, AI asistent integrovanÃ½ do Microsoft 365, trpÃ­ chybou oznaÄenou jako CW1226324, kterÃ¡ umoÅ¾Åˆuje skenovat a shrnovat e-maily oznaÄenÃ© jako dÅ¯vÄ›rnÃ© v aplikaci Outlook. Tato chyba obchÃ¡zÃ­ ochrannÃ© mechanismy urÄenÃ© k blokovÃ¡nÃ­ automatizovanÃ½ch nÃ¡strojÅ¯ a postihuje sloÅ¾ky OdeslanÃ© a Koncepty. Microsoft potvrzuje problÃ©m a zavÃ¡dÃ­ opravu pro postiÅ¾enÃ© ÃºÄty, avÅ¡ak bez specifikace, kdy bude dostupnÃ¡ pro vÅ¡echny uÅ¾ivatele.

## KlÃ­ÄovÃ© body
- **Chyba CW1226324**: Copilot Chat v Microsoft 365 Äte dÅ¯vÄ›rnÃ© e-maily v Outlooku navzdory oznaÄenÃ­, kterÃ© mÃ¡ brÃ¡nit automatizovanÃ©mu zpracovÃ¡nÃ­.
- **PostiÅ¾enÃ© sloÅ¾ky**: PrimÃ¡rnÄ› OdeslanÃ© (Sent) a Koncepty (Drafts), kde se Äasto uklÃ¡dajÃ­ citlivÃ¡ data jako smlouvy nebo lÃ©kaÅ™skÃ© informace.
- **Oprava v procesu**: Microsoft ji postupnÄ› nasazuje, ale rozsah dopadu se mÅ¯Å¾e mÄ›nit a plnÃ½ report je viditelnÃ½ jen pro administrÃ¡tory Microsoft 365.
- **Å½Ã¡dnÃ© Ãºdaje o poÄtu uÅ¾ivatelÅ¯**: Firma neuvÃ¡dÃ­, kolik ÃºÄtÅ¯ je ovlivnÄ›no, coÅ¾ zvyÅ¡uje nejistotu.
- **Zdroj**: ZprÃ¡va pochÃ¡zÃ­ pÅ™Ã­mo od Microsoftu, shrnuto na PCWorld a BleepingComputer.

## Podrobnosti
Chyba CW1226324 odhaluje fundamentÃ¡lnÃ­ slabinu v integraci AI do produktivnÃ­ch nÃ¡strojÅ¯ jako je Microsoft 365. Copilot, coÅ¾ je AI asistent zaloÅ¾enÃ½ na velkÃ½ch jazykovÃ½ch modelech (LLM), slouÅ¾Ã­ k automatizaci ÃºkolÅ¯ jako shrnutÃ­ e-mailÅ¯, generovÃ¡nÃ­ odpovÄ›dÃ­ nebo analÃ½za obsahu. V tomto pÅ™Ã­padÄ› vÅ¡ak ignoruje oznaÄenÃ­ â€dÅ¯vÄ›rnÃ©â€œ (confidential), kterÃ© je v Outlooku navrÅ¾eno specificky pro blokovÃ¡nÃ­ skenovÃ¡nÃ­ automatizovanÃ½mi systÃ©my, vÄetnÄ› AI. To znamenÃ¡, Å¾e Copilot mÅ¯Å¾e naÄÃ­st obsah e-mailÅ¯ obsahujÃ­cÃ­ch obchodnÃ­ smlouvy, prÃ¡vnÃ­ korespondenci, Ãºdaje z policejnÃ­ch vyÅ¡etÅ™ovÃ¡nÃ­ nebo osobnÃ­ zdravotnÃ­ informace.

ProblÃ©m se projevuje v Copilot Chat, coÅ¾ je rozhranÃ­ pro konverzaci s AI pÅ™Ã­mo v Microsoft 365 aplikacÃ­ch. UÅ¾ivatelÃ©, kteÅ™Ã­ aktivujÃ­ funkci shrnutÃ­ e-mailÅ¯, riskujÃ­ nechtÄ›nÃ© zpracovÃ¡nÃ­ citlivÃ½ch dat. Tyto data mohou bÃ½t nÃ¡slednÄ› zpracovÃ¡na modelem, potenciÃ¡lnÄ› uloÅ¾ena v logÃ¡ch nebo â€“ v nejhorÅ¡Ã­m pÅ™Ã­padÄ› â€“ pouÅ¾ita pro trÃ©nink modelÅ¯, jak je bÄ›Å¾nÃ© u cloudovÃ½ch AI sluÅ¾eb. Microsoft sice tvrdÃ­, Å¾e data nejsou automaticky posÃ­lÃ¡na do trÃ©ninkovÃ½ch sad, ale absence transparentnosti v bug reportu (dostupnÃ©m jen pro adminy) brÃ¡nÃ­ plnÃ©mu posouzenÃ­ rizik.

Oprava je nasazovÃ¡na postupnÄ›, ale bez ÄasovÃ©ho rÃ¡mce. To je typickÃ© pro enterprise prostÅ™edÃ­, kde aktualizace probÃ­hajÃ­ v waves podle tenantÅ¯. ZatÃ­mco bÄ›Å¾nÃ­ uÅ¾ivatelÃ© nemohou report prohlÃ©dnout, zdroje jako BleepingComputer a PCWorld potvrzujÃ­ detaily na zÃ¡kladÄ› uniklÃ½ch informacÃ­. Tento incident navazuje na pÅ™edchozÃ­ kontroverze kolem Copilotu, jako je nechtÄ›nÃ© sdÃ­lenÃ­ dat mezi uÅ¾ivateli nebo nesprÃ¡vnÃ© shrnutÃ­, coÅ¾ ukazuje na nezralost AI v citlivÃ½ch prostÅ™edÃ­ch.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tato chyba podtrhuje systÃ©movÃ© rizika integrace AI do firemnÃ­ch systÃ©mÅ¯, kde dÅ¯vÄ›rnost dat je klÃ­ÄovÃ¡. V Å¡irÅ¡Ã­m kontextu posiluje skepsi vÅ¯Äi spolehlivosti LLM v produktech gigantÅ¯ jako Microsoft, kde AI slibuje produktivitu, ale Äasto selhÃ¡vÃ¡ v bezpeÄnostnÃ­ch zÃ¡kladÅ¯. Pro uÅ¾ivatele Microsoft 365 to znamenÃ¡ nutnost okamÅ¾itÃ©ho omezenÃ­ pÅ™Ã­stupu Copilotu k dÅ¯vÄ›rnÃ½m sloÅ¾kÃ¡m, zatÃ­mco firmy ÄekajÃ­ na fix. V kyberbezpeÄnostnÃ­m ekosystÃ©mu to zvyÅ¡uje tlak na regulace jako EU AI Act, kterÃ½ vyÅ¾aduje transparentnost u vysokorizikovÃ½ch AI systÃ©mÅ¯. Pokud se rozsah dopadu rozÅ¡Ã­Å™Ã­, mÅ¯Å¾e to vÃ©st k Å¾alobÃ¡m nebo ztrÃ¡tÄ› dÅ¯vÄ›ry, coÅ¾ brzdÃ­ adopci AI v podnicÃ­ch. CelkovÄ› to ilustruje, proÄ bezpeÄnostnÃ­ objevy v AI patÅ™Ã­ mezi prÅ¯lomovÃ© â€“ odhalujÃ­ chyby, kterÃ© mohou mÃ­t reÃ¡lnÃ© dopady na miliardy uÅ¾ivatelÅ¯.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.pcworld.com/article/3064782/copilot-bug-allows-ai-to-read-confidential-outlook-emails.html)

**Zdroj:** ğŸ“° PCWorld
