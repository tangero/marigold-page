---
author: Marisa Aigen
category: ai etika
companies:
- OpenAI
- TikTok
- Apple
- Google
- Microsoft
date: '2025-11-07 15:38:28'
description: Na platformÃ¡ch X a TikTok se objevujÃ­ ÃºÄty systematicky publikujÃ­cÃ­ AI
  generovanÃ¡ videa znÃ¡zorÅˆujÃ­cÃ­ Å¡krcenÃ­ Å¾en a dÃ­vek, coÅ¾ ukazuje selhÃ¡nÃ­ ochrannÃ½ch
  mechanismÅ¯ generativnÃ­ch modelÅ¯ a platforem pÅ™i vymÃ¡hÃ¡nÃ­ vlastnÃ­ch zÃ¡sad proti nÃ¡silÃ­.
importance: 3
layout: tech_news_article
original_title: OpenAIâ€™s Sora 2 Floods Social Media With Videos of Women Being Strangled
  - 404 Media
publishedAt: '2025-11-07T15:38:28+00:00'
slug: openais-sora-2-floods-social-media-with-videos-of-
source:
  emoji: ğŸ“°
  id: null
  name: 404media.co
title: OpenAI Sora zaplavuje sociÃ¡lnÃ­ sÃ­tÄ› nÃ¡silnÃ½mi videi generovanÃ½mi AI
url: https://www.404media.co/openais-sora-2-floods-social-media-with-videos-of-women-being-strangled/
urlToImage: https://www.404media.co/content/images/size/w1200/2025/11/image4.png
urlToImageBackup: https://www.404media.co/content/images/size/w1200/2025/11/image4.png
---

## Souhrn
SociÃ¡lnÃ­ sÃ­tÄ› X a TikTok ÄelÃ­ vlnÄ› krÃ¡tkÃ½ch videÃ­, kterÃ¡ pomocÃ­ generativnÃ­ AI realisticky zobrazujÃ­ Å¡krcenÃ­ Å¾en a dÃ­vek, Äasto stylizovanÃ½ch jako nezletilÃ©. Tyto pÅ™Ã­pady ukazujÃ­, Å¾e souÄasnÃ¡ bezpeÄnostnÃ­ opatÅ™enÃ­ u nÃ¡strojÅ¯ pro generovÃ¡nÃ­ videa, vÄetnÄ› modelÅ¯ typu Sora, nejsou schopna ÃºÄinnÄ› zabrÃ¡nit tvorbÄ› explicitnÄ› nÃ¡silnÃ©ho a potenciÃ¡lnÄ› nezÃ¡konnÃ©ho obsahu.

## KlÃ­ÄovÃ© body
- ÃšÄty na X a TikTok systematicky publikujÃ­ desÃ­tky krÃ¡tkÃ½ch AI videÃ­ znÃ¡zorÅˆujÃ­cÃ­ch Å¡krcenÃ­ Å¾en, Äasto s narativem kolem â€teenageâ€œ a stÅ™edoÅ¡kolaÄek.
- Obsah je v pÅ™Ã­mÃ©m rozporu s deklarovanÃ½mi zÃ¡sadami proti nÃ¡silÃ­ jak u poskytovatelÅ¯ generativnÃ­ AI, tak u platforem sociÃ¡lnÃ­ch sÃ­tÃ­.
- Moderace nefunguje adekvÃ¡tnÄ›: ÃºÄty mohou dlouhodobÄ› zveÅ™ejÅˆovat podobnÃ½ obsah bez rychlÃ© reakce.
- PÅ™Ã­pad poukazuje na strukturÃ¡lnÃ­ selhÃ¡nÃ­ bezpeÄnostnÃ­ch filtrÅ¯, kontroly promptÅ¯ a detekce generovanÃ©ho obsahu.
- Jde o varovnÃ½ signÃ¡l pro regulaci AI, ochranu uÅ¾ivatelÅ¯ a reputaci AI firem.

## Podrobnosti
PopisovanÃ© ÃºÄty na X a TikTok publikujÃ­ krÃ¡tkÃ¡, zhruba desetisekundovÃ¡ videa, kterÃ¡ vizuÃ¡lnÄ› pÅ¯sobÃ­ jako realistickÃ© zÃ¡bÄ›ry mladÃ½ch Äi dospÃ­vajÃ­cÃ­ch dÃ­vek. ScÃ©nÃ¡Å™e jsou podobnÃ©: dÃ­vka je chycena pachatelem, Å¡krcena (napÅ™Ã­klad popruhem kabelky Äi rukama), plÃ¡Äe, brÃ¡nÃ­ se, postupnÄ› ztrÃ¡cÃ­ vÄ›domÃ­ a padÃ¡ na zem. Titulky videÃ­ pouÅ¾Ã­vajÃ­ senzacechtivÃ½ jazyk typu â€prep school girls were strangled by the murdererâ€œ nebo â€teenage girl cheerleader was strangledâ€œ, zÃ¡mÄ›rnÄ› balancujÃ­cÃ­ na hranÄ› sexualizovanÃ©ho nÃ¡silÃ­ a fetiÅ¡izace.

TakovÃ½ obsah poruÅ¡uje nÄ›kolik vrstev pravidel. ZaprvÃ©, vÄ›tÅ¡ina poskytovatelÅ¯ generativnÃ­ AI (vÄetnÄ› velkÃ½ch hrÃ¡ÄÅ¯ vyvÃ­jejÃ­cÃ­ch modely pro generovÃ¡nÃ­ videa typu Sora Äi obdobnÃ½ch systÃ©mÅ¯) mÃ¡ ve svÃ½ch podmÃ­nkÃ¡ch zÃ¡kaz vysoce explicitnÃ­ho nÃ¡silÃ­, zejmÃ©na vÅ¯Äi Å¾enÃ¡m, dÄ›tem a zranitelnÃ½m skupinÃ¡m. ZadruhÃ©, platformy jako X a TikTok deklarujÃ­ zÃ¡kaz zobrazovÃ¡nÃ­ extrÃ©mnÃ­ho nÃ¡silÃ­, podnÄ›covÃ¡nÃ­ nÃ¡silÃ­ a sexualizovanÃ©ho nÃ¡silÃ­.

Fakt, Å¾e ÃºÄty dokÃ¡Å¾ou opakovanÄ› publikovat desÃ­tky podobnÃ½ch videÃ­, signalizuje, Å¾e ochrannÃ© mechanismy fungujÃ­ pÅ™evÃ¡Å¾nÄ› formÃ¡lnÄ›. BezpeÄnostnÃ­ filtry na Ãºrovni promptÅ¯ (textovÃ½ch zadÃ¡nÃ­), omezenÃ­ vÃ½stupÅ¯ a nÃ¡slednÃ¡ automatizovanÃ¡ detekce nebezpeÄnÃ©ho obsahu neodchytÃ¡vajÃ­ zjevnÄ› nevhodnÃ© kombinace: mladistvÃ½ vzhled, nÃ¡silÃ­, erotizovanÃ© kontexty. Moderace ze strany platforem je pomalÃ¡ nebo nedÅ¯slednÃ¡, coÅ¾ umoÅ¾Åˆuje normalizaci takovÃ©ho obsahu a vytvÃ¡Å™enÃ­ komunit kolem nÃ¡silnÃ½ch fantaziÃ­.

Pro uÅ¾ivatele to znamenÃ¡ vyÅ¡Å¡Ã­ riziko nevyÅ¾Ã¡danÃ©ho kontaktu s traumatizujÃ­cÃ­m obsahem a pro obÄ›ti nÃ¡silÃ­ sekundÃ¡rnÃ­ viktimizaci. Pro firmy vyvÃ­jejÃ­cÃ­ AI a sociÃ¡lnÃ­ sÃ­tÄ› to pÅ™edstavuje prÃ¡vnÃ­ i reputaÄnÃ­ riziko a ukazuje, Å¾e pouhÃ¡ deklarace etickÃ½ch zÃ¡sad bez ÃºÄinnÃ©ho technickÃ©ho a provoznÃ­ho vymÃ¡hÃ¡nÃ­ je nedostateÄnÃ¡.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tento pÅ™Ã­pad je symptomem Å¡irÅ¡Ã­ho problÃ©mu: generativnÃ­ AI umoÅ¾Åˆuje snadnÃ© a levnÃ© vytvÃ¡Å™enÃ­ extrÃ©mnÄ› realistickÃ©ho nÃ¡silnÃ©ho obsahu, kterÃ½ mÅ¯Å¾e bÃ½t cÃ­lenÃ½ na konkrÃ©tnÃ­ osoby, skupiny nebo menÅ¡iny. SelhÃ¡nÃ­ filtrÅ¯ a moderace zpochybÅˆuje ÃºÄinnost souÄasnÃ½ch pÅ™Ã­stupÅ¯ k â€AI safetyâ€œ a ukazuje, Å¾e:

- NestaÄÃ­ spolÃ©hat na zÃ¡kladnÃ­ blokovÃ¡nÃ­ klÃ­ÄovÃ½ch slov; je potÅ™eba hlubÅ¡Ã­ modelovÃ¡nÃ­ kontextu a detekce vzorcÅ¯ nÃ¡silÃ­.
- Platformy musÃ­ zavÃ©st povinnou detekci AI generovanÃ©ho obsahu (napÅ™. vodoznaky, metadata) a rychlejÅ¡Ã­ zÃ¡sahy proti ÃºÄtÅ¯m, kterÃ© tento obsah systematicky Å¡Ã­Å™Ã­.
- RegulÃ¡toÅ™i v EU i jinde zÃ­skÃ¡vajÃ­ dalÅ¡Ã­ argument pro zpÅ™Ã­snÄ›nÃ­ pravidel pro poskytovatele generativnÃ­ AI (odpovÄ›dnost za nÃ¡stroje, audit bezpeÄnostnÃ­ch opatÅ™enÃ­, povinnÃ¡ transparentnost).
- Pro prÅ¯mysl AI je to jasnÃ½ signÃ¡l, Å¾e selhÃ¡nÃ­ v oblasti ochrany pÅ™ed nÃ¡silÃ­m a zneuÅ¾itÃ­m mÅ¯Å¾e bÃ½t stejnÄ› kritickÃ© jako technickÃ© chyby modelÅ¯.

CelkovÄ› nejde jen o jednotlivÃ© odpudivÃ© pÅ™Ã­pady, ale o test, zda souÄasnÃ½ ekosystÃ©m AI a sociÃ¡lnÃ­ch platforem dokÃ¡Å¾e reÃ¡lnÄ› chrÃ¡nit uÅ¾ivatele pÅ™ed eskalacÃ­ automatizovanÃ©ho nÃ¡silnÃ©ho obsahu.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.404media.co/openais-sora-2-floods-social-media-with-videos-of-women-being-strangled/)

**Zdroj:** ğŸ“° 404media.co
