---
author: Marisa Aigen
category: ai prÃ¡vo
date: '2025-11-25 19:09:16'
description: NovÃ½ ÄlÃ¡nek v Journal of Free Speech Law analyzuje prÃ¡vnÃ­ a spoleÄenskÃ©
  dÅ¯sledky halucinacÃ­ v pokroÄilÃ½ch jazykovÃ½ch modelech, kterÃ© mohou Å¡Ã­Å™it nepravdivÃ©
  informace s potenciÃ¡lem pomluvy.
importance: 4
layout: tech_news_article
original_title: 'Journal of Free Speech Law: "Inevitable Errors: Defamation by Hallucination
  in AI Reasoning Models," by Lyrissa Lidsky & Andrew Daves'
publishedAt: '2025-11-25T19:09:16+00:00'
slug: journal-of-free-speech-law-inevitable-errors-defam
source:
  emoji: ğŸ“°
  id: null
  name: Reason
title: 'NezbytnÃ© chyby: Pomluva zpÅ¯sobenÃ¡ halucinacemi v AI modely uvaÅ¾ovÃ¡nÃ­'
url: https://reason.com/volokh/2025/11/25/journal-of-free-speech-law-inevitable-errors-defamation-by-hallucination-in-ai-reasoning-models-by-lyrissa-lidsky-andrew-daves/
urlToImage: https://d2eehagpk5cl65.cloudfront.net/img/q60/uploads/2021/11/reason-logo-featured-placeholder.jpg.webp
urlToImageBackup: https://d2eehagpk5cl65.cloudfront.net/img/q60/uploads/2021/11/reason-logo-featured-placeholder.jpg.webp
---

## Souhrn
ÄŒlÃ¡nek â€NezbytnÃ© chyby: Pomluva zpÅ¯sobenÃ¡ halucinacemi v AI modely uvaÅ¾ovÃ¡nÃ­â€œ od Lyrissy Lidsky a Andrewa Daves publikovanÃ½ v Journal of Free Speech Law zkoumÃ¡ rostoucÃ­ riziko prÃ¡vnÄ› zÃ¡vaznÃ© pomluvy generovanÃ© pokroÄilÃ½mi jazykovÃ½mi modely (LLM). AutoÅ™i upozorÅˆujÃ­, Å¾e i kdyÅ¾ se umÄ›lÃ¡ obecnÃ¡ inteligence (AGI) teprve blÃ­Å¾Ã­, souÄasnÃ© modely uÅ¾ dosahujÃ­ vÃ½konu srovnatelnÃ©ho s lidmi v Ãºzce definovanÃ½ch ÃºlohÃ¡ch â€“ a zÃ¡roveÅˆ produkujÃ­ halucinace, kterÃ© mohou poÅ¡kodit reputaci jednotlivcÅ¯.

## KlÃ­ÄovÃ© body
- PokroÄilÃ© jazykovÃ© modely (napÅ™. OpenAI o3-preview) dosÃ¡hly 75â€“88 % ÃºrovnÄ› AGI v testech, ale stÃ¡le trpÃ­ halucinacemi.
- Halucinace v LLM mohou vÃ©st k prÃ¡vnÄ› postiÅ¾itelnÃ© pomluvÄ›, zejmÃ©na pÅ™i generovÃ¡nÃ­ faleÅ¡nÃ½ch tvrzenÃ­ o Å¾ivÃ½ch osobÃ¡ch.
- OpenAI, Anthropic, Google a DeepSeek intenzivnÄ› zvyÅ¡ujÃ­ schopnosti svÃ½ch modelÅ¯ uvaÅ¾ovat a odpovÃ­dat lidsky, coÅ¾ zvyÅ¡uje riziko Å¡Ã­Å™enÃ­ neovÄ›Å™enÃ½ch informacÃ­.
- Modely jako OpenAI o1 jiÅ¾ dosahujÃ­ lepÅ¡Ã­ch vÃ½sledkÅ¯ neÅ¾ prÅ¯mÄ›rnÃ­ prÃ¡vnÃ­ci v testech jako LSAT.
- ÄŒlÃ¡nek upozorÅˆuje na prÃ¡vnÃ­ mezeru: souÄasnÃ© prÃ¡vo nenÃ­ pÅ™ipraveno na odpovÄ›dnost za Å¡kody zpÅ¯sobenÃ© autonomnÃ­mi AI systÃ©my.

## Podrobnosti
AutoÅ™i ÄlÃ¡nku poukazujÃ­ na paradox: zatÃ­mco spoleÄnosti jako OpenAI oficiÃ¡lnÄ› prohlaÅ¡ujÃ­, Å¾e jejich cÃ­lem je, aby AGI â€prospÄ›la celÃ©mu lidstvuâ€œ, jejich modely uÅ¾ dnes generujÃ­ nepravdivÃ© tvrzenÃ­, kterÃ¡ mohou mÃ­t reÃ¡lnÃ© prÃ¡vnÃ­ dÅ¯sledky. NapÅ™Ã­klad LLM mÅ¯Å¾e vytvoÅ™it faleÅ¡nou informaci o tom, Å¾e nÄ›kdo spÃ¡chal trestnÃ½ Äin, coÅ¾ splÅˆuje definici pomluvy podle mnoha prÃ¡vnÃ­ch systÃ©mÅ¯. ZvlÃ¡Å¡tÄ› problematickÃ© je, Å¾e tyto modely jsou stÃ¡le ÄastÄ›ji integrovÃ¡ny do vyhledÃ¡vaÄÅ¯, asistentÅ¯ a dokonce prÃ¡vnÃ­ch nÃ¡strojÅ¯, kde uÅ¾ivatelÃ© implicitnÄ› dÅ¯vÄ›Å™ujÃ­ jejich vÃ½stupÅ¯m. OpenAI o1, kterÃ½ pÅ™ekonÃ¡vÃ¡ mediÃ¡n pÅ™ijatÃ½ch studentÅ¯ prÃ¡vnickÃ½ch fakult v USA v testu LSAT, je pÅ™Ã­kladem toho, jak rychle se modely blÃ­Å¾Ã­ lidskÃ© Ãºrovni uvaÅ¾ovÃ¡nÃ­ â€“ avÅ¡ak bez odpovÃ­dajÃ­cÃ­ schopnosti ovÄ›Å™ovat faktickou sprÃ¡vnost. Tento rozpor mezi kognitivnÃ­m vÃ½konem a faktickou spolehlivostÃ­ vytvÃ¡Å™Ã­ prÃ¡vnÃ­ a etickÃ© dilema, kterÃ© ÄlÃ¡nek detailnÄ› analyzuje.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tento ÄlÃ¡nek pÅ™ichÃ¡zÃ­ v klÃ­ÄovÃ©m okamÅ¾iku, kdy se AI stÃ¡vÃ¡ souÄÃ¡stÃ­ kaÅ¾dodennÃ­ho rozhodovÃ¡nÃ­ â€“ od zpravodajstvÃ­ po prÃ¡vnÃ­ poradenstvÃ­. Pokud nebudou zavedeny jasnÃ© prÃ¡vnÃ­ rÃ¡mce pro odpovÄ›dnost za halucinace, hrozÃ­ vlna Å¾alob a ztrÃ¡ta dÅ¯vÄ›ry ve veÅ™ejnÃ© informaÄnÃ­ systÃ©my. ZÃ¡roveÅˆ to ukazuje, Å¾e technologickÃ½ pokrok v AI nemÅ¯Å¾e bÄ›Å¾et oddÄ›lenÄ› od prÃ¡vnÃ­ a etickÃ© regulace. Pro vÃ½vojÃ¡Å™e, uÅ¾ivatele i regulÃ¡tory je proto nezbytnÃ© pochopit, Å¾e â€inteligentnÃ­â€œ model nenÃ­ zÃ¡rukou pravdivosti â€“ a Å¾e halucinace nejsou vedlejÅ¡Ã­m efektem, ale inherentnÃ­ vlastnostÃ­ souÄasnÃ½ch LLM.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://reason.com/volokh/2025/11/25/journal-of-free-speech-law-inevitable-errors-defamation-by-hallucination-in-ai-reasoning-models-by-lyrissa-lidsky-andrew-daves/)

**Zdroj:** ğŸ“° Reason
