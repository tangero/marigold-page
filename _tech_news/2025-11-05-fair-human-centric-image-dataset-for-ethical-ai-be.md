---
author: Marisa Aigen
category: ai etika
companies:
- OpenAI
- Google
- Microsoft
- Meta
- Amazon
date: '2025-11-05 16:22:08'
description: VÃ½zkumnÃ­ci pÅ™edstavili dataset FHIBE (Fair Human-Centric Image Benchmark),
  kterÃ½ je navrÅ¾en tak, aby umoÅ¾nil etickÃ©, transparentnÃ­ a detailnÃ­ testovÃ¡nÃ­ pÅ™edsudkÅ¯
  v poÄÃ­taÄovÃ©m vidÄ›nÃ­, zejmÃ©na u systÃ©mÅ¯ pracujÃ­cÃ­ch s lidskÃ½mi obrazy.
importance: 3
layout: tech_news_article
original_title: Fair human-centric image dataset for ethical AI benchmarking - Nature
publishedAt: '2025-11-05T16:22:08+00:00'
slug: fair-human-centric-image-dataset-for-ethical-ai-be
source:
  emoji: ğŸ“°
  id: null
  name: Nature.com
title: 'Fair Human-Centric Image Benchmark: novÃ½ etickÃ½ dataset pro hodnocenÃ­ fairness
  v AI'
url: https://www.nature.com/articles/s41586-025-09716-2
urlToImage: https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41586-025-09716-2/MediaObjects/41586_2025_9716_Fig1_HTML.png
urlToImageBackup: https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41586-025-09716-2/MediaObjects/41586_2025_9716_Fig1_HTML.png
---

## Souhrn
NovÃ½ dataset Fair Human-Centric Image Benchmark (FHIBE) nabÃ­zÃ­ systematicky navrÅ¾enou, veÅ™ejnÄ› dostupnou sadu snÃ­mkÅ¯ lidÃ­, kterÃ¡ respektuje souhlas, soukromÃ­, fÃ©rovou kompenzaci a bezpeÄnost ÃºÄastnÃ­kÅ¯. CÃ­lem je poskytnout robustnÃ­ nÃ¡stroj pro hodnocenÃ­ fairness modelÅ¯ poÄÃ­taÄovÃ©ho vidÄ›nÃ­ a vytvoÅ™it realistickÃ½ standard pro odpovÄ›dnou sprÃ¡vu dat v AI.

## KlÃ­ÄovÃ© body
- FHIBE je lidsky centricÌŒnyÌ obrazovyÌ dataset vytvoÅ™enÃ½ s dÅ¯razem na souhlas, soukromÃ­, bezpeÄnost a fÃ©rovou odmÄ›nu zÃºÄastnÄ›nÃ½ch osob.
- Obsahuje Å¡irokÃ© spektrum demografickÃ½ch, fyzickÃ½ch, environmentÃ¡lnÃ­ch a technickÃ½ch anotacÃ­ pro detailnÃ­ analÃ½zu pÅ™edsudkÅ¯ v modelech.
- Je urÄen jako benchmark pro hodnocenÃ­ fairness u Ãºloh jako pose estimation, person segmentation, face detection/verification a visual question answering.
- Dataset je navrÅ¾en tak, aby minimalizoval prÃ¡vnÃ­ i etickÃ¡ rizika spojenÃ¡ s dÅ™Ã­vÄ›jÅ¡Ã­mi problematickÃ½mi datasety vyuÅ¾Ã­vanÃ½mi bez souhlasu.
- AutoÅ™i se snaÅ¾Ã­ nastavit novÃ½ standard pro kurÃ¡torstvÃ­ dat a ukÃ¡zat praktickÃ½ nÃ¡vod, jak budovat dÅ¯vÄ›ryhodnÃ© AI systÃ©my.

## Podrobnosti
Datasety pro poÄÃ­taÄovÃ© vidÄ›nÃ­ patÅ™Ã­ k zÃ¡sadnÃ­ infrastruktuÅ™e modernÃ­ AI, ale historicky byly Äasto sestavovÃ¡ny z otevÅ™enÃ½ch nebo seÅ¡krÃ¡banÃ½ch zdrojÅ¯ bez informovanÃ©ho souhlasu osob, bez adekvÃ¡tnÃ­ho zohlednÄ›nÃ­ citlivÃ½ch atributÅ¯ a bez kontroly nad tÃ­m, jak jsou snÃ­mky dÃ¡le pouÅ¾ity. To vedlo k modelÅ¯m, kterÃ© trpÃ­ systematickÃ½mi pÅ™edsudky vÅ¯Äi urÄitÃ½m demografickÃ½m skupinÃ¡m a zÃ¡roveÅˆ pÅ™edstavujÃ­ reputaÄnÃ­, prÃ¡vnÃ­ i bezpeÄnostnÃ­ riziko pro firmy, kterÃ© je nasazujÃ­.

FHIBE reaguje na tuto situaci jako ÃºÄelovÄ› navrÅ¾enÃ½ â€fairness benchmarkâ€œ pro lidsky orientovanÃ© poÄÃ­taÄovÃ© vidÄ›nÃ­. ObrazovÃ¡ data byla sbÃ­rÃ¡na s dÅ¯razem na informovanÃ½ souhlas, transparentnÃ­ podmÃ­nky vyuÅ¾itÃ­ a fÃ©rovou kompenzaci ÃºÄastnÃ­kÅ¯. To Å™eÅ¡Ã­ ÄastÃ½ problÃ©m dosavadnÃ­ch datasetÅ¯, kde lidÃ© nebyli informovÃ¡ni o pouÅ¾itÃ­ svÃ½ch fotografiÃ­ pro trÃ©nink a testovÃ¡nÃ­ AI.

Dataset obsahuje detailnÃ­ anotace: demografickÃ© charakteristiky (napÅ™. vÄ›k, genderovÃ¡ identita, odstÃ­n pleti), fyzickÃ© rysy, obleÄenÃ­, rÅ¯znÃ© prostÅ™edÃ­ a svÄ›telnÃ© podmÃ­nky, technickÃ© parametry snÃ­mku, a v nÄ›kterÃ½ch pÅ™Ã­padech anotace na Ãºrovni pozice, segmentace a konkrÃ©tnÃ­ch pixelÅ¯. DÃ­ky tomu umoÅ¾Åˆuje nejen mÄ›Å™it agregovanou pÅ™esnost, ale takÃ© odhalovat jemnÄ›jÅ¡Ã­ formy biasu â€“ napÅ™Ã­klad rozdÃ­lnÃ½ vÃ½kon modelu pro rÅ¯znÃ© kombinace atributÅ¯, situacÃ­ a prostÅ™edÃ­.

FHIBE je koncipovÃ¡n jako hodnoticÃ­ dataset, nikoliv jako nekontrolovanÃ½ zdroj pro masivnÃ­ trÃ©novÃ¡nÃ­. Pro vÃ½vojÃ¡Å™e AI to znamenÃ¡ moÅ¾nost oddÄ›lit trÃ©nink na internÃ­ch nebo komerÄnÃ­ch datech od transparentnÃ­ho, eticky kurÃ¡torovanÃ©ho testovÃ¡nÃ­ fairness. Pro instituce a regulovanÃ© sektory (banky, pojiÅ¡Å¥ovny, zdravotnictvÃ­, veÅ™ejnÃ¡ sprÃ¡va) pak dataset nabÃ­zÃ­ konkrÃ©tnÃ­ nÃ¡stroj, jak doklÃ¡dat, Å¾e jejich systÃ©my proÅ¡ly testy spravedlnosti na datech zÃ­skanÃ½ch v souladu s modernÃ­mi etickÃ½mi standardy.

## ProÄ je to dÅ¯leÅ¾itÃ©
FHIBE posouvÃ¡ diskusi o etice a fairness v AI od obecnÃ½ch prohlÃ¡Å¡enÃ­ k praktickÃ©mu, technicky pouÅ¾itelnÃ©mu nÃ¡stroji. Z pohledu prÅ¯myslu poskytuje realistickou cestu, jak sladit vÃ½voj poÄÃ­taÄovÃ©ho vidÄ›nÃ­ s poÅ¾adavky na ochranu soukromÃ­, souhlas a nediskriminaci, coÅ¾ je klÃ­ÄovÃ© v kontextu pÅ™ichÃ¡zejÃ­cÃ­ch regulacÃ­ typu EU AI Act. ZÃ¡roveÅˆ vytvÃ¡Å™Ã­ tlak na opuÅ¡tÄ›nÃ­ historicky problematickÃ½ch datasetÅ¯, kterÃ© vznikly bez souhlasu a reprezentujÃ­ riziko pro firmy i akademickÃ© instituce.

Pro vÃ½zkumnÃ­ky a vÃ½vojÃ¡Å™e FHIBE ukazuje, Å¾e technicky kvalitnÃ­ dataset lze navrhnout spolu s etickÃ½mi principy od zaÄÃ¡tku, nikoli jako dodateÄnou opravu. Pro uÅ¾ivatele a dotÄenÃ© komunity to znamenÃ¡ vyÅ¡Å¡Ã­ Å¡anci, Å¾e systÃ©my pro rozpoznÃ¡vÃ¡nÃ­ obliÄejÅ¯, sledovÃ¡nÃ­ pohybu, verifikaci identity nebo analÃ½zu scÃ©n budou vyhodnocovÃ¡ny na souborech, kterÃ© lÃ©pe odrÃ¡Å¾ejÃ­ reÃ¡lnou rozmanitost a minimalizujÃ­ systematickÃ© selhÃ¡vÃ¡nÃ­ vÅ¯Äi zranitelnÃ½m skupinÃ¡m.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.nature.com/articles/s41586-025-09716-2)

**Zdroj:** ğŸ“° Nature.com
