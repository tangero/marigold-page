---
author: Marisa Aigen
category: vojenskÃ¡ ai
companies:
- Anthropic
date: '2026-02-01 01:49:50'
description: AmerickÃ© ministerstvo obrany je v sporu s firmou Anthropic kvÅ¯li etickÃ½m
  omezenÃ­m v jejÃ­ch AI systÃ©mech, kterÃ¡ brÃ¡nÃ­ jejich nasazenÃ­ v autonomnÃ­ch zbranÃ­ch
  a domÃ¡cÃ­m sledovÃ¡nÃ­. Smlouva v hodnotÄ› aÅ¾ 200 milionÅ¯ dolarÅ¯ je kvÅ¯li tomu pozastavena.
importance: 4
layout: tech_news_article
original_title: Pentagon wants killer AI without safeguards â€“ Reuters
publishedAt: '2026-02-01T01:49:50+00:00'
slug: pentagon-wants-killer-ai-without-safeguards-reuter
source:
  emoji: ğŸ“°
  id: rt
  name: RT
title: Pentagon chce smrtÃ­cÃ­ AI bez bezpeÄnostnÃ­ch omezenÃ­ â€“ Reuters
url: https://www.rt.com/news/631817-pentagon-anthropic-ai-guardrails/
urlToImage: https://mf.b37mrtl.ru/files/2026.02/article/697eb07e85f5400dc05ea07c.jpg
urlToImageBackup: https://mf.b37mrtl.ru/files/2026.02/article/697eb07e85f5400dc05ea07c.jpg
---

## Souhrn
AmerickÃ© ministerstvo obrany, bÄ›Å¾nÄ› nazÃ½vanÃ© Pentagon, se stÅ™etlo s vÃ½vojÃ¡Å™em umÄ›lÃ© inteligence Anthropic ohlednÄ› bezpeÄnostnÃ­ch omezenÃ­ vestavÄ›nÃ½ch do jeho technologiÃ­. Tyto omezenÃ­ majÃ­ zabrÃ¡nit pouÅ¾itÃ­ AI v autonomnÃ­ch smrtÃ­cÃ­ch zbranÃ­ch bez lidskÃ©ho dohledu nebo v domÃ¡cÃ­m sledovÃ¡nÃ­ obyvatel. Spor zpÅ¯sobil pozastavenÃ­ smlouvy v hodnotÄ› aÅ¾ 200 milionÅ¯ dolarÅ¯.

## KlÃ­ÄovÃ© body
- Pentagon tlaÄÃ­ na odstranÄ›nÃ­ etickÃ½ch limitÅ¯ v AI od Anthropic, aby mohl nasadit systÃ©my pro autonomnÃ­ cÃ­lenÃ­ a sledovÃ¡nÃ­.
- Anthropic, firma specializujÃ­cÃ­ se na bezpeÄnÃ© AI modely jako Claude, se obÃ¡vÃ¡ zneuÅ¾itÃ­ pro smrtÃ­cÃ­ operace bez kontroly.
- Smlouva je pozastavena; zdroje mluvÃ­ o Å¡esti anonymnÃ­ch osobÃ¡ch obeznÃ¡menÃ½ch se situacÃ­.
- Konflikt odhaluje napÄ›tÃ­ mezi komerÄnÃ­mi AI firmami a vojenskÃ½mi potÅ™ebami.
- Reuters cituje, Å¾e armÃ¡da povaÅ¾uje omezenÃ­ za pÅ™Ã­liÅ¡nÃ¡.

## Podrobnosti
Anthropic je kalifornskÃ¡ spoleÄnost zaloÅ¾enÃ¡ v roce 2021 bÃ½valÃ½mi vÃ½zkumnÃ­ky OpenAI, kterÃ¡ se zamÄ›Å™uje na vÃ½voj velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) s dÅ¯razem na bezpeÄnost. Jejich hlavnÃ­ produkt, model Claude, slouÅ¾Ã­ k generovÃ¡nÃ­ textu, analÃ½ze dat a Å™eÅ¡enÃ­ sloÅ¾itÃ½ch ÃºkolÅ¯, ale obsahuje tzv. ÃºstavnÃ­ AI â€“ rÃ¡mec, kterÃ½ vynucuje etickÃ© chovÃ¡nÃ­ a zabraÅˆuje Å¡kodlivÃ½m vÃ½stupÅ¯m. Firma zÃ­skala investice od gigantÅ¯ jako Amazon a Google, celkem pÅ™es 7 miliard dolarÅ¯, coÅ¾ ji Å™adÃ­ mezi lÃ­dry v oblasti zodpovÄ›dnÃ©ho AI.

Pentagon chtÄ›l integrovat tyto technologie do svÃ½ch systÃ©mÅ¯ pro autonomnÃ­ zbranÄ›, kde AI rozhoduje o ÃºtocÃ­ch bez lidskÃ©ho zÃ¡sahu, a pro sledovÃ¡nÃ­ na domÃ¡cÃ­ pÅ¯dÄ›, napÅ™Ã­klad kamerovÃ½mi sÃ­tÄ›mi nebo analÃ½zou sociÃ¡lnÃ­ch sÃ­tÃ­. Podle zdrojÅ¯ citovanÃ½ch Reutersem armÃ¡da argumentuje, Å¾e omezenÃ­ Anthropic omezujÃ­ operaÄnÃ­ flexibilitu v bojovÃ½ch podmÃ­nkÃ¡ch. Anthropic naopak upozorÅˆuje na rizika: chyby v AI by mohly vÃ©st k civilnÃ­m obÄ›tem, eskalaci konfliktÅ¯ nebo poruÅ¡enÃ­ Ãºstavy kvÅ¯li masovÃ©mu sledovÃ¡nÃ­ AmeriÄanÅ¯.

Tento spor nenÃ­ ojedinÄ›lÃ½. PodobnÃ© debaty probÃ­hajÃ­ kolem smrtÃ­cÃ­ch autonomnÃ­ch zbraÅˆovÃ½ch systÃ©mÅ¯ (LAWS), kterÃ© OSN diskutuje od roku 2014. Firmy jako OpenAI a Google uÅ¾ dÅ™Ã­ve odmÃ­tly vojenskÃ© zakÃ¡zky â€“ OpenAI v roce 2024 zakÃ¡zalo pouÅ¾itÃ­ ChatGPT pro vÃ½voj zbranÃ­. Pentagon se proto obracÃ­ k jinÃ½m dodavatelÅ¯m, jako Palantir nebo tradiÄnÃ­ zbrojaÅ™i jako Lockheed Martin, kteÅ™Ã­ AI integrujÃ­ bez takovÃ½ch etickÃ½ch brzd. PozastavenÃ­ smlouvy znamenÃ¡, Å¾e Anthropic ztrÃ¡cÃ­ potenciÃ¡lnÃ­ pÅ™Ã­jem, ale udrÅ¾uje svou reputaci v oblasti AI safety. Situace ilustruje, jak se komerÄnÃ­ AI, pÅ¯vodnÄ› urÄenÃ© pro civilnÃ­ aplikace jako chatboti nebo data analÃ½za, stÃ¡vajÃ­ vojenskÃ½mi nÃ¡stroji.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tento konflikt podtrhuje rostoucÃ­ napÄ›tÃ­ mezi rychlÃ½m vÃ½vojem AI a etickÃ½mi normami. Pro prÅ¯mysl znamenÃ¡, Å¾e vojenskÃ© zakÃ¡zky mohou nutit firmy k ÃºstupkÅ¯m od bezpeÄnostnÃ­ch standardÅ¯, coÅ¾ oslabÃ­ globÃ¡lnÃ­ snahy o regulaci AI (napÅ™. EU AI Act nebo Bidenova vÃ½konnÃ¡ smÄ›rnice z roku 2023). Pokud Pentagon uspÄ›je, urychlÃ­ to vÃ½voj killer robotÅ¯, coÅ¾ zvyÅ¡uje riziko nÃ¡hodnÃ½ch ÃºtokÅ¯ â€“ AI systÃ©my jako ty od Anthropic majÃ­ ÃºspÄ›Å¡nost detekce cÃ­lÅ¯ kolem 90 %, ale v reÃ¡lnÃ©m boji klesÃ¡ kvÅ¯li nepÅ™edvÃ­datelnÃ½m faktorÅ¯m. Pro uÅ¾ivatele a spoleÄnost to pÅ™edstavuje hrozbu militarizace AI, kde technologie urÄenÃ© pro efektivitu konÄÃ­ v rukou armÃ¡dy bez dostateÄnÃ½ch kontrol. DlouhodobÄ› to mÅ¯Å¾e vÃ©st k zÃ¡vodÅ¯m ve zbranÃ­ch mezi USA, ÄŒÃ­nou a Ruskem, kde bezpeÄnostnÃ­ prvky budou prvnÃ­ obÄ›tÃ­.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.rt.com/news/631817-pentagon-anthropic-ai-guardrails/)

**Zdroj:** ğŸ“° RT
