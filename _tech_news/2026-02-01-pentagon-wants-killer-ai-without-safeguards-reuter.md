---
author: Marisa Aigen
category: vojenskÃ¡ ai
companies:
- Anthropic
date: '2026-02-01 01:49:50'
description: AmerickÃ© ministerstvo obrany je v konfliktu s firmou Anthropic kvÅ¯li
  etickÃ½m omezenÃ­m v jejich AI systÃ©mech, kterÃ¡ brÃ¡nÃ­ nasazenÃ­ v autonomnÃ­ch zbranÃ­ch
  a domÃ¡cÃ­m dohledu. Smlouva v hodnotÄ› aÅ¾ 200 milionÅ¯ dolarÅ¯ je kvÅ¯li tomu pozastavena.
importance: 4
layout: tech_news_article
original_title: Pentagon wants killer AI without safeguards â€“ Reuters
publishedAt: '2026-02-01T01:49:50+00:00'
slug: pentagon-wants-killer-ai-without-safeguards-reuter
source:
  emoji: ğŸ“°
  id: rt
  name: RT
title: Pentagon chce smrtÃ­cÃ­ AI bez bezpeÄnostnÃ­ch omezenÃ­ â€“ Reuters
url: https://www.rt.com/news/631817-pentagon-anthropic-ai-guardrails/
urlToImage: https://mf.b37mrtl.ru/files/2026.02/article/697eb07e85f5400dc05ea07c.jpg
urlToImageBackup: https://mf.b37mrtl.ru/files/2026.02/article/697eb07e85f5400dc05ea07c.jpg
---

## Souhrn
AmerickÃ© ministerstvo obrany, znÃ¡mÃ© jako Pentagon, je v ostrÃ©m sporu s vÃ½vojÃ¡Å™em umÄ›lÃ© inteligence Anthropic ohlednÄ› bezpeÄnostnÃ­ch omezenÃ­ vestavÄ›nÃ½ch do jejich AI modelÅ¯. Tyto omezenÃ­ brÃ¡nÃ­ pouÅ¾itÃ­ technologiÃ­ pro autonomnÃ­ smrtÃ­cÃ­ operace bez lidskÃ©ho dohledu nebo pro domÃ¡cÃ­ sledovÃ¡nÃ­ obÄanÅ¯. Kontrakt v hodnotÄ› aÅ¾ 200 milionÅ¯ dolarÅ¯ zÅ¯stÃ¡vÃ¡ neuzavÅ™enÃ½ kvÅ¯li neshodÃ¡m.

## KlÃ­ÄovÃ© body
- Pentagon povaÅ¾uje omezenÃ­ Anthropic za pÅ™Ã­liÅ¡ restriktivnÃ­ a chce je odstranit pro plnÃ© vojenskÃ© nasazenÃ­ AI.
- Anthropic se obÃ¡vÃ¡ zneuÅ¾itÃ­ svÃ½ch AI nÃ¡strojÅ¯ pro zabÃ­jenÃ­ bez dostateÄnÃ©ho lidskÃ©ho zÃ¡sahu nebo pro sledovÃ¡nÃ­ AmeriÄanÅ¯.
- Spor zastavil smlouvu v hodnotÄ› aÅ¾ 200 milionÅ¯ dolarÅ¯, jak uvedlo Å¡est zdrojÅ¯ pro Reuters.
- Firma Anthropic, zaloÅ¾enÃ¡ v San Franciscu, se specializuje na bezpeÄnÃ© AI modely jako Claude s dÅ¯razem na etiku.
- Tento konflikt odhaluje napÄ›tÃ­ mezi komerÄnÃ­mi AI firmami a vojenskÃ½mi potÅ™ebami.

## Podrobnosti
AmerickÃ© ministerstvo obrany se snaÅ¾Ã­ uzavÅ™Ã­t smlouvu s Anthropic, firmou zamÄ›Å™enou na vÃ½voj velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) jako je Å™ada Claude, kterÃ© jsou navrÅ¾eny s vestavÄ›nÃ½mi bezpeÄnostnÃ­mi mechanismy znÃ¡mÃ½mi jako safeguards. Tyto mechanismy zabraÅˆujÃ­ AI v generovÃ¡nÃ­ instrukcÃ­ pro autonomnÃ­ zbranÄ›, jako jsou drony nebo robotickÃ© systÃ©my schopnÃ© samostatnÃ©ho zacÃ­lenÃ­ a Ãºderu bez lidskÃ©ho schvÃ¡lenÃ­, a zÃ¡roveÅˆ omezujÃ­ pouÅ¾itÃ­ pro masovÃ© sledovÃ¡nÃ­ v domÃ¡cÃ­m prostÅ™edÃ­, napÅ™Ã­klad analÃ½zu dat z kamer nebo sociÃ¡lnÃ­ch sÃ­tÃ­. Podle Å¡esti zdrojÅ¯ obeznÃ¡menÃ½ch se situacÃ­, kterÃ© cituje Reuters, vojenÅ¡tÃ­ pÅ™edstavitelÃ© argumentujÃ­, Å¾e komerÄnÃ­ AI by mÄ›la bÃ½t plnÄ› vyuÅ¾itelnÃ¡ pro obrannÃ© ÃºÄely bez tÄ›chto "pÅ™ehnanÃ½ch" limitÅ¯.

Anthropic, kterÃ½ byl zaloÅ¾en bÃ½valÃ½mi vÃ½zkumnÃ­ky z OpenAI s cÃ­lem priorizovat bezpeÄnost umÄ›lÃ© inteligence prostÅ™ednictvÃ­m pÅ™Ã­stupu nazvanÃ©ho constitutional AI, tyto obavy zdÅ¯razÅˆuje. Jejich modely jsou trÃ©novÃ¡ny tak, aby odmÃ­taly poÅ¾adavky na Å¡kodlivÃ© akce, vÄetnÄ› plÃ¡novÃ¡nÃ­ ÃºtokÅ¯ nebo dezinformacÃ­. VojenskÃ¡ strana naopak tvrdÃ­, Å¾e v boji proti protivnÃ­kÅ¯m jako ÄŒÃ­na nebo Rusko potÅ™ebuje rychlÃ© a autonomnÃ­ systÃ©my, kde zpoÅ¾dÄ›nÃ­ zpÅ¯sobenÃ© lidskÃ½m rozhodovÃ¡nÃ­m mÅ¯Å¾e bÃ½t fatÃ¡lnÃ­. Tento spor nastal v kontextu rostoucÃ­ho zÃ¡jmu Pentagonu o AI, kterÃ½ investoval miliardy dolarÅ¯ do programÅ¯ jako Replicator pro nasazenÃ­ tisÃ­cÅ¯ autonomnÃ­ch dronÅ¯.

Zdroj ÄlÃ¡nku, RT.com, Äasto prezentuje zprÃ¡vy s protizÃ¡padnÃ­m nÃ¡dechem a pouÅ¾Ã­vÃ¡ termÃ­n "Department of War" mÃ­sto oficiÃ¡lnÃ­ho "Department of Defense", coÅ¾ podtrhuje propagandistickÃ½ tÃ³n. PÅ™esto fakta o sporu potvrzuje Reuters na zÃ¡kladÄ› vÃ­ce zdrojÅ¯, coÅ¾ naznaÄuje reÃ¡lnÃ½ konflikt v praxi komercializace AI pro armÃ¡du.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tento spor ilustruje klÃ­ÄovÃ© napÄ›tÃ­ v ekosystÃ©mu umÄ›lÃ© inteligence mezi etickÃ½mi standardy soukromÃ½ch firem a vojenskÃ½mi poÅ¾adavky stÃ¡tÅ¯. Pokud Pentagon uspÄ›je v prosazenÃ­ AI bez safeguards, mohlo by to vÃ©st k rychlejÅ¡Ã­mu nasazenÃ­ autonomnÃ­ch zbranÃ­, coÅ¾ zvyÅ¡uje rizika chybnÃ©ho zacÃ­lenÃ­ civilistÅ¯ nebo eskalace konfliktÅ¯ bez lidskÃ©ho zÃ¡sahu. Naopak, pokud firmy jako Anthropic udrÅ¾Ã­ svÃ¡ omezenÃ­, zpomalÃ­ to vojenskou adopci pokroÄilÃ½ch AI, coÅ¾ by mohlo ovlivnit globÃ¡lnÃ­ rovnovÃ¡hu sil â€“ zejmÃ©na kdyÅ¾ ÄŒÃ­na a Rusko nemajÃ­ podobnÃ© etickÃ© brzdy. Pro prÅ¯mysl to znamenÃ¡, Å¾e investoÅ™i do AI firem budou Äelit tlaku na Ãºpravu bezpeÄnostnÃ­ch politik, coÅ¾ by mohlo oslabit dÅ¯vÄ›ru veÅ™ejnosti v technologii. V Å¡irÅ¡Ã­m kontextu to urychluje debatu o mezinÃ¡rodnÃ­ch dohodÃ¡ch o zabijÃ¡ckÃ½ch robotech, jako je snaha OSN o zÃ¡kaz lethal autonomous weapons systems (LAWS).

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.rt.com/news/631817-pentagon-anthropic-ai-guardrails/)

**Zdroj:** ğŸ“° RT
