---
author: Marisa Aigen
category: autonomnÃ­ agenti
companies:
- Anthropic
date: '2026-02-21 12:00:00'
description: Jak Anthropic vydÃ¡vÃ¡ svÃ© nejautonomnÄ›jÅ¡Ã­ agenty, rostoucÃ­ konflikt s
  armÃ¡dou odhaluje nemoÅ¾nou volbu mezi globÃ¡lnÃ­m Å¡kÃ¡lovÃ¡nÃ­m a principem â€bezpeÄnost
  na prvnÃ­m mÃ­stÄ›â€œ
importance: 5
layout: tech_news_article
original_title: Anthropicâ€™s safety-first AI collides with the Pentagon as Claude expands
  into autonomous agents
publishedAt: '2026-02-21T12:00:00+00:00'
slug: anthropics-safety-first-ai-collides-with-the-penta
source:
  emoji: ğŸ“°
  id: null
  name: Scientific American
title: AnthropicÅ¯v AI s prioritou bezpeÄnosti se stÅ™etÃ¡vÃ¡ s Pentagonem pÅ™i rozÅ¡Ã­Å™enÃ­
  Claude do autonomnÃ­ch agentÅ¯
url: https://www.scientificamerican.com/article/anthropics-safety-first-ai-collides-with-the-pentagon-as-claude-expands-into/
urlToImage: https://static.scientificamerican.com/dam/m/26c0e466ea5342b5/original/GettyImages-2260944681.jpg?m=1771624497.843&w=1200
urlToImageBackup: https://static.scientificamerican.com/dam/m/26c0e466ea5342b5/original/GettyImages-2260944681.jpg?m=1771624497.843&w=1200
---

### Souhrn
Anthropic 5. Ãºnora 2026 vydal model Claude Opus 4.6, svÅ¯j nejsilnÄ›jÅ¡Ã­ AI dosud, kterÃ½ koordinuje tÃ½my autonomnÃ­ch agentÅ¯ pro paralelnÃ­ zpracovÃ¡nÃ­ ÃºkolÅ¯. O dvanÃ¡ct dnÃ­ pozdÄ›ji nÃ¡sledoval levnÄ›jÅ¡Ã­ Sonnet 4.6 s podobnÃ½mi schopnostmi v kÃ³dovÃ¡nÃ­ a ovlÃ¡dÃ¡nÃ­ poÄÃ­taÄÅ¯. SpoleÄnost rychle roste, ale ÄelÃ­ hrozbÄ› od Pentagonu, kterÃ½ mÅ¯Å¾e oznaÄit Anthropic za riziko v dodavatelskÃ©m Å™etÄ›zci kvÅ¯li omezenÃ­m vojenskÃ©ho vyuÅ¾itÃ­.

### KlÃ­ÄovÃ© body
- Claude Opus 4.6 umoÅ¾Åˆuje koordinaci vÃ­ce autonomnÃ­ch AI agentÅ¯, kteÅ™Ã­ si dÄ›lÃ­ prÃ¡ci a dokonÄujÃ­ ji paralelnÄ›.
- Sonnet 4.6 ovlÃ¡dÃ¡ webovÃ© aplikace a vyplÅˆuje formulÃ¡Å™e na Ãºrovni ÄlovÄ›ka, s pracovnÃ­ pamÄ›tÃ­ pro malou knihovnu dat.
- Enterprise zÃ¡kaznÃ­ci tvoÅ™Ã­ 80 % pÅ™Ã­jmÅ¯; minulÃ½ tÃ½den uzavÅ™eno financovÃ¡nÃ­ 30 miliard dolarÅ¯ pÅ™i valuaci 380 miliard.
- Pentagon hrozÃ­ oznaÄenÃ­m za â€supply chain riskâ€œ, coÅ¾ by znemoÅ¾nilo pouÅ¾itÃ­ Claude v citlivÃ½ch kontraktech.
- V roce 2024 modely Anthropic sotva zvlÃ¡daly prohlÃ­Å¾eÄ; nynÃ­ dosahujÃ­ lidskÃ© ÃºrovnÄ›.

### Podrobnosti
Anthropic, firma zamÄ›Å™enÃ¡ na vÃ½voj bezpeÄnÃ½ch velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) jako alternativa k OpenAI, dosÃ¡hla v Ãºnoru 2026 vÃ½znamnÃ©ho milnÃ­ku s vydÃ¡nÃ­m Claude Opus 4.6. Tento model pÅ™inÃ¡Å¡Ã­ autonomnÃ­ agenty, coÅ¾ jsou samostatnÃ© AI instance schopnÃ© koordinovat se navzÃ¡jem: dÄ›lÃ­ si sloÅ¾itÃ© Ãºkoly, jako je analÃ½za dat nebo vÃ½voj softwaru, a vykonÃ¡vajÃ­ je paralelnÄ› bez lidskÃ©ho zÃ¡sahu. NapÅ™Ã­klad agent mÅ¯Å¾e delegovat ÄÃ¡st vÃ½zkumu jinÃ©mu agentovi, zatÃ­mco tÅ™etÃ­ ovlÃ¡dÃ¡ nÃ¡stroje. Sonnet 4.6, levnÄ›jÅ¡Ã­ varianta, tÃ©mÄ›Å™ dorovnÃ¡vÃ¡ Opus v programovÃ¡nÃ­ a interakci s poÄÃ­taÄi â€“ naviguje webovÃ© aplikace, vyhledÃ¡vÃ¡ informace, vyplÅˆuje formulÃ¡Å™e a spravuje sloÅ¾itÃ© workflow. Oba modely majÃ­ pracovnÃ­ pamÄ›Å¥ dostateÄnÄ› velkou na uloÅ¾enÃ­ obsahu malÃ© knihovny, coÅ¾ umoÅ¾Åˆuje dlouhodobou autonomnÃ­ prÃ¡ci bez opakovanÃ©ho naÄÃ­tÃ¡nÃ­ kontextu.

Tento pokrok kontrastuje s rokem 2024, kdy prvnÃ­ modely Anthropic pro ovlÃ¡dÃ¡nÃ­ poÄÃ­taÄÅ¯ sotva zvlÃ¡daly zÃ¡kladnÃ­ prohlÃ­Å¾eÄ. NynÃ­ enterprise klienti, jako firmy v IT a financÃ­ch, generujÃ­ 80 % pÅ™Ã­jmÅ¯. MinulÃ½ tÃ½den Anthropic uzavÅ™el kolo financovÃ¡nÃ­ 30 miliard dolarÅ¯ pÅ™i valuaci 380 miliard, coÅ¾ ho Å™adÃ­ mezi nejrychleji rostoucÃ­ tech spoleÄnosti. NicmÃ©nÄ› tento rÅ¯st ohroÅ¾uje princip â€bezpeÄnost na prvnÃ­m mÃ­stÄ›â€œ: Anthropic omezuje vojenskÃ© aplikace, aby minimalizoval rizika zneuÅ¾itÃ­, napÅ™Ã­klad v autonomnÃ­ch zbranÃ­ch. Pentagon na to reaguje hrozbou oznaÄenÃ­ za â€supply chain riskâ€œ â€“ Å¡tÃ­tek typickÃ½ pro zahraniÄnÃ­ protivnÃ­ky â€“, coÅ¾ by donutilo kontrakty vynechat Claude z citlivÃ½ch projektÅ¯. Tento stÅ™et ilustruje dilema: globÃ¡lnÃ­ expanze vyÅ¾aduje armÃ¡dnÃ­ zakÃ¡zky, ale poruÅ¡enÃ­ bezpeÄnostnÃ­ch pravidel by podkopalo dÅ¯vÄ›ru investorÅ¯ a uÅ¾ivatelÅ¯. Kriticky Å™eÄeno, autonomnÃ­ agenti urychlujÃ­ cestu k AGI, ale bez robustnÃ­ch omezenÃ­ zvyÅ¡ujÃ­ rizika chyb nebo zneuÅ¾itÃ­ v reÃ¡lnÃ©m svÄ›tÄ›.

### ProÄ je to dÅ¯leÅ¾itÃ©
Tento vÃ½voj posouvÃ¡ autonomnÃ­ agenty z experimentu do praxe, umoÅ¾ÅˆujÃ­cÃ­ firmÃ¡m automatizovat sloÅ¾itÃ© procesy jako vÃ½voj softwaru nebo analÃ½zu dat, coÅ¾ zvyÅ¡uje produktivitu o Å™Ã¡dy. Pro prÅ¯mysl znamenÃ¡ konkurenci pro OpenAI a Google, kde Claude nabÃ­zÃ­ bezpeÄnÄ›jÅ¡Ã­ alternativu s enterprise zamÄ›Å™enÃ­m. Konflikt s Pentagonem odhaluje Å¡irÅ¡Ã­ napÄ›tÃ­ v AI ekosystÃ©mu: firmy jako Anthropic musÃ­ balancovat mezi etikou a komerÄnÃ­m tlakem. Pokud Pentagon prosadÃ­ svÅ¯j postoj, mohlo by to donutit zmÄ›nu politiky, coÅ¾ oslabÃ­ bezpeÄnostnÃ­ standardy v AGI vÃ½voji. Naopak, udrÅ¾enÃ­ omezenÃ­ by omezilo Å¡kÃ¡lovÃ¡nÃ­, ale posÃ­lilo dÅ¯vÄ›ryhodnost. V kontextu rychlÃ©ho pokroku smÄ›rem k AGI je to varovÃ¡nÃ­ pÅ™ed etickÃ½mi kompromisy, kterÃ© ovlivnÃ­ globÃ¡lnÃ­ regulace AI.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.scientificamerican.com/article/anthropics-safety-first-ai-collides-with-the-pentagon-as-claude-expands-into/)

**Zdroj:** ğŸ“° Scientific American
