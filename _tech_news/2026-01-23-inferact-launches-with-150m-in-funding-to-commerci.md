---
author: Marisa Aigen
category: startupy
companies:
- Inferact Inc
- Andreessen Horowitz
- Lightspeed
date: '2026-01-23 01:13:45'
description: Skupina vÃ½zkumnÃ­kÅ¯ v umÄ›lÃ© inteligenci spustila startup Inferact Inc.,
  kterÃ½ bude komercializovat open-source projekt vLLM pro optimalizaci inference velkÃ½ch
  jazykovÃ½ch modelÅ¯. FinancovÃ¡nÃ­ seed kola ÄinÃ­ 150 milionÅ¯ dolarÅ¯ s oceÅˆovÃ¡nÃ­m spoleÄnosti
  800 milionÅ¯ dolarÅ¯.
importance: 4
layout: tech_news_article
original_title: Inferact launches with $150M in funding to commercialize vLLM
publishedAt: '2026-01-23T01:13:45+00:00'
slug: inferact-launches-with-150m-in-funding-to-commerci
source:
  emoji: ğŸ“°
  id: null
  name: SiliconANGLE News
title: Inferact spouÅ¡tÃ­ s 150 miliony dolarÅ¯ financovÃ¡nÃ­ na komercializaci vLLM
url: https://siliconangle.com/2026/01/22/inferact-launches-150m-funding-commercialize-vllm/
urlToImage: https://d15shllkswkct0.cloudfront.net/wp-content/blogs.dir/1/files/2026/01/AI.png
urlToImageBackup: https://d15shllkswkct0.cloudfront.net/wp-content/blogs.dir/1/files/2026/01/AI.png
---

## Souhrn
Skupina vÃ½zkumnÃ­kÅ¯ z University of California at Berkeley spustila startup Inferact Inc., kterÃ½ plÃ¡nuje komercializovat open-source projekt vLLM. Tento nÃ¡stroj slouÅ¾Ã­ k urychlenÃ­ inference u velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) prostÅ™ednictvÃ­m optimalizacÃ­ pamÄ›ti a vÃ½konu. SpoleÄnost zÃ­skala 150 milionÅ¯ dolarÅ¯ v seed kole vedenÃ©m investory Andreessen Horowitz a Lightspeed, s oceÅˆovÃ¡nÃ­m 800 milionÅ¯ dolarÅ¯.

## KlÃ­ÄovÃ© body
- ZaloÅ¾enÃ­ Inferact Inc. tÃ½mem vÄetnÄ› Iona Stoicu, spoluzakladatele Databricks a Å™editele Sky Computing Lab na UC Berkeley.
- vLLM je open-source knihovna vyvinutÃ¡ v roce 2023, nynÃ­ s pÅ™es 2000 pÅ™ispÄ›vateli, kterÃ¡ optimalizuje inference LLM snÃ­Å¾enÃ­m spotÅ™eby pamÄ›ti a zrychlenÃ­m generovÃ¡nÃ­ odpovÄ›dÃ­.
- KlÃ­ÄovÃ¡ funkce PagedAttention umoÅ¾Åˆuje uklÃ¡dat KV cache v nepÅ™ipojenÃ½ch ÄÃ¡stech RAM, coÅ¾ minimalizuje plÃ½tvÃ¡nÃ­ pamÄ›tÃ­.
- DalÅ¡Ã­ optimalizace zahrnujÃ­ kvantizaci vah modelÅ¯ pro zmenÅ¡enÃ­ pamÄ›Å¥ovÃ© stopy.
- InvestoÅ™i: Andreessen Horowitz, Lightspeed, Databricks Ventures, UC Berkeley Chancellorâ€™s Fund.

## Podrobnosti
Inferact Inc. vznikl za ÃºÄelem pÅ™evodu open-source projektu vLLM do komerÄnÃ­ho produktu. vLLM je knihovna urÄenÃ¡ pro softwarovÃ© tÃ½my, kterÃ© potÅ™ebujÃ­ zpracovÃ¡vat inference Ãºlohy u velkÃ½ch jazykovÃ½ch modelÅ¯ rychleji a efektivnÄ›ji. KdyÅ¾ LLM obdrÅ¾Ã­ vÃ½zvu (prompt), provÃ¡dÃ­ vÃ½poÄty po ÄÃ¡stech: nejprve malou porci, uloÅ¾Ã­ vÃ½sledky do KV cache (klÃ­Ä-hodnota cache, kterÃ¡ uchovÃ¡vÃ¡ intermediÃ¡rnÃ­ stavy pro autoregresivnÃ­ generovÃ¡nÃ­), potÃ© pokraÄuje dalÅ¡Ã­ porcÃ­ a aktualizuje cache, dokud nevygeneruje kompletnÃ­ odpovÄ›Ä. Tento proces vyÅ¾aduje znaÄnÃ© mnoÅ¾stvÃ­ pamÄ›ti, protoÅ¾e KV cache roste s dÃ©lkou kontextu.

HlavnÃ­ inovace vLLM spoÄÃ­vÃ¡ v PagedAttention, kterÃ© uklÃ¡dÃ¡ data KV cache do nepÅ™ipojenÃ½ch sekcÃ­ serverovÃ© RAM, podobnÄ› jako paging v operaÄnÃ­ch systÃ©mech. To eliminuje fragmentaci pamÄ›ti, kdy jsou bloky pamÄ›ti rozhÃ¡zenÃ© a nevyuÅ¾itÃ© mezery zpÅ¯sobujÃ­ plÃ½tvÃ¡nÃ­. VÃ½sledkem je vÃ½raznÃ© snÃ­Å¾enÃ­ pamÄ›Å¥ovÃ© spotÅ™eby, coÅ¾ umoÅ¾Åˆuje spouÅ¡tÄ›t vÄ›tÅ¡Ã­ modely na mÃ©nÄ› hardwaru, napÅ™Ã­klad na mÃ©nÄ› GPU. NavÃ­c vLLM aplikuje kvantizaci, metodu komprese vah modelu, kterÃ¡ zmenÅ¡uje jejich velikost bez vÃ½raznÃ© ztrÃ¡ty pÅ™esnosti â€“ napÅ™Ã­klad pÅ™evodem vah z 16bitovÃ©ho na 8bitovÃ½ nebo niÅ¾Å¡Ã­ formÃ¡t.

KromÄ› Ãºspor pamÄ›ti vLLM zvyÅ¡uje i rychlost inference. StandardnÃ­ LLM generujÃ­ odpovÄ›di token po tokenu, coÅ¾ je sekvenÄnÃ­ proces; vLLM ho optimalizuje pro vyÅ¡Å¡Ã­ propustnost (throughput), coÅ¾ je klÃ­ÄovÃ© pro nasazenÃ­ v produkci, jako chatbotech nebo API sluÅ¾bÃ¡ch. Projekt byl pÅ¯vodnÄ› vyvinut v Sky Computing Lab na UC Berkeley pod vedenÃ­m Iona Stoicu, kterÃ½ je profesorem informatiky, spoluzakladatelem Databricks (spoleÄnost zamÄ›Å™enÃ¡ na big data a AI platformy) a mÃ¡ zkuÅ¡enosti s distribuovanÃ½mi systÃ©my. Od roku 2023 se k vÃ½voji pÅ™idalo pÅ™es 2000 vÃ½vojÃ¡Å™Å¯, coÅ¾ ukazuje na Å¡irokou komunitnÃ­ podporu.

Seed kolo ve vÃ½Å¡i 150 milionÅ¯ dolarÅ¯ vedli Andreessen Horowitz (a16z, znÃ¡mÃ½ venture fond zamÄ›Å™enÃ½ na tech a AI) a Lightspeed Venture Partners, s ÃºÄastÃ­ Databricks Ventures, fondu UC Berkeley Chancellorâ€™s Fund a dalÅ¡Ã­ch. OceÅˆovÃ¡nÃ­ na 800 milionÅ¯ dolarÅ¯ signalizuje vysokÃ© oÄekÃ¡vÃ¡nÃ­ od investorÅ¯ v oblasti AI infrastruktury. Inferact plÃ¡nuje rozÅ¡Ã­Å™it vLLM do plnohodnotnÃ© komerÄnÃ­ platformy, pravdÄ›podobnÄ› s podporou, Å¡kÃ¡lovatelnostÃ­ a integracemi pro enterprise zÃ¡kaznÃ­ky.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tato komercializace vLLM nastavuje trend v AI ekosystÃ©mu, kde open-source projekty jako Llama nebo Mistral pÅ™echÃ¡zejÃ­ do placenÃ½ch sluÅ¾eb, coÅ¾ zvyÅ¡uje dostupnost efektivnÃ­ inference, ale zÃ¡roveÅˆ riskuje fragmentaci komunity. Pro prÅ¯mysl znamenÃ¡ niÅ¾Å¡Ã­ nÃ¡klady na hardware â€“ inference tvoÅ™Ã­ vÄ›tÅ¡inu provoznÃ­ch vÃ½dajÅ¯ u LLM sluÅ¾eb â€“ a umoÅ¾Åˆuje Å¡kÃ¡lovat aplikace jako RAG systÃ©my nebo real-time chaty. V kontextu rostoucÃ­ poptÃ¡vky po GPU (napÅ™. od Nvidia) pomÃ¡hÃ¡ vLLM sniÅ¾ovat zÃ¡vislost na hardwaru, coÅ¾ je kritickÃ© pro menÅ¡Ã­ firmy. NicmÃ©nÄ› jako expert upozorÅˆuji, Å¾e komercializace mÅ¯Å¾e omezit volnÃ½ vÃ½voj, pokud Inferact zavede uzavÅ™enÃ© funkce, a konkurence od TensorRT-LLM nebo Hugging Face TGI zÅ¯stÃ¡vÃ¡ silnÃ¡. CelkovÄ› posiluje to pozici Berkeley jako centra AI vÃ½zkumu a urychluje adopci efektivnÃ­ch inference nÃ¡strojÅ¯.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://siliconangle.com/2026/01/22/inferact-launches-150m-funding-commercialize-vllm/)

**Zdroj:** ğŸ“° SiliconANGLE News
