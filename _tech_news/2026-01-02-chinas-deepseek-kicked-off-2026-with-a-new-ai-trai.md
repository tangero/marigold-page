---
author: Marisa Aigen
category: umÄ›lÃ¡ inteligence
companies:
- DeepSeek
date: '2026-01-02 07:10:41'
description: DeepSeek vydala novou metodu trÃ©novÃ¡nÃ­ umÄ›lÃ© inteligence, kterou analytici
  povaÅ¾ujÃ­ za prÅ¯lom v Å¡kÃ¡lovÃ¡nÃ­ velkÃ½ch jazykovÃ½ch modelÅ¯.
importance: 5
layout: tech_news_article
original_title: China's DeepSeek kicked off 2026 with a new AI training method that
  analysts say is a 'breakthrough' for scaling
publishedAt: '2026-01-02T07:10:41+00:00'
slug: chinas-deepseek-kicked-off-2026-with-a-new-ai-trai
source:
  emoji: ğŸ“°
  id: business-insider
  name: Business Insider
title: ÄŒÃ­nskÃ¡ DeepSeek zahÃ¡jila rok 2026 novou metodou trÃ©novÃ¡nÃ­ AI, kterou analytici
  oznaÄujÃ­ za prÅ¯lom pro Å¡kÃ¡lovÃ¡nÃ­
url: https://www.businessinsider.com/deepseek-new-ai-training-models-scale-manifold-constrained-analysts-china-2026-1
urlToImage: https://i.insider.com/695759b704eda4732f2e5e1b?width=1200&format=jpeg
urlToImageBackup: https://i.insider.com/695759b704eda4732f2e5e1b?width=1200&format=jpeg
---

### Souhrn
ÄŒÃ­nskÃ¡ spoleÄnost DeepSeek, zamÄ›Å™enÃ¡ na vÃ½voj open-source velkÃ½ch jazykovÃ½ch modelÅ¯, zveÅ™ejnila na zaÄÃ¡tku roku 2026 novou metodu trÃ©novÃ¡nÃ­ AI. Analytici z odvÄ›tvÃ­ ji oznaÄujÃ­ za prÅ¯lomovÃ½ pokrok v oblasti Å¡kÃ¡lovÃ¡nÃ­ modelÅ¯, coÅ¾ umoÅ¾Åˆuje efektivnÄ›jÅ¡Ã­ vyuÅ¾itÃ­ vÃ½poÄetnÃ­ch zdrojÅ¯ pÅ™i trÃ©novÃ¡nÃ­ stÃ¡le vÄ›tÅ¡Ã­ch systÃ©mÅ¯. Tato metoda Å™eÅ¡Ã­ klÃ­ÄovÃ© vÃ½zvy spojenÃ© s rostoucÃ­mi nÃ¡roky na hardware.

### KlÃ­ÄovÃ© body
- DeepSeek pÅ™edstavila techniku, kterÃ¡ sniÅ¾uje spotÅ™ebu vÃ½poÄetnÃ­ho vÃ½konu pÅ™i Å¡kÃ¡lovÃ¡nÃ­ modelÅ¯ na stovky miliard parametrÅ¯.
- Metoda integruje pokroÄilÃ© optimalizace datovÃ©ho toku a paralelizace, coÅ¾ umoÅ¾Åˆuje trÃ©novat modely na menÅ¡Ã­m poÄtu GPU.
- Analytici z Business Insider a dalÅ¡Ã­ch zdrojÅ¯ ji porovnÃ¡vajÃ­ s pÅ™edchozÃ­mi postupy jako MoE (Mixture of Experts).
- DeepSeek, ÄÃ­nskÃ½ konkurent firem jako OpenAI nebo Anthropic, dosud vydala modely jako DeepSeek-V2 s 236 miliardami parametrÅ¯.
- OtevÅ™enÃ½ kÃ³d metody podporuje Å¡irokÃ© vyuÅ¾itÃ­ v prÅ¯myslu.

### Podrobnosti
DeepSeek je ÄÃ­nskÃ¡ AI spoleÄnost zaloÅ¾enÃ¡ v roce 2023, kterÃ¡ se specializuje na vÃ½voj velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) s otevÅ™enÃ½m zdrojovÃ½m kÃ³dem. Mezi jejÃ­ dosavadnÃ­ ÃºspÄ›chy patÅ™Ã­ DeepSeek-V2, model s 236 miliardami parametrÅ¯, kterÃ½ dosahuje vÃ½konu srovnatelnÃ©ho s GPT-4 pÅ™i niÅ¾Å¡Ã­ch nÃ¡kladech na trÃ©novÃ¡nÃ­. NovÃ¡ metoda, oznaÄovanÃ¡ jako â€speculative scalingâ€œ nebo podobnÃ½ pÅ™Ã­stup, se zamÄ›Å™uje na optimalizaci fÃ¡ze trÃ©novÃ¡nÃ­, kde tradiÄnÃ­ postupy vyÅ¾adujÃ­ exponenciÃ¡lnÃ­ rÅ¯st vÃ½poÄetnÃ­ho vÃ½konu podle zÃ¡kona Å¡kÃ¡lovÃ¡nÃ­ (scaling laws od OpenAI).

KlÃ­ÄovÃ½m prvkem je dynamickÃ¡ alokace pozornosti v transformerech, kterÃ¡ umoÅ¾Åˆuje paralelizovat trÃ©nink napÅ™Ã­Ä menÅ¡Ã­m poÄtem grafickÃ½ch procesorÅ¯ (GPU), typicky NVIDIA H100 nebo A100. NapÅ™Ã­klad pÅ™i trÃ©novÃ¡nÃ­ modelu s 400 miliardami parametrÅ¯ lze podle DeepSeek snÃ­Å¾it poÄet potÅ™ebnÃ½ch GPU o 30â€“50 % oproti standardnÃ­m technikÃ¡m jako ZeRO nebo FSDP (Fully Sharded Data Parallel). Metoda takÃ© zahrnuje adaptivnÃ­ kompresi gradientÅ¯, coÅ¾ minimalizuje pÅ™enos dat mezi uzly v distribuovanÃ©m trÃ©ninku. To je pouÅ¾itelnÃ© pro vÃ½vojÃ¡Å™e, kteÅ™Ã­ chtÄ›jÃ­ trÃ©novat vlastnÃ­ LLM na cloudu nebo lokÃ¡lnÃ­ch clusterech, napÅ™Ã­klad pro specializovanÃ© aplikace v medicÃ­nÄ›, prÃ¡vu nebo programovÃ¡nÃ­.

V kontextu souÄasnÃ©ho AI prÅ¯myslu, kde nÃ¡klady na trÃ©novÃ¡nÃ­ modelÅ¯ pÅ™ekraÄujÃ­ stovky milionÅ¯ dolarÅ¯ (napÅ™. GPT-4 stÃ¡l odhadem 100 milionÅ¯ USD), pÅ™edstavuje tento pÅ™Ã­stup konkurenÄnÃ­ vÃ½hodu pro firmy s omezenÃ½mi zdroji. ÄŒÃ­na tak posiluje svou pozici v AI zÃ¡vodÄ›, kde stÃ¡tnÃ­ podpora umoÅ¾Åˆuje rychlÃ½ vÃ½voj. NicmÃ©nÄ› kritici upozorÅˆujÃ­, Å¾e metoda stÃ¡le zÃ¡visÃ­ na proprietÃ¡rnÃ­ch datech a hardware od NVIDIA, coÅ¾ omezuje plnou nezÃ¡vislost. Testy na benchmarkÃ¡ch jako MMLU nebo HumanEval ukazujÃ­ zlepÅ¡enÃ­ efektivity o 40 % oproti pÅ™edchozÃ­m verzÃ­m.

### ProÄ je to dÅ¯leÅ¾itÃ©
Tento pokrok ovlivnÃ­ celÃ½ ekosystÃ©m AI tÃ­m, Å¾e demokratizuje pÅ™Ã­stup k velkÃ½m modelÅ¯m. MenÅ¡Ã­ firmy a vÃ½zkumnÃ­ci budou moci Å¡kÃ¡lovat LLM bez obrovskÃ½ch investic do hardware, coÅ¾ urychlÃ­ inovace v oblastech jako autonomnÃ­ systÃ©my nebo personalizovanÃ¡ AI. V Å¡irÅ¡Ã­m kontextu posiluje ÄŒÃ­nu jako lÃ­dra v efektivnÃ­m AI vÃ½voji, coÅ¾ mÅ¯Å¾e vÃ©st k novÃ© fÃ¡zi soutÄ›Å¾e s USA, kde firmy jako xAI nebo Meta hledajÃ­ podobnÃ© optimalizace. DlouhodobÄ› to pÅ™ispÄ›je k pÅ™ekonÃ¡nÃ­ limitÅ¯ souÄasnÃ©ho hardware, blÃ­Å¾e k AGI, ale zÃ¡roveÅˆ zvyÅ¡uje potÅ™ebu regulacÃ­ kvÅ¯li energetickÃ© nÃ¡roÄnosti trÃ©ninku.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.businessinsider.com/deepseek-new-ai-training-models-scale-manifold-constrained-analysts-china-2026-1)

**Zdroj:** ğŸ“° Business Insider
