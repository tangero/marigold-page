---
author: Marisa Aigen
category: ai
companies:
- Anthropic
date: '2026-02-14 00:43:14'
description: AmerickÃ¡ armÃ¡da aktivnÄ› vyuÅ¾ila model umÄ›lÃ© inteligence Claude od Anthropic
  bÄ›hem operace na zatÄenÃ­ venezuelskÃ©ho prezidenta NicolÃ¡se Madura minulÃ½ mÄ›sÃ­c,
  podle zprÃ¡v serverÅ¯ Axios a Wall Street Journal. Firma Anthropic veÅ™ejnÄ› zdÅ¯razÅˆuje
  bezpeÄnostnÃ­ omezenÃ­ svÃ© technologie proti vojenskÃ©mu nasazenÃ­.
importance: 5
layout: tech_news_article
original_title: Pentagon used Claude AI to kidnap Maduro â€“ media
publishedAt: '2026-02-14T00:43:14+00:00'
slug: pentagon-used-claude-ai-to-kidnap-maduro-media
source:
  emoji: ğŸ“°
  id: rt
  name: RT
title: Pentagon pouÅ¾il Claude AI k zatÄenÃ­ Madura â€“ mÃ©dia
url: https://www.rt.com/news/632479-anthropic-pantagon-venezuela-raid/
urlToImage: https://mf.b37mrtl.ru/files/2026.02/article/698fc62a20302767f83d68b3.jpg
urlToImageBackup: https://mf.b37mrtl.ru/files/2026.02/article/698fc62a20302767f83d68b3.jpg
---

## Souhrn
AmerickÃ¡ armÃ¡da pouÅ¾ila model umÄ›lÃ© inteligence Claude od spoleÄnosti Anthropic pÅ™Ã­mo bÄ›hem smrtelnÃ© razie ve Venezuele, kterÃ¡ mÄ›la za cÃ­l zatknout prezidenta NicolÃ¡se Madura. Podle zprÃ¡v Axios a Wall Street Journal doÅ¡lo k poruÅ¡enÃ­ firemnÃ­ch zÃ¡sad Anthropic, kterÃ© zakazujÃ­ pouÅ¾itÃ­ technologie pro nÃ¡silÃ­, vÃ½voj zbranÃ­ nebo sledovÃ¡nÃ­. Role modelu Claude v operaci zÅ¯stÃ¡vÃ¡ nejasnÃ¡, ale naznaÄuje reÃ¡lnÃ© nasazenÃ­ AI v bojovÃ½ch podmÃ­nkÃ¡ch.

## KlÃ­ÄovÃ© body
- Pentagon nasadil Claude AI aktivnÄ› bÄ›hem samotnÃ© operace, nejen v pÅ™Ã­pravÄ›.
- Anthropic, kalifornskÃ¡ firma specializujÃ­cÃ­ se na bezpeÄnÃ© modely velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM), mÃ¡ politiky explicitnÄ› zakazujÃ­cÃ­ vojenskÃ© aplikace.
- ArmÃ¡da dÅ™Ã­ve pouÅ¾Ã­vala AI pro analÃ½zu satelitnÃ­ch snÃ­mkÅ¯ a zpravodajskÃ½ch dat v reÃ¡lnÃ©m Äase.
- ZprÃ¡vy vychÃ¡zejÃ­ z Axios a Wall Street Journal, publikovanÃ© v pÃ¡tek.
- Operace probÄ›hla minulÃ½ mÄ›sÃ­c, s vazbami na vrcholovÃ© pÅ™edstavitele jako Donald Trump, Pete Hegseth a John Ratcliffe.

## Podrobnosti
SpoleÄnost Anthropic, zaloÅ¾enÃ¡ v San Franciscu bÃ½valÃ½mi vÃ½zkumnÃ­ky OpenAI, se profiluje jako lÃ­dr v bezpeÄnÃ©m vÃ½voji AI. JejÃ­ model Claude, konkurent GPT od OpenAI nebo Gemini od Google, je navrÅ¾en s dÅ¯razem na "safeguards" â€“ bezpeÄnostnÃ­ mechanismy, kterÃ© majÃ­ brÃ¡nit zneuÅ¾itÃ­. Tyto mechanismy zahrnujÃ­ omezenÃ­ na poÅ¾adavky, kterÃ© by mohly vÃ©st k nÃ¡silÃ­, vÃ½voji zbranÃ­ nebo masovÃ©mu sledovÃ¡nÃ­. NapÅ™Ã­klad Claude odmÃ­tÃ¡ generovat instrukce pro sestavenÃ­ vÃ½buÅ¡nin nebo plÃ¡novÃ¡nÃ­ ÃºtokÅ¯. PÅ™esto, podle zprÃ¡v, americkÃ¡ armÃ¡da tento model pouÅ¾ila bÄ›hem operace v Venezuele, kterÃ¡ skonÄila smrtelnÃ½mi obÄ›Å¥mi a pokusem o zatÄenÃ­ Madura.

Axios a Wall Street Journal uvÃ¡dÄ›jÃ­, Å¾e Claude nebyl omezen na pÅ™edoperaÄnÃ­ fÃ¡zi, jako je plÃ¡novÃ¡nÃ­ nebo simulace, ale byl nasazen v reÃ¡lnÃ©m Äase. ArmÃ¡da mÃ¡ zkuÅ¡enosti s AI: dÅ™Ã­ve slouÅ¾ily modely k automatickÃ© analÃ½ze satelitnÃ­ch snÃ­mkÅ¯ pro detekci pohybu, rozpoznÃ¡vÃ¡nÃ­ vozidel nebo predikci chovÃ¡nÃ­ protivnÃ­kÅ¯ na zÃ¡kladÄ› zpravodajskÃ½ch dat. V tomto pÅ™Ã­padÄ› by Claude mohl pomÃ¡hat s okamÅ¾itou interpretacÃ­ dat z dronÅ¯, optimalizacÃ­ trasy jednotek nebo dokonce generovÃ¡nÃ­m taktickÃ½ch doporuÄenÃ­. Å½Ã¡dnÃ© detaily o pÅ™esnÃ©m nasazenÃ­ nejsou znÃ¡my, coÅ¾ vyvolÃ¡vÃ¡ otÃ¡zky o tom, jak byly obejdeny bezpeÄnostnÃ­ filtry â€“ moÅ¾nÃ¡ speciÃ¡lnÄ› upravenou verzÃ­ modelu nebo nepÅ™Ã­mÃ½m pouÅ¾itÃ­m.

Venezuela je dlouhodobÄ› terÄem americkÃ© zahraniÄnÃ­ politiky kvÅ¯li sankcÃ­m a kritice Madurova reÅ¾imu. Operace, zmÃ­nÄ›nÃ¡ v kontextu administrativy Donalda Trumpa s ministrynÃ­ Pete Hegsethem (pÅ™ezdÃ­vanÃ½m Secretary of War) a Å™editelem CIA Johnem Ratcliffem, podtrhuje eskalaci. RT.com, kterÃ½ ÄlÃ¡nek publikoval, Äasto cituje zÃ¡padnÃ­ zdroje, ale s ruskÃ½m Ãºhlem pohledu, coÅ¾ naznaÄuje geopolitickou motivaci zprÃ¡vy.

## ProÄ je to dÅ¯leÅ¾itÃ©
Toto odhalenÃ­ pÅ™edstavuje prÅ¯lomovÃ½ moment v debatÄ› o vojenskÃ©m nasazenÃ­ AI. Anthropic, kterÃ½ sbÃ­rÃ¡ miliardy na vÃ½voj bezpeÄnÃ½ch systÃ©mÅ¯, evidentnÄ› nedokÃ¡zal zabrÃ¡nit zneuÅ¾itÃ­ svÃ© technologie stÃ¡tnÃ­mi aktÃ©ry. To oslabuje dÅ¯vÄ›ru v soukromÃ© safeguardy a zdÅ¯razÅˆuje nutnost regulacÃ­ na Ãºrovni vlÃ¡dy, jako je navrhovanÃ½ AI Act v EU nebo exportnÃ­ kontroly v USA. Pro prÅ¯mysl znamenÃ¡ riziko: vojenskÃ© zakÃ¡zky mohou pÅ™evÃ¡Å¾it etickÃ© standardy, coÅ¾ urychlÃ­ zÃ¡vod v zbrojenÃ­ AI mezi USA, ÄŒÃ­nou a Ruskem. UÅ¾ivatelÃ© a vÃ½vojÃ¡Å™i budou nynÃ­ vÃ­ce skeptiÄtÃ­ k claimÅ¯m o bezpeÄnosti LLM, protoÅ¾e reÃ¡lnÃ© nasazenÃ­ v konfliktech mÅ¯Å¾e vÃ©st k eskalaci chyb â€“ napÅ™Ã­klad Å¡patnÃ© identifikaci cÃ­lÅ¯. V Å¡irÅ¡Ã­m ekosystÃ©mu to posiluje argumenty za otevÅ™enÃ½ zdroj AI, kde komunita mÅ¯Å¾e auditovat safeguardy, oproti uzavÅ™enÃ½m modelÅ¯m jako Claude.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.rt.com/news/632479-anthropic-pantagon-venezuela-raid/)

**Zdroj:** ğŸ“° RT
