---
author: Marisa Aigen
category: ai rizika
date: '2026-01-18 00:15:01'
description: V listopadu 2024 vydali prezidenti USA Joe Biden a ÄŒÃ­ny Si Å¤in-pching
  prvnÃ­ podstatnÃ© spoleÄnÃ© prohlÃ¡Å¡enÃ­ o nÃ¡rodnÄ›bezpeÄnostnÃ­ch rizicÃ­ch spojenÃ½ch s
  AI. Oba stÃ¡ty uznÃ¡vajÃ­ potÅ™ebu udrÅ¾ovat bezpeÄnost a stabilitu v tÃ©to oblasti.
importance: 4
layout: tech_news_article
original_title: China and America Must Get Serious About AI Risk
people:
- Joe Biden
- Xi Jinping
publishedAt: '2026-01-18T00:15:01+00:00'
slug: china-and-america-must-get-serious-about-ai-risk
source:
  emoji: ğŸ“°
  id: null
  name: Khabarhub.com
title: ÄŒÃ­na a USA se musÃ­ vÃ¡Å¾nÄ› zabÃ½vat riziky umÄ›lÃ© inteligence
url: https://english.khabarhub.com/2026/18/516408/
urlToImage: https://english.khabarhub.com/wp-content/uploads/2026/01/Khabar-Hub-Article_Jake-Sullivan_1200px-630X-px-Final_2026.01.16-Upload.jpg
urlToImageBackup: https://english.khabarhub.com/wp-content/uploads/2026/01/Khabar-Hub-Article_Jake-Sullivan_1200px-630X-px-Final_2026.01.16-Upload.jpg
---

## Souhrn
V listopadu 2024 na summitu APEC v Peru vydali prezident USA Joe Biden a prezident ÄŒÃ­ny Si Å¤in-pching prvnÃ­ podstatnÃ© spoleÄnÃ© prohlÃ¡Å¡enÃ­ zamÄ›Å™enÃ© na nÃ¡rodnÄ›bezpeÄnostnÃ­ rizika umÄ›lÃ© inteligence. Dokument zdÅ¯razÅˆuje shodu obou velmocÃ­ na nutnosti spoluprÃ¡ce pÅ™i Å™eÅ¡enÃ­ rizik AI, vÄetnÄ› prevence zneuÅ¾itÃ­ v konfliktech a zajiÅ¡tÄ›nÃ­ bezpeÄnÃ©ho vÃ½voje technologiÃ­. Toto prohlÃ¡Å¡enÃ­ pÅ™edstavuje krok k deeskalaci v globÃ¡lnÃ­m zÃ¡vodÄ› o AI nadvlÃ¡du.

## KlÃ­ÄovÃ© body
- PrvnÃ­ substantivnÃ­ spoleÄnÃ© prohlÃ¡Å¡enÃ­ USA a ÄŒÃ­ny o nÃ¡rodnÄ›bezpeÄnostnÃ­ch rizicÃ­ch AI.
- Shoda na potÅ™ebÄ› udrÅ¾ovat bezpeÄnost a stabilitu AI systÃ©mÅ¯.
- ZÃ¡vazek k dialogu o rizicÃ­ch, vÄetnÄ› testovÃ¡nÃ­ bezpeÄnosti modelÅ¯ a prevence misinformacÃ­.
- Odkaz na existujÃ­cÃ­ rÃ¡mce, jako jsou exportnÃ­ kontroly na AI Äipy.
- VÃ½zva k mezinÃ¡rodnÃ­ spoluprÃ¡ci bez omezenÃ­ technologickÃ©ho pokroku.

## Podrobnosti
ProhlÃ¡Å¡enÃ­ bylo publikovÃ¡no po bilaterÃ¡lnÃ­m setkÃ¡nÃ­ BidenovÃ½ch a SiovÃ½ch na okraji summitu APEC v LimÄ›, kde obÄ› strany uznaly, Å¾e rychlÃ½ vÃ½voj AI pÅ™inÃ¡Å¡Ã­ nejen pÅ™Ã­leÅ¾itosti, ale i zÃ¡vaÅ¾nÃ¡ rizika pro nÃ¡rodnÃ­ bezpeÄnost. Mezi klÃ­ÄovÃ© body patÅ™Ã­ uznÃ¡nÃ­ potÅ™eby "udrÅ¾ovat bezpeÄnost a stabilitu AI v systÃ©mech kritickÃ© infrastruktury", coÅ¾ zahrnuje ochranu pÅ™ed autonomnÃ­mi zbranÄ›mi, kybernetickÃ½mi Ãºtoky podporovanÃ½mi AI nebo ztrÃ¡tou kontroly nad pokroÄilÃ½mi modely, jako jsou velkÃ© jazykovÃ© modely (LLM). USA a ÄŒÃ­na se zavÃ¡zaly k pokraÄujÃ­cÃ­mu dialogu prostÅ™ednictvÃ­m pracovnÃ­ch skupin, kterÃ© by mÄ›ly diskutovat o standardech testovÃ¡nÃ­ bezpeÄnosti AI, vÄetnÄ› red-teamingovÃ½ch cviÄenÃ­, kde se modely ÃºmyslnÄ› testujÃ­ na zranitelnosti vÅ¯Äi jailbreakÅ¯m nebo manipulaci vÃ½stupÅ¯.

Toto prohlÃ¡Å¡enÃ­ navazuje na pÅ™edchozÃ­ napÄ›tÃ­, jako jsou americkÃ© exportnÃ­ restrikce na pokroÄilÃ© GPU Äipy od Nvidia, kterÃ© brÃ¡nÃ­ ÄŒÃ­nÄ› v pÅ™Ã­stupu k hardwaru nutnÃ©mu pro trÃ©nink velkÃ½ch AI modelÅ¯. ÄŒÃ­na naopak rozvÃ­jÃ­ vlastnÃ­ ekosystÃ©m s modely jako DeepSeek nebo Qwen od Alibaba, kterÃ© dosahujÃ­ vÃ½konu srovnatelnÃ©ho s GPT-4, ale bez zÃ¡padnÃ­ch ÄipÅ¯. ProhlÃ¡Å¡enÃ­ neÅ™eÅ¡Ã­ pÅ™Ã­mo tyto restrikce, ale naznaÄuje moÅ¾nost koordinace, aby se zabrÃ¡nilo eskalaci do technologickÃ© vÃ¡lky. DÃ¡le se zmiÅˆuje spoluprÃ¡ce proti AI-generovanÃ½m dezinformacÃ­m, coÅ¾ je aktuÃ¡lnÃ­ v kontextu voleb a geopolitickÃ½ch konfliktÅ¯, kde nÃ¡stroje jako deepfakes mohou destabilizovat spoleÄnosti.

V praxi to znamenÃ¡ pro prÅ¯mysl potÅ™ebu pÅ™izpÅ¯sobit se potenciÃ¡lnÃ­m spoleÄnÃ½m standardÅ¯m bezpeÄnosti. NapÅ™Ã­klad firmy jako OpenAI nebo Anthropic v USA jiÅ¾ implementujÃ­ bezpeÄnostnÃ­ vrstvy v modelech Claude, kterÃ© zabraÅˆujÃ­ generovÃ¡nÃ­ Å¡kodlivÃ©ho obsahu, zatÃ­mco ÄÃ­nskÃ© firmy jako Baidu s ErnÃ­m musÃ­ dodrÅ¾ovat stÃ¡tnÃ­ cenzuru. Pro uÅ¾ivatele to pÅ™edstavuje nadÄ›ji na mÃ©nÄ› rizikovÃ© AI nÃ¡stroje, ale zÃ¡roveÅˆ riziko zpomalenÃ­ inovacÃ­ kvÅ¯li regulacÃ­m. CelkovÄ› dokument nenÃ­ prÃ¡vnÄ› zÃ¡vaznÃ½, ale signalizuje zmÄ›nu od konfrontace k pragmatickÃ© spoluprÃ¡ci.

## ProÄ je to dÅ¯leÅ¾itÃ©
Toto prohlÃ¡Å¡enÃ­ mÄ›nÃ­ dynamiku globÃ¡lnÃ­ho AI ekosystÃ©mu, kde USA a ÄŒÃ­na kontrolujÃ­ pÅ™es 80 % vÃ½poÄetnÃ­ho vÃ½konu pro AI trÃ©nink. Bez spoluprÃ¡ce hrozÃ­ zÃ¡vod v zbrojenÃ­ s AI zbranÄ›mi, jako autonomnÃ­ drony nebo kybernetickÃ© systÃ©my schopnÃ© sebezdokonalovÃ¡nÃ­. Pro evropskÃ½ prÅ¯mysl, vÄetnÄ› ÄeskÃ½ch firem v AI jako GoodAI, to otevÃ­Å™e dveÅ™e k mezinÃ¡rodnÃ­m standardÅ¯m, kterÃ© by mohly sjednotit bezpeÄnostnÃ­ poÅ¾adavky podle EU AI Act. DlouhodobÄ› sniÅ¾uje rizika existenciÃ¡lnÃ­ch hrozeb, jako je AGI mimo kontrolu, a podporuje udrÅ¾itelnÃ½ vÃ½voj. NicmÃ©nÄ› skepticky lze konstatovat, Å¾e bez konkrÃ©tnÃ­ch krokÅ¯, jako sdÃ­lenÃ­ dat o incidentech, zÅ¯stane deklaracÃ­ bez reÃ¡lnÃ©ho dopadu.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://english.khabarhub.com/2026/18/516408/)

**Zdroj:** ğŸ“° Khabarhub.com
