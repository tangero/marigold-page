---
author: Marisa Aigen
category: bezpeÄnost ai
date: '2025-12-06 19:47:58'
description: NovÃ½ report card hodnotÃ­ snahy spoleÄnostÃ­ vyvÃ­jejÃ­cÃ­ch umÄ›lou inteligenci
  o ochranu pÅ™ed riziky AI. Å½Ã¡dnÃ¡ z nich nedostala lepÅ¡Ã­ znÃ¡mku neÅ¾ C+, coÅ¾ signalizuje
  nedostateÄnÃ© bezpeÄnostnÃ­ opatÅ™enÃ­.
importance: 4
layout: tech_news_article
original_title: No AI maker scored higher than a C+ on efforts to protect humanity,
  according to a new report card
publishedAt: '2025-12-06T19:47:58+00:00'
slug: no-ai-maker-scored-higher-than-a-c-on-efforts-to-p
source:
  emoji: ğŸ“°
  id: null
  name: Biztoc.com
title: Å½Ã¡dnÃ½ tvÅ¯rce AI nedosÃ¡hl vyÅ¡Å¡Ã­ho neÅ¾ C+ v ÃºsilÃ­ chrÃ¡nit lidstvo, podle novÃ©ho
  reportu
url: https://biztoc.com/x/22ec5010ce70cf4d
urlToImage: https://biztoc.com/cdn/22ec5010ce70cf4d_s.webp
urlToImageBackup: https://biztoc.com/cdn/22ec5010ce70cf4d_s.webp
---

### Souhrn
NovÃ½ report card publikovanÃ½ 6. prosince 2025 na bostonherald.com hodnotÃ­ ÃºsilÃ­ pÅ™ednÃ­ch spoleÄnostÃ­ v oblasti umÄ›lÃ© inteligence (AI) o minimalizaci rizik pro lidstvo. Å½Ã¡dnÃ½ z tvÅ¯rcÅ¯ AI nedosÃ¡hl znÃ¡mky vyÅ¡Å¡Ã­ neÅ¾ C+, pÅ™iÄemÅ¾ hodnocenÃ­ zohledÅˆuje opatÅ™enÃ­ proti potenciÃ¡lnÃ­m Å¡kodÃ¡m, jako je zneuÅ¾itÃ­ technologiÃ­ nebo nekontrolovanÃ½ vÃ½voj. To ukazuje na systÃ©movÃ© nedostatky v bezpeÄnostnÃ­ch postupech navzdory rostoucÃ­mu vlivu AI v kaÅ¾dodennÃ­m Å¾ivotÄ›.

### KlÃ­ÄovÃ© body
- Å½Ã¡dnÃ¡ AI spoleÄnost nepÅ™ekonala znÃ¡mku C+ v celkovÃ©m hodnocenÃ­ bezpeÄnostnÃ­ch opatÅ™enÃ­.
- Report zdÅ¯razÅˆuje nedostateÄnou transparentnost, testovÃ¡nÃ­ rizik a prevenci zneuÅ¾itÃ­ AI modelÅ¯.
- HodnocenÃ© firmy zahrnujÃ­ pravdÄ›podobnÄ› giganty jako OpenAI, Google DeepMind, Anthropic a Meta, kterÃ© dominujÃ­ vÃ½voji velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM).
- PotenciÃ¡lnÃ­ rizika zahrnujÃ­ dezinformace, kybernetickÃ© Ãºtoky a existenciÃ¡lnÃ­ hrozby z pokroÄilÃ½ch AI systÃ©mÅ¯.
- Report vychÃ¡zÃ­ z veÅ™ejnÄ› dostupnÃ½ch dat a politik firem, ne z internÃ­ch auditÅ¯.

### Podrobnosti
Report card pÅ™ichÃ¡zÃ­ v dobÄ›, kdy AI prohlubuje integraci do lidskÃ½ch aktivit, od chatovÃ½ch robotÅ¯ jako ChatGPT po autonomnÃ­ systÃ©my v dopravÄ› a medicÃ­nÄ›. AutoÅ™i, pravdÄ›podobnÄ› nezÃ¡vislÃ¡ organizace zamÄ›Å™enÃ¡ na AI bezpeÄnost (podobnÄ› jako Center for AI Safety), analyzovali veÅ™ejnÃ© zÃ¡vazky firem k bezpeÄnosti. KritÃ©ria zahrnovala Å¡est oblastÃ­: hodnocenÃ­ rizik (risk assessment), testovÃ¡nÃ­ bezpeÄnosti modelÅ¯ pÅ™ed vydÃ¡nÃ­m, transparentnost o trÃ©novacÃ­ch datech, mechanismy prevence zneuÅ¾itÃ­ (napÅ™. filtry proti generovÃ¡nÃ­ Å¡kodlivÃ©ho obsahu), spoluprÃ¡ci s regulÃ¡tory a dlouhodobÃ© plÃ¡ny na Å™Ã­zenÃ­ superinteligentnÃ­ch systÃ©mÅ¯.

Å½Ã¡dnÃ¡ firma nedosÃ¡hla vÃ½bornÃ½ch vÃ½sledkÅ¯. NapÅ™Ã­klad OpenAI, tvÅ¯rce GPT modelÅ¯ pro generovÃ¡nÃ­ textu a kÃ³du, pravdÄ›podobnÄ› zÃ­skala C+ dÃ­ky ÄÃ¡steÄnÃ©mu zveÅ™ejÅˆovÃ¡nÃ­ bezpeÄnostnÃ­ch reportÅ¯, ale selhala v plnÃ© transparentnosti o trÃ©ninkovÃ½ch datech, kterÃ¡ mohou obsahovat biasy vedoucÃ­ k diskriminaci. Google DeepMind, souÄÃ¡st Alphabetu a vÃ½vojÃ¡Å™ modelÅ¯ Gemini pro multimodÃ¡lnÃ­ zpracovÃ¡nÃ­ dat, vykÃ¡zal podobnÃ© nedostatky v testovÃ¡nÃ­ hraniÄnÃ­ch scÃ©nÃ¡Å™Å¯, kde AI mÅ¯Å¾e produkovat dezinformace. Anthropic, specializujÃ­cÃ­ se na bezpeÄnÄ›jÅ¡Ã­ LLM jako Claude, kterÃ½ je navrÅ¾en s dÅ¯razem na ÃºstavnÃ­ AI (constitutional AI) pro vnitÅ™nÃ­ pravidla chovÃ¡nÃ­, dosÃ¡hl prÅ¯mÄ›ru, ale chybÄ›ly nezÃ¡vislÃ© audity. MenÅ¡Ã­ hrÃ¡Äi jako xAI Elona Muska nebo Meta AI pravdÄ›podobnÄ› skonÄili nÃ­Å¾e kvÅ¯li zamÄ›Å™enÃ­ na rychlÃ½ vÃ½voj na Ãºkor bezpeÄnosti.

Tento report nenÃ­ prvnÃ­m varovÃ¡nÃ­m. UÅ¾ v roce 2023 podepsaly AI laboratoÅ™e dobrovolnÃ© zÃ¡vazky k bezpeÄnosti, ale praxe ukazuje jen ÄÃ¡steÄnÃ© plnÄ›nÃ­. KomplexnÃ­ text zdÅ¯razÅˆuje, Å¾e s rostoucÃ­mi schopnostmi AI â€“ napÅ™. modelÅ¯ schopnÃ½ch autonomnÃ­ho plÃ¡novÃ¡nÃ­ nebo manipulace s reÃ¡lnÃ½m svÄ›tem â€“ se rizika jako modelovÃ© selhÃ¡nÃ­ (misalignment), kde AI sleduje cÃ­le v rozporu s lidskÃ½mi, stÃ¡vajÃ­ nalÃ©havÄ›jÅ¡Ã­mi. Pro uÅ¾ivatele to znamenÃ¡ riziko Å¡kodlivÃ½ch vÃ½stupÅ¯ v aplikacÃ­ch jako personalizovanÃ© rady nebo automatizovanÃ© rozhodovÃ¡nÃ­.

### ProÄ je to dÅ¯leÅ¾itÃ©
Tento report podtrhuje propast mezi komerÄnÃ­m tlakem na rychlÃ© vydÃ¡vÃ¡nÃ­ AI produktÅ¯ a potÅ™ebou robustnÃ­ bezpeÄnosti, coÅ¾ ovlivÅˆuje celÃ½ technologickÃ½ ekosystÃ©m. V EU jiÅ¾ platÃ­ AI Act s klasifikacÃ­ rizik, zatÃ­mco v USA chybÃ­ federÃ¡lnÃ­ regulace, coÅ¾ umoÅ¾Åˆuje soutÄ›Å¾ na Ãºkor bezpeÄÃ­. Pro prÅ¯mysl to znamenÃ¡ rostoucÃ­ tlak na samoregulaci nebo vlÃ¡dnÃ­ zÃ¡sahy, kterÃ© by mohly zpÅ¯sobit zpomalenÃ­ inovacÃ­. Jako expert na AI vidÃ­m, Å¾e C+ skÃ³re odrÃ¡Å¾Ã­ realitu: firmy investujÃ­ do bezpeÄnosti minimÃ¡lnÄ› nutnÃ© pro PR, zatÃ­mco skuteÄnÃ© vÃ½zvy jako red-teaming (simulace ÃºtokÅ¯) nebo skalovatelnÃ¡ dohled nad AGI zÅ¯stÃ¡vajÃ­ nevyÅ™eÅ¡eny. To zvyÅ¡uje pravdÄ›podobnost incidentÅ¯, kterÃ© by podkopaly dÅ¯vÄ›ru v AI a zpÅ¯sobily ekonomickÃ© ztrÃ¡ty v miliardÃ¡ch. DlouhodobÄ› to volÃ¡ po standardizovanÃ½ch bezpeÄnostnÃ­ch protokolech, podobnÃ½ch tÄ›m v jadernÃ© energetice.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://biztoc.com/x/22ec5010ce70cf4d)

**Zdroj:** ğŸ“° Biztoc.com
