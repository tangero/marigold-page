---
author: Marisa Aigen
category: umÄ›lÃ¡ inteligence
companies:
- Google
- OpenAI
- DeepSeek
date: '2026-02-14 11:02:11'
description: Google a OpenAI varujÃ­, Å¾e konkurenti vÄetnÄ› ÄÃ­nskÃ© DeepSeek zkoumajÃ­
  jejich modely AI, aby ukradli zÃ¡kladnÃ­ uvaÅ¾ovacÃ­ mechanismy a pÅ™evzali je do svÃ½ch
  systÃ©mÅ¯. Tento proces nazÃ½vÃ¡ Google â€distillation attacksâ€œ a detekoval ho v reÃ¡lnÃ©m
  Äase na modelu Gemini.
importance: 5
layout: tech_news_article
original_title: 'How AI could eat itself: Competitors can probe models to steal their
  secrets and clone them'
publishedAt: '2026-02-14T11:02:11+00:00'
slug: how-ai-could-eat-itself-competitors-can-probe-mode
source:
  emoji: ğŸ“°
  id: null
  name: Theregister.com
title: 'Jak by se AI mohla seÅ¾rat sama: Konkurenti mohou sondovat modely, ukrÃ¡st jejich
  tajemstvÃ­ a klonovat je'
url: https://www.theregister.com/2026/02/14/ai_risk_distillation_attacks/
urlToImage: https://regmedia.co.uk/2017/02/21/clone_army_star_wars.jpg
urlToImageBackup: https://regmedia.co.uk/2017/02/21/clone_army_star_wars.jpg
---

### Souhrn
DvÄ› nejvÄ›tÅ¡Ã­ firmy v oblasti AI, Google a OpenAI, tento tÃ½den upozornily na hrozbu, kdy konkurenti jako ÄÃ­nskÃ¡ DeepSeek systematicky testujÃ­ jejich velkÃ© jazykovÃ© modely (LLM), aby zÃ­skali data o jejich internÃ­m uvaÅ¾ovÃ¡nÃ­ a nÃ¡slednÄ› tyto schopnosti napodobily ve svÃ½ch vlastnÃ­ch modelech. Google tento typ Ãºtoku oznaÄuje jako â€distillation attacksâ€œ, pÅ™i kterÃ½ch ÃºtoÄnÃ­ci pouÅ¾Ã­vajÃ­ tisÃ­ce promptÅ¯ k extrakci logiky modelu. I pÅ™es detekci v reÃ¡lnÃ©m Äase zÅ¯stÃ¡vÃ¡ toto riziko obtÃ­Å¾nÄ› odstranitelnÃ©.

### KlÃ­ÄovÃ© body
- Google Threat Intelligence Group identifikoval kampaÅˆ s vÃ­ce neÅ¾ 100 000 prompty zamÄ›Å™enou na replikaci uvaÅ¾ovÃ¡nÃ­ modelu Gemini v neanglickÃ½ch jazycÃ­ch.
- ÃštoÄnÃ­ci jsou soukromÃ© firmy z celÃ©ho svÄ›ta, vÄetnÄ› ÄŒÃ­ny, kterÃ© chtÄ›jÃ­ levnÄ›ji napodobit drahÃ© LLM za miliardy dolarÅ¯.
- DeepSeek, ÄÃ­nskÃ¡ firma specializujÃ­cÃ­ se na vÃ½voj open-source AI modelÅ¯, je zmÃ­nÄ›na jako pÅ™Ã­klad takovÃ©ho aktÃ©ra.
- Distilace poruÅ¡uje podmÃ­nky sluÅ¾by Google a ohroÅ¾uje hodnotu duÅ¡evnÃ­ho vlastnictvÃ­ v AI.
- Google ochrÃ¡nil svÃ© internÃ­ stopy uvaÅ¾ovÃ¡nÃ­, ale riziko nelze plnÄ› eliminovat.

### Podrobnosti
ÄŒlÃ¡nek popisuje novÃ½ typ intelektuÃ¡lnÃ­ho vlastnictvÃ­ v oblasti AI, kde konkurenti zneuÅ¾Ã­vajÃ­ legitimnÃ­ pÅ™Ã­stup k veÅ™ejnÃ½m rozhranÃ­m (API) modelÅ¯ jako Gemini nebo GPT. Tyto modely, trÃ©novanÃ© na miliardÃ¡ch dolarÅ¯, skrÃ½vajÃ­ sloÅ¾itÃ© internÃ­ procesy uvaÅ¾ovÃ¡nÃ­, kterÃ© umoÅ¾ÅˆujÃ­ Å™eÅ¡it Ãºlohy od pÅ™ekladu po programovÃ¡nÃ­. ÃštoÄnÃ­ci posÃ­lajÃ­ tisÃ­ce specifickÃ½ch promptÅ¯ â€“ textovÃ½ch instrukcÃ­ â€“, aby mapovali, jak model reaguje na rÅ¯znÃ© scÃ©nÃ¡Å™e. ZÃ­skanÃ¡ data pak slouÅ¾Ã­ k trÃ©ninku vlastnÃ­ch menÅ¡Ã­ch modelÅ¯, coÅ¾ dramaticky sniÅ¾uje nÃ¡klady na vÃ½voj.

John Hultquist z Google Threat Intelligence Group uvedl, Å¾e jde o soukromÃ© firmy z celÃ©ho svÄ›ta, kterÃ© vidÃ­ v AI klÃ­Äovou technologii s nekoneÄnÃ½m zÃ¡jmem o replikaci. KonkrÃ©tnÄ› kampaÅˆ na Gemini pouÅ¾ila pÅ™es 100 000 promptÅ¯ v neanglickÃ½ch jazycÃ­ch napÅ™Ã­Ä Å¡irokou Å¡kÃ¡lou ÃºkolÅ¯, aby zachytila uvaÅ¾ovacÃ­ schopnosti. Google to detekoval v reÃ¡lnÃ©m Äase a zablokoval pÅ™Ã­stup k citlivÃ½m stopÃ¡m (reasoning traces), kterÃ© ukazujÃ­ krok za krokem myÅ¡lenÃ­ modelu. OpenAI podobnÄ› varuje pÅ™ed takovÃ½mi praktikami.

DeepSeek, ÄÃ­nskÃ¡ spoleÄnost zamÄ›Å™enÃ¡ na vÃ½voj vÃ½konnÃ½ch open-source LLM jako DeepSeek-V2, je explicitnÄ› zmÃ­nÄ›na jako pÅ™Ã­klad. Tyto modely slouÅ¾Ã­ k tvorbÄ› textu, kÃ³du nebo analÃ½z a konkurujÃ­ zÃ¡padnÃ­m alternativÃ¡m niÅ¾Å¡Ã­mi nÃ¡klady. Distilace umoÅ¾Åˆuje firmÃ¡m jako DeepSeek pÅ™eskoÄit fÃ¡zi drahÃ©ho trÃ©ninku na obÅ™Ã­ch datech a GPU clusterech, coÅ¾ urychluje globÃ¡lnÃ­ soutÄ›Å¾. NicmÃ©nÄ› toto zneuÅ¾itÃ­ otevÃ­rÃ¡ dveÅ™e k dalÅ¡Ã­m rizikÅ¯m, jako je Ãºnik proprietÃ¡rnÃ­ch znalostÃ­ nebo zneuÅ¾itÃ­ k Å¡kodlivÃ½m ÃºÄelÅ¯m.

### ProÄ je to dÅ¯leÅ¾itÃ©
Tento vÃ½voj odhaluje zranitelnost AI ekosystÃ©mu, kde miliardovÃ© investice do LLM mohou bÃ½t snadno napodobeny, coÅ¾ oslabuje konkurenÄnÃ­ vÃ½hodu firem jako Google nebo OpenAI. Pro prÅ¯mysl znamenÃ¡ levnÄ›jÅ¡Ã­ vstup do AI pro menÅ¡Ã­ hrÃ¡Äe, vÄetnÄ› stÃ¡tÅ¯ jako ÄŒÃ­na, coÅ¾ urychluje globÃ¡lnÃ­ proliferaci pokroÄilÃ½ch systÃ©mÅ¯, ale zÃ¡roveÅˆ zvyÅ¡uje rizika jako Å¡Ã­Å™enÃ­ dezinformacÃ­ nebo kybernetickÃ© Ãºtoky. UÅ¾ivatelÃ© pocÃ­tÃ­ dopady v podobÄ› rychlejÅ¡Ã­ho nasazenÃ­ levnÃ½ch klonÅ¯, kterÃ© mohou bÃ½t mÃ©nÄ› bezpeÄnÃ©. DlouhodobÄ› to nutÃ­ vÃ½vojÃ¡Å™e k posÃ­lenÃ­ ochrany, napÅ™Ã­klad omezenÃ­m API nebo vodoznaky v vÃ½stupech, coÅ¾ ovlivnÃ­ dostupnost AI pro Å¡irokou veÅ™ejnost. V kontextu eskalujÃ­cÃ­ AI zÃ¡vodnosti to podtrhuje potÅ™ebu novÃ½ch standardÅ¯ pro ochranu IP v digitÃ¡lnÃ­ Ã©Å™e.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.theregister.com/2026/02/14/ai_risk_distillation_attacks/)

**Zdroj:** ğŸ“° Theregister.com
