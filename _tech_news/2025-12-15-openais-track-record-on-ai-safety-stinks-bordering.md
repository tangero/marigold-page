---
author: Marisa Aigen
category: bezpeÄnost ai
companies:
- OpenAI
date: '2025-12-15 15:46:59'
description: AlespoÅˆ dva vÃ½zkumnÃ­ci nedÃ¡vno opustili OpenAI kvÅ¯li obavÃ¡m, Å¾e spoleÄnost
  nebyla zcela upÅ™Ã­mnÃ¡ a pÅ™Ã­liÅ¡ opatrnÃ¡ pÅ™i publikovÃ¡nÃ­ vÃ½zkumu, zejmÃ©na pokud se
  tÃ½kal negativnÃ­ch aspektÅ¯ technologie.
importance: 4
layout: tech_news_article
original_title: OpenAI's track record on AI safety stinks â€” bordering on â€œfunctioning
  as a de facto advocacy armâ€ rather than a genuine research lab
publishedAt: '2025-12-15T15:46:59+00:00'
slug: openais-track-record-on-ai-safety-stinks-bordering
source:
  emoji: ğŸ“°
  id: null
  name: Windows Central
title: Bilance OpenAI v oblasti bezpeÄnosti AI je Å¡patnÃ¡ â€“ spÃ­Å¡ funguje jako zastupitelskÃ½
  orgÃ¡n neÅ¾ skuteÄnÃ© vÃ½zkumnÃ© laboratoÅ™
url: https://www.windowscentral.com/artificial-intelligence/openai-chatgpt/openais-track-record-on-ai-safety-stinks-bordering-on-functioning-as-a-de-facto-advocacy-arm-rather-than-a-genuine-research-lab
urlToImage: https://cdn.mos.cms.futurecdn.net/icEsbwynRpnZAFBCXUbpPg-2560-80.jpg
urlToImageBackup: https://cdn.mos.cms.futurecdn.net/icEsbwynRpnZAFBCXUbpPg-2560-80.jpg
---

### Souhrn
NedÃ¡vno opustili OpenAI alespoÅˆ dva vÃ½zkumnÃ­ci, kteÅ™Ã­ upozornili na neupÅ™Ã­mnost a pÅ™Ã­liÅ¡nou opatrnost firmy pÅ™i zveÅ™ejÅˆovÃ¡nÃ­ vÃ½zkumu o AI, zejmÃ©na pokud jde o negativnÃ­ dopady nebo ekonomickÃ© rizika. Mezi odchÃ¡zejÃ­cÃ­mi byl ekonomickÃ½ vÃ½zkumnÃ­k Tom Cunningham, kterÃ½ ve vnitÅ™nÃ­ zprÃ¡vÄ› oznaÄil tÃ½m za â€de facto zastupitelskÃ½ orgÃ¡n OpenAIâ€œ mÃ­sto opravdovÃ©ho vÃ½zkumnÃ©ho centra. HlavnÃ­ strategickÃ½ Å™editel Jason Kwon tyto kritiky odmÃ­tl a zdÅ¯raznil, Å¾e OpenAI musÃ­ nejen poukazovat na problÃ©my, ale i nabÃ­zet Å™eÅ¡enÃ­.

### KlÃ­ÄovÃ© body
- Odchod Toma Cunninghama a dalÅ¡Ã­ho vÃ½zkumnÃ­ka kvÅ¯li posunu od objektivnÃ­ho vÃ½zkumu k propagaci.
- Obavy se tÃ½kaly zatajovÃ¡nÃ­ negativnÃ­ch vÃ½sledkÅ¯ vÃ½zkumu o bezpeÄnosti AI a ekonomickÃ½ch dopadech.
- VnitÅ™nÃ­ memo od Jasona Kwona obhajuje aktivnÃ­ roli OpenAI jako lÃ­dra v nasazovÃ¡nÃ­ AI.
- Kontext: OpenAI ÄelÃ­ tlaku investorÅ¯ na pÅ™echod k for-profit modelu a intenzivnÃ­ konkurenci od Anthropic Äi Google.
- Report pochÃ¡zÃ­ z WIRED a odhaluje vnitÅ™nÃ­ napÄ›tÃ­ v oblasti AI safety.

### Podrobnosti
OpenAI, firma znÃ¡mÃ¡ vÃ½vojem modelÅ¯ jako GPT sÃ©rie pro generovÃ¡nÃ­ textu, konverzaci a dalÅ¡Ã­ Ãºlohy zpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka, prochÃ¡zÃ­ turbulentnÃ­m obdobÃ­m. KromÄ› tlaku investorÅ¯ na zmÄ›nu na ziskovÄ› orientovanou spoleÄnost, coÅ¾ by otevÅ™elo dveÅ™e moÅ¾nÃ½m pÅ™evzetÃ­m, a internÃ­ho â€kÃ³dovÃ©ho ÄervenÃ©hoâ€œ alarmu pro zlepÅ¡enÃ­ ChatGPT kvÅ¯li konkurenci, teÄ ÄelÃ­ kritice z Å™ad vlastnÃ­ch zamÄ›stnancÅ¯. Podle reportÃ¡Å¾e WIRED zveÅ™ejnÄ›nÃ© nedÃ¡vno odeÅ¡li alespoÅˆ dva vÃ½zkumnÃ­ci s odÅ¯vodnÄ›nÃ­m, Å¾e OpenAI pÅ™i publikovÃ¡nÃ­ vÃ½zkumu tajÃ­ nebo minimalizuje negativnÃ­ nÃ¡lezy. KonkrÃ©tnÄ› se to tÃ½kÃ¡ zprÃ¡v, kterÃ© by ukazovaly rizika AI, jako jsou bezpeÄnostnÃ­ zranitelnosti nebo Å¡irÅ¡Ã­ ekonomickÃ© dopady, napÅ™Ã­klad ztrÃ¡ty pracovnÃ­ch mÃ­st v dÅ¯sledku automatizace.

Tom Cunningham, bÃ½valÃ½ ekonomickÃ½ vÃ½zkumnÃ­k OpenAI zabÃ½vajÃ­cÃ­ se dopady AI na trh prÃ¡ce a ekonomiku, ve svÃ© vnitÅ™nÃ­ rozluÄkovÃ© zprÃ¡vÄ› pro kolegy napsal, Å¾e tÃ½m se odklÃ¡nÃ­ od pÅ¯vodnÃ­ho cÃ­le â€“ provÃ¡dÄ›t nezÃ¡vislÃ½ vÃ½zkum â€“ a promÄ›Åˆuje se v â€de facto zastupitelskÃ½ orgÃ¡n pro OpenAIâ€œ. To znamenÃ¡, Å¾e vÃ½zkum slouÅ¾Ã­ spÃ­Å¡ k podpoÅ™e firemnÃ­ agendy neÅ¾ k objektivnÃ­mu zkoumÃ¡nÃ­ rizik. OpenAI reagovalo internÃ­m memem od hlavnÃ­ho strategickÃ©ho Å™editele Jasona Kwona, kterÃ½ tvrdÃ­: â€NenÃ­ to o tom, Å¾e bychom o tÄ›chto tÃ©matech nemÄ›li mluvit. Jsme nejen vÃ½zkumnou institucÃ­, ale takÃ© aktÃ©rem, kterÃ½ AI nasazuje do svÄ›ta, a proto musÃ­me nÃ©st odpovÄ›dnost za vÃ½sledky.â€œ Kwon zdÅ¯raznil potÅ™ebu nejen identifikovat problÃ©my s AI, ale i budovat Å™eÅ¡enÃ­, coÅ¾ podtrhuje aktivnÃ­ roli firmy v ekosystÃ©mu umÄ›lÃ© inteligence.

Tato situace nenÃ­ izolanÃ¡. OpenAI nedÃ¡vno uzavÅ™elo partnerstvÃ­ s vlÃ¡dnÃ­mi institucemi a korporacemi, coÅ¾ zvyÅ¡uje tlak na pozitivnÃ­ veÅ™ejnÃ½ obraz. Firmy jako Anthropic, kterÃ¡ se specializuje na bezpeÄnÄ›jÅ¡Ã­ AI modely jako Claude pro Ãºlohy podobnÃ© GPT, nebo Google s Gemini, tlaÄÃ­ na OpenAI v inovacÃ­ch i bezpeÄnosti. Kritika z vnitÅ™ka naznaÄuje systÃ©movÃ© problÃ©my v tom, jak velkÃ© AI laboratoÅ™e balansujÃ­ mezi vÃ½zkumem a komerÄnÃ­mi zÃ¡jmy.

### ProÄ je to dÅ¯leÅ¾itÃ©
Tato zprÃ¡va oslabuje dÅ¯vÄ›ru v OpenAI jako lÃ­dra v AI safety, coÅ¾ je klÃ­ÄovÃ© pro regulace a etickÃ© nasazenÃ­ technologiÃ­ jako velkÃ© jazykovÃ© modely (LLM). Pokud vÃ½zkum nenÃ­ transparentnÃ­, hrozÃ­ podcenÄ›nÃ­ rizik, jako jsou halucinace modelÅ¯, bias v datech nebo ekonomickÃ¡ disrupce. Pro prÅ¯mysl to znamenÃ¡ vÄ›tÅ¡Ã­ tlak na nezÃ¡vislÃ© audity a open-source alternativy jako Llama od Meta. V Å¡irÅ¡Ã­m kontextu posiluje debatu o potÅ™ebÄ› regulacÃ­ EU AI Act nebo nÃ¡rodnÃ­ch rÃ¡mcÅ¯, kde integrity vÃ½zkumu rozhoduje o bezpeÄnÃ©m vÃ½voji AGI. UÅ¾ivatelÃ© by mÄ›li bÃ½t opatrnÃ­ pÅ™i spolÃ©hÃ¡nÃ­ na AI nÃ¡stroje bez ovÄ›Å™enÃ½ch bezpeÄnostnÃ­ch dat, coÅ¾ mÅ¯Å¾e ovlivnit adopci v kritickÃ½ch oblastech jako medicÃ­na nebo finance.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.windowscentral.com/artificial-intelligence/openai-chatgpt/openais-track-record-on-ai-safety-stinks-bordering-on-functioning-as-a-de-facto-advocacy-arm-rather-than-a-genuine-research-lab)

**Zdroj:** ğŸ“° Windows Central
