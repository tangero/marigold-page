---
author: Marisa Aigen
category: ai
companies:
- Anthropic
date: '2026-02-25 01:08:46'
description: SpoleÄnost Anthropic, kterÃ¡ se profilovala jako nejbÄ›Å¾nÄ›jÅ¡Ã­ v oblasti
  AI bezpeÄnosti, ruÅ¡Ã­ centrÃ¡lnÃ­ pilÃ­Å™ svÃ© politiky Responsible Scaling Policy. Firma
  nynÃ­ nebude trÃ©novat pokroÄilÃ© AI modely pouze na zÃ¡kladÄ› pÅ™edchozÃ­ho zaruÄenÃ­ bezpeÄnostnÃ­ch
  opatÅ™enÃ­, kvÅ¯li tlaku konkurence.
importance: 4
layout: tech_news_article
original_title: Anthropic Drops Flagship Safety Pledge
publishedAt: '2026-02-25T01:08:46+00:00'
slug: anthropic-drops-flagship-safety-pledge
source:
  emoji: ğŸ“°
  id: time
  name: Time
title: Anthropic opouÅ¡tÃ­ svÅ¯j hlavnÃ­ bezpeÄnostnÃ­ zÃ¡vazek
url: https://time.com/7380854/exclusive-anthropic-drops-flagship-safety-pledge/
urlToImage: https://api.time.com/wp-content/uploads/2026/02/Anthropic.jpg?quality=85&w=1200&h=628&crop=1
urlToImageBackup: https://api.time.com/wp-content/uploads/2026/02/Anthropic.jpg?quality=85&w=1200&h=628&crop=1
---

## Souhrn
SpoleÄnost Anthropic, vÃ½vojÃ¡Å™ modelÅ¯ Claude, opouÅ¡tÃ­ klÃ­ÄovÃ½ zÃ¡vazek svÃ© bezpeÄnostnÃ­ politiky Responsible Scaling Policy (RSP). DÅ™Ã­ve slibovala, Å¾e nebude trÃ©novat AI systÃ©my nad urÄitou ÃºroveÅˆ, pokud nemÅ¯Å¾e pÅ™edem zaruÄit adekvÃ¡tnÃ­ bezpeÄnostnÃ­ opatÅ™enÃ­. NovÃ¡ verze politiky toto omezenÃ­ ruÅ¡Ã­ a nahrazuje ho sliby vÄ›tÅ¡Ã­ transparentnosti a odloÅ¾enÃ­ vÃ½voje jen za specifickÃ½ch podmÃ­nek.

## KlÃ­ÄovÃ© body
- Anthropic ruÅ¡Ã­ zÃ¡vazek k nepokraÄovÃ¡nÃ­ v trÃ©novÃ¡nÃ­ AI bez pÅ™edchozÃ­ho zaruÄenÃ­ bezpeÄnosti.
- DÅ¯vod: RychlÃ½ pokrok v AI a akce konkurentÅ¯, jako OpenAI, ÄinÃ­ unilaterÃ¡lnÃ­ zÃ¡vazky neudrÅ¾itelnÃ½mi.
- NovÃ¡ RSP zahrnuje vÄ›tÅ¡Ã­ transparentnost o bezpeÄnostnÃ­ch testech vlastnÃ­ch modelÅ¯.
- Firma slibuje pÅ™ekonÃ¡vat bezpeÄnostnÃ­ snahy konkurentÅ¯ a odloÅ¾it vÃ½voj, pokud bude lÃ­drem v AI zÃ¡vodÄ› a rizika katastrofy budou vÃ½znamnÃ¡.
- ZmÄ›na znamenÃ¡ mÃ©nÄ› striktnÃ­ sebeomezenÃ­ oproti pÅ¯vodnÃ­ verzi.

## Podrobnosti
Anthropic, americkÃ¡ firma zamÄ›Å™enÃ¡ na vÃ½voj velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) jako Claude, se v roce 2023 zavÃ¡zala k Responsible Scaling Policy (RSP), jejÃ­mÅ¾ jÃ¡drem byl princip, Å¾e nebude trÃ©novat AI systÃ©my nad urÄitÃ½mi vÃ½konovÃ½mi hranicemi â€“ napÅ™Ã­klad na Ãºrovni ASL-3 (Artificial System Level) â€“ bez pÅ™edchozÃ­ho ovÄ›Å™enÃ­ bezpeÄnostnÃ­ch opatÅ™enÃ­. Tato politika mÄ›la brÃ¡nit rychlÃ©mu nasazenÃ­ potenciÃ¡lnÄ› nebezpeÄnÃ½ch systÃ©mÅ¯ a slouÅ¾ila jako argument proti tlaku trhu na urychlenÃ½ vÃ½voj. Podle exkluzivnÃ­ho rozhovoru pro TIME s vrchnÃ­m vÄ›deckÃ½m pracovnÃ­kem Jaredem Kaplanem vÅ¡ak firma v poslednÃ­ch mÄ›sÃ­cÃ­ch RSP radikÃ¡lnÄ› pÅ™epracovala.

Kaplan uvedl, Å¾e zastavenÃ­ trÃ©novÃ¡nÃ­ by nepomohlo nikomu, protoÅ¾e konkurenti jako OpenAI pokraÄujÃ­ vpÅ™ed. NovÃ¡ politika, kterou TIME prohlÃ©dla, zachovÃ¡vÃ¡ nÄ›kterÃ© prvky, jako zÃ¡vazek k transparentnosti: Anthropic bude zveÅ™ejÅˆovat vÃ½sledky bezpeÄnostnÃ­ch testÅ¯ svÃ½ch modelÅ¯, vÄetnÄ› detailÅ¯ o jejich chovÃ¡nÃ­ v rizikovÃ½ch scÃ©nÃ¡Å™Ã­ch. DÃ¡le slibuje, Å¾e bezpeÄnostnÃ­ ÃºsilÃ­ pÅ™ekonÃ¡ konkurenty, a zavazuje se k odloÅ¾enÃ­ vÃ½voje, pokud bude povaÅ¾ovÃ¡na za lÃ­dra v AI zÃ¡vodÄ› a riziko globÃ¡lnÃ­ katastrofy bude podle jejÃ­ch lÃ­drÅ¯ vÃ½znamnÃ©. PÅ™esto je novÃ¡ RSP mÃ©nÄ› restriktivnÃ­ â€“ dÅ™Ã­ve kategoricky zakazovala trÃ©novÃ¡nÃ­ nad hranicÃ­ bez pÅ™ipravenÃ½ch opatÅ™enÃ­, nynÃ­ toto pravidlo chybÃ­.

Tato zmÄ›na pÅ™ichÃ¡zÃ­ v dobÄ›, kdy Anthropic, dÅ™Ã­ve zaostÃ¡vajÃ­cÃ­ za OpenAI, dosahuje technologickÃ½ch ÃºspÄ›chÅ¯ a zÃ­skÃ¡vÃ¡ masivnÃ­ investice, vÄetnÄ› od Amazonu. RSP byla navrÅ¾ena jako Å¡kÃ¡lovatelnÃ½ rÃ¡mec, kde se bezpeÄnostnÃ­ poÅ¾adavky zvyÅ¡ujÃ­ s vÃ½konem modelu (napÅ™. ASL-4 pro systÃ©my s riziky biologickÃ½ch zbranÃ­). RuÅ¡enÃ­ centrÃ¡lnÃ­ho pilÃ­Å™e signalizuje posun k pragmatickÃ©mu pÅ™Ã­stupu, kde se bezpeÄnost Å™Ã­dÃ­ spÃ­Å¡e konkurenÄnÃ­m prostÅ™edÃ­m neÅ¾ absolutnÃ­mi pravidly. Pro uÅ¾ivatele to znamenÃ¡, Å¾e budoucÃ­ modely Claude mohou pÅ™ijÃ­t rychleji, ale s potenciÃ¡lnÄ› vyÅ¡Å¡Ã­m rizikem neoÄekÃ¡vanÃ©ho chovÃ¡nÃ­, jako halucinace nebo manipulace.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tato zmÄ›na oslabuje dÅ¯vÄ›ryhodnost AI safety hnutÃ­, kterÃ© Anthropic vedl. V Å¡irÅ¡Ã­m kontextu AI prÅ¯myslu, kde firmy jako OpenAI a Google DeepMind takÃ© upravujÃ­ bezpeÄnostnÃ­ zÃ¡vazky (napÅ™. po sporu o superalignment u OpenAI), ukazuje na selhÃ¡nÃ­ sebeomezujÃ­cÃ­ch mechanismÅ¯. Pokud top laboratoÅ™e nebudou brzdit vÃ½voj kvÅ¯li rizikÅ¯m, zvyÅ¡uje se pravdÄ›podobnost incidentÅ¯ s pokroÄilÃ½mi AI, jako nesprÃ¡vnÃ© rady v kritickÃ½ch oblastech (zdravotnictvÃ­, bezpeÄnost). Pro prÅ¯mysl to znamenÃ¡ eskalaci zÃ¡vodu, kde bezpeÄnost nÃ¡sleduje za vÃ½konem, coÅ¾ mÅ¯Å¾e vÃ©st k regulatornÃ­m zÃ¡sahÅ¯m od vlÃ¡d. Kriticky Å™eÄeno, Anthropic pÅ™iznÃ¡vÃ¡, Å¾e trÅ¾nÃ­ sÃ­ly pÅ™evaÅ¾ujÃ­ nad ideÃ¡ly, coÅ¾ podtrhuje potÅ™ebu externÃ­ch regulacÃ­ pro skuteÄnou kontrolu nad AGI pokrokem.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://time.com/7380854/exclusive-anthropic-drops-flagship-safety-pledge/)

**Zdroj:** ğŸ“° Time
