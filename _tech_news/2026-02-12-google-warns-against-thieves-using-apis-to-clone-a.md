---
author: Marisa Aigen
category: kybernetika
companies:
- Google
date: '2026-02-12 19:13:57'
description: Google Threat Intelligence Group upozorÅˆuje na rostoucÃ­ hrozbu model
  extraction attacks, pÅ™i kterÃ½ch ÃºtoÄnÃ­ci vyuÅ¾Ã­vajÃ­ legitimnÃ­ pÅ™Ã­stup k API velkÃ½ch
  jazykovÃ½ch modelÅ¯ (LLM) k zÃ­skÃ¡nÃ­ dat pro trÃ©novÃ¡nÃ­ vlastnÃ­ch kopiÃ­. Tato technika
  umoÅ¾Åˆuje rychlÃ© a levnÃ© replikovÃ¡nÃ­ proprietÃ¡rnÃ­ch modelÅ¯ bez nutnosti tradiÄnÃ­ho
  prolomenÃ­ systÃ©mÅ¯.
importance: 4
layout: tech_news_article
original_title: Google Warns Against Thieves Using APIs to Clone AI Models
publishedAt: '2026-02-12T19:13:57+00:00'
slug: google-warns-against-thieves-using-apis-to-clone-a
source:
  emoji: ğŸ“°
  id: null
  name: pymnts.com
title: Google varuje pÅ™ed zlodÄ›ji, kteÅ™Ã­ klonujÃ­ AI modely pomocÃ­ API
url: https://www.pymnts.com/cybersecurity/2026/google-warns-against-thieves-using-apis-to-clone-ai-models/
urlToImage: https://www.pymnts.com/wp-content/uploads/2026/02/Google-AI-theft.jpeg
urlToImageBackup: https://www.pymnts.com/wp-content/uploads/2026/02/Google-AI-theft.jpeg
---

## Souhrn
Google Threat Intelligence Group (GTIG), divize Google zamÄ›Å™enÃ¡ na detekci kybernetickÃ½ch hrozeb, v blogovÃ©m pÅ™Ã­spÄ›vku z 12. Ãºnora 2026 varuje pÅ™ed novou formou krÃ¡deÅ¾e duÅ¡evnÃ­ho vlastnictvÃ­ v oblasti umÄ›lÃ© inteligence. ÃštoÄnÃ­ci zneuÅ¾Ã­vajÃ­ veÅ™ejnÄ› dostupnÃ¡ API velkÃ½ch jazykovÃ½ch modelÅ¯ k provÃ¡dÄ›nÃ­ tzv. model extraction attacks nebo distillation attacks, ÄÃ­mÅ¾ extrahujÃ­ klÃ­ÄovÃ© informace pro vytvoÅ™enÃ­ vlastnÃ­ch klonÅ¯ tÄ›chto modelÅ¯. GTIG spoleÄnÄ› s Google DeepMind v roce 2025 identifikovalo a zlikvidovalo nÄ›kolik takovÃ½ch pokusÅ¯.

## KlÃ­ÄovÃ© body
- ÃštoÄnÃ­ci zÃ­skÃ¡vajÃ­ legitimnÃ­ pÅ™Ã­stup k API LLM, jako jsou ty od OpenAI nebo Google, a opakovanÄ› dotazujÃ­ model na specifickÃ¡ data.
- ExtrahovanÃ¡ data slouÅ¾Ã­ k trÃ©novÃ¡nÃ­ novÃ½ch modelÅ¯, coÅ¾ dramaticky sniÅ¾uje nÃ¡klady a Äas oproti vÃ½voji od nuly.
- GTIG zdÅ¯razÅˆuje pÅ™echod od tradiÄnÃ­ch hackerstvÃ­ k legÃ¡lnÃ­m API zneuÅ¾itÃ­m, coÅ¾ ztÄ›Å¾uje detekci.
- LegitimnÃ­ pouÅ¾itÃ­ distillation existuje, ale bez souhlasu jde o krÃ¡deÅ¾.
- V roce 2025 byly identifikovÃ¡ny a naruÅ¡eny konkrÃ©tnÃ­ Ãºtoky.

## Podrobnosti
Model extraction attacks fungujÃ­ tak, Å¾e ÃºtoÄnÃ­k platÃ­ za standardnÃ­ pÅ™Ã­stup k API LLM, napÅ™Ã­klad k modelÅ¯m jako GPT nebo Gemini. OpakovanÃ½mi dotazy â€“ Äasto tisÃ­ci nebo milionem â€“ zÃ­skÃ¡vÃ¡ vÃ½stupy modelu na peÄlivÄ› navrÅ¾enÃ© vstupy. Tyto pÃ¡ry vstup-vÃ½stup pak slouÅ¾Ã­ jako trÃ©ninkovÃ¡ data pro novÃ½ model, kterÃ½ se chovÃ¡ podobnÄ› jako originÃ¡l. Tato metoda, znÃ¡mÃ¡ takÃ© jako model distillation, umoÅ¾Åˆuje pÅ™enÃ©st znalosti z velkÃ©ho â€uÄiteleâ€œ (teacher model) do menÅ¡Ã­ho â€Å¾Ã¡kaâ€œ (student model), kterÃ½ je efektivnÄ›jÅ¡Ã­ v nasazenÃ­.

GTIG uvÃ¡dÃ­, Å¾e tradiÄnÃ­ zpÅ¯soby krÃ¡deÅ¾e high-tech znalostÃ­ zahrnovaly intruze do sÃ­tÃ­ a krÃ¡deÅ¾ datovÃ½ch sad s obchodnÃ­mi tajemstvÃ­mi. Dnes staÄÃ­ placenÃ½ API klÃ­Ä, coÅ¾ democratizuje pÅ™Ã­stup k tÃ©to technice. Google DeepMind, vÃ½zkumnÃ© centrum Google specializujÃ­cÃ­ se na pokroÄilou AI, pomohlo identifikovat anomÃ¡lnÃ­ chovÃ¡nÃ­ v API volÃ¡nÃ­ch, jako jsou neobvyklÃ© objemy dotazÅ¯ nebo specifickÃ© patterny, kterÃ© naznaÄujÃ­ extraction.

V praxi to znamenÃ¡, Å¾e firmy jako OpenAI, Anthropic nebo xAI, kterÃ© nabÃ­zejÃ­ LLM jako sluÅ¾bu (SaaS), musÃ­ zavÃ©st pokroÄilÃ© ochrany. Mezi nÄ› patÅ™Ã­ omezenÃ­ rychlosti dotazÅ¯ (rate limiting), detekce anomÃ¡liÃ­ v API provozu, vodoznaky ve vÃ½stupech (watermarking) pro sledovÃ¡nÃ­ zneuÅ¾itÃ­ nebo dokonce Å¡ifrovanÃ© odpovÄ›di, kterÃ© brÃ¡nÃ­ efektivnÃ­mu trÃ©ninku. Bez tÄ›chto opatÅ™enÃ­ mohou ÃºtoÄnÃ­ci replikovat modely stojÃ­cÃ­ stovky milionÅ¯ dolarÅ¯ za zlomek ceny â€“ trÃ©nink GPT-4 odhadnÄ› stÃ¡l pÅ™es 100 milionÅ¯ USD na GPU vÃ½poÄtech.

GTIG zdÅ¯razÅˆuje, Å¾e tato hrozba roste s integracÃ­ LLM do podnikovÃ½ch systÃ©mÅ¯, kde proprietÃ¡rnÃ­ fine-tuning pÅ™edstavuje konkurenÄnÃ­ vÃ½hodu. PÅ™Ã­kladem mÅ¯Å¾e bÃ½t klonovÃ¡nÃ­ specializovanÃ©ho modelu pro lÃ©kaÅ™skou diagnostiku nebo finanÄnÃ­ analÃ½zu.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tato hrozba ohroÅ¾uje ekonomickÃ½ model AI prÅ¯myslu, kde hlavnÃ­ hodnotou jsou nejen data, ale i architektura a trÃ©ninkovÃ© postupy modelÅ¯. Pokud se klonovÃ¡nÃ­ stane bÄ›Å¾nÃ½m, snÃ­Å¾Ã­ se motivace k investicÃ­m do vÃ½voje â€“ odhaduje se, Å¾e globÃ¡lnÃ­ vÃ½daje na AI v roce 2025 pÅ™ekroÄily 200 miliard USD. Pro uÅ¾ivatele to znamenÃ¡ riziko Å¡Ã­Å™enÃ­ nekvalitnÃ­ch klonÅ¯, kterÃ© mohou obsahovat chyby originÃ¡lu nebo bÃ½t zneuÅ¾ity k Å¡Ã­Å™enÃ­ dezinformacÃ­.

V Å¡irÅ¡Ã­m kontextu urychluje to zÃ¡vod o bezpeÄnost AI: firmy musÃ­ balancovat otevÅ™enost API pro inovace s ochranou IP. Jako expert vidÃ­m, Å¾e bez standardizovanÃ½ch protokolÅ¯, jako jsou ty navrhovanÃ© OpenAI v podobnÃ½ch paperÅ¯ch, se hrozba rozÅ¡Ã­Å™Ã­ na edge computing a on-device AI. Google sÃ¡m nasadil detekÄnÃ­ systÃ©my, ale doporuÄuji vÅ¡em poskytovatelÅ¯m LLM implementovat behaviorÃ¡lnÃ­ analÃ½zu API logÅ¯. DlouhodobÄ› to povede k hybridnÃ­m modelÅ¯m, kde ÄÃ¡st vÃ½poÄtÅ¯ bÄ›Å¾Ã­ lokÃ¡lnÄ›, mimo dosah API zneuÅ¾itÃ­.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.pymnts.com/cybersecurity/2026/google-warns-against-thieves-using-apis-to-clone-ai-models/)

**Zdroj:** ğŸ“° pymnts.com
