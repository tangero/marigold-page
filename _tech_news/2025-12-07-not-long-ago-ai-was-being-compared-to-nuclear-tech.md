---
author: Marisa Aigen
category: regulace ai
date: '2025-12-07 10:07:02'
description: VlÃ¡dy kvÅ¯li ekonomickÃ½m ziskÅ¯m opustily plÃ¡ny na bezpeÄnostnÃ­ opatÅ™enÃ­
  a nechÃ¡vajÃ­ firmy s umÄ›lou inteligencÃ­ se regulovat samy. SvÄ›t se k rizikÅ¯m AI vrÃ¡tÃ­
  aÅ¾ po katastrofÄ›?
importance: 3
layout: tech_news_article
original_title: Not long ago, AI was being compared to nuclear technology. So why
  be content with self-regulation?
publishedAt: '2025-12-07T10:07:02+00:00'
slug: not-long-ago-ai-was-being-compared-to-nuclear-tech
source:
  emoji: ğŸ“°
  id: null
  name: Livemint
title: NedÃ¡vno se umÄ›lÃ¡ inteligence pÅ™irovnÃ¡vala k jadernÃ© technologii. ProÄ se tedy
  spokojit s vlastnÃ­ regulacÃ­?
url: https://www.livemint.com/opinion/online-views/ai-nuclear-technology-self-regulation-artificial-intelligence-rishi-sunak-elon-musk-economy-11765084943811.html
urlToImage: https://www.livemint.com/lm-img/img/2025/12/07/1600x900/logo/SINGAPORE-NEF-82_1765091783865_1765091799178.jpg
urlToImageBackup: https://www.livemint.com/lm-img/img/2025/12/07/1600x900/logo/SINGAPORE-NEF-82_1765091783865_1765091799178.jpg
---

## Souhrn
BÃ½valÃ½ britskÃ½ premiÃ©r Rishi Sunak zmÄ›nil postoj k regulaci umÄ›lÃ© inteligence: z organizÃ¡tora prvnÃ­ho globÃ¡lnÃ­ho summitu o bezpeÄnosti AI v roce 2023 se stal odpÅ¯rcem jakÃ½chkoli zÃ¡konÅ¯. VlÃ¡dy po celÃ©m svÄ›tÄ› upouÅ¡tÄ›jÃ­ od pÅ™Ã­snÃ½ch pravidel kvÅ¯li ekonomickÃ½m pÅ™Ã­leÅ¾itostem, pÅ™estoÅ¾e AI jako ChatGPT se Å¡Ã­Å™Ã­ rychlostÃ­ nejrychleji rostoucÃ­ho softwaru v historii. ÄŒlÃ¡nek varuje, Å¾e ÄekÃ¡nÃ­ na katastrofu pÅ™ed zavedenÃ­m regulacÃ­ je riskantnÃ­ sÃ¡zka.

## KlÃ­ÄovÃ© body
- Rishi Sunak poÅ™Ã¡dal v roce 2023 AI Safety Summit, nynÃ­ tvrdÃ­, Å¾e regulace nenÃ­ potÅ™eba, protoÅ¾e firmy jako OpenAI dobrovolnÄ› spolupracujÃ­ s vÃ½zkumnÃ­ky.
- VlÃ¡dy prioritizujÃ­ ekonomickÃ½ rÅ¯st pÅ™ed bezpeÄnostÃ­, bez dÅ¯kazÅ¯ o Å¡irokÃ©m poÅ¡kozenÃ­.
- ChatGPT dosÃ¡hl 10 % globÃ¡lnÃ­ populace uÅ¾ivatelÅ¯ do tÅ™Ã­ let od spuÅ¡tÄ›nÃ­ a ovlivÅˆuje lidskÃ© myÅ¡lenÃ­.
- Firmy jako OpenAI ÄelÃ­ Å¾alobÃ¡m za psychickÃ© Å¡kody zpÅ¯sobenÃ© uÅ¾ivateli.

## Podrobnosti
ÄŒlÃ¡nek Parmy OlsonovÃ© zmiÅˆuje, jak se Rishi Sunak v roce 2023 obÃ¡val rizik AI natolik, Å¾e svolal svÄ›tovÃ© lÃ­dry a Elona Muska na prvnÃ­ globÃ¡lnÃ­ summit o bezpeÄnosti AI v Bletchley Parku. Tehdy BritÃ¡nie chtÄ›la bÃ½t â€domovem regulace bezpeÄnosti AIâ€œ. Dnes Sunak tvrdÃ­, Å¾e firmy jako OpenAI fungujÃ­ dobÅ™e v spoluprÃ¡ci s bezpeÄnostnÃ­mi vÃ½zkumnÃ­ky v LondÃ½nÄ›, kteÅ™Ã­ testujÃ­ modely na potenciÃ¡lnÃ­ Å¡kody, jako je Å¡Ã­Å™enÃ­ dezinformacÃ­ nebo manipulace. Tyto firmy se dobrovolnÄ› nechÃ¡vajÃ­ auditovat, coÅ¾ Sunak povaÅ¾uje za pozitivnÃ­. Autorka vÅ¡ak poukazuje na riziko: co se stane, kdyÅ¾ se firmy rozhodnou spoluprÃ¡ci ukonÄit?

Tento obrat odrÃ¡Å¾Ã­ globÃ¡lnÃ­ trend. VlÃ¡dy vidÃ­ v AI Å¡anci na oÅ¾ivenÃ­ stagnujÃ­cÃ­ch ekonomik â€“ napÅ™Ã­klad ChatGPT od OpenAI, firmy zamÄ›Å™enÃ© na velkÃ© jazykovÃ© modely (LLM), kterÃ© generujÃ­ text, odpovÄ›di a kÃ³d na zÃ¡kladÄ› trÃ©ninku na obrovskÃ½ch datech, se stal nejrychleji rostoucÃ­m softwarem. Do tÅ™Ã­ let od spuÅ¡tÄ›nÃ­ v prosinci 2022 ho pouÅ¾Ã­vÃ¡ 10 % svÄ›tovÃ© populace, coÅ¾ pÅ™ekonÃ¡vÃ¡ rÅ¯st sociÃ¡lnÃ­ch sÃ­tÃ­. Studie naznaÄujÃ­, Å¾e takovÃ© nÃ¡stroje mÄ›nÃ­ zpÅ¯sob myÅ¡lenÃ­ uÅ¾ivatelÅ¯, sniÅ¾ujÃ­ kritickÃ© myÅ¡lenÃ­ a zvyÅ¡ujÃ­ zÃ¡vislost. OpenAI uÅ¾ ÄelÃ­ Å¾alobÃ¡m od rodin obÄ›tÃ­, kterÃ© upadly do bludÅ¯ nebo sebevraÅ¾d po nadmÄ›rnÃ©m pouÅ¾Ã­vÃ¡nÃ­ modelu.

SrovnÃ¡nÃ­ s jadernou technologiÃ­ je trefnÃ©: i ta se pÅ¯vodnÄ› Å¡Ã­Å™ila bez regulacÃ­ kvÅ¯li ekonomickÃ½m a vojenskÃ½m vÃ½hodÃ¡m, neÅ¾ pÅ™iÅ¡ly ÄŒernobyl nebo FukuÅ¡ima. AI rizika zahrnujÃ­ nejen psychickÃ© dopady, ale i autonomnÃ­ zbranÄ›, masivnÃ­ dezinformace ovlivÅˆujÃ­cÃ­ volby nebo ztrÃ¡tu pracovnÃ­ch mÃ­st v kreativnÃ­ch oborech. Firmy jako OpenAI, Anthropic nebo Google DeepMind slibujÃ­ dobrovolnÃ© bezpeÄnostnÃ­ testy, ale bez zÃ¡kona mohou priorizovat zisky.

## ProÄ je to dÅ¯leÅ¾itÃ©
Regulace AI ovlivnÃ­ celÃ½ technologickÃ½ ekosystÃ©m. Bez pÅ™Ã­snÃ½ch pravidel hrozÃ­ eskalace rizik, jako je zneuÅ¾itÃ­ LLM k tvorbÄ› deepfakeÅ¯ nebo kyberÃºtokÅ¯, coÅ¾ by zpÅ¯sobilo systÃ©movÃ© selhÃ¡nÃ­. EvropskÃ¡ unie s AI Actem vede v pÅ™Ã­stupu â€riziko podle rizikaâ€œ, kde vysoce rizikovÃ© systÃ©my (napÅ™. v medicÃ­nÄ› nebo dopravÄ›) podlÃ©hajÃ­ auditÅ¯m. USA a BritÃ¡nie se spÃ­Å¡ spolÃ©hajÃ­ na dobrovolnost, coÅ¾ kritici jako Yoshua Bengio varujÃ­ jako chybu. Pro prÅ¯mysl to znamenÃ¡ rychlejÅ¡Ã­ inovace, ale pro uÅ¾ivatele vyÅ¡Å¡Ã­ expozici rizikÅ¯m â€“ od zÃ¡vislosti na AI po spoleÄenskÃ© polarizace. DlouhodobÄ› by absence regulacÃ­ mohla vÃ©st k globÃ¡lnÃ­mu â€AI incidentuâ€œ, kterÃ½ donutÃ­ reaktivnÃ­ opatÅ™enÃ­, podobnÄ› jako u klimatickÃ½ch zmÄ›n.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.livemint.com/opinion/online-views/ai-nuclear-technology-self-regulation-artificial-intelligence-rishi-sunak-elon-musk-economy-11765084943811.html)

**Zdroj:** ğŸ“° Livemint
