---
author: Marisa Aigen
category: regulace ai
date: '2025-12-07 10:07:02'
description: VlÃ¡dy kvÅ¯li ekonomickÃ½m ziskÅ¯m opouÅ¡tÄ›jÃ­ bezpeÄnostnÃ­ opatÅ™enÃ­ a nechÃ¡vajÃ­
  AI firmy regulovat se samy. SvÄ›t se k rizikÅ¯m umÄ›lÃ© inteligence vrÃ¡tÃ­ aÅ¾ po katastrofÄ›?
importance: 3
layout: tech_news_article
original_title: Not long ago, AI was being compared to nuclear technology. So why
  be content with self-regulation?
publishedAt: '2025-12-07T10:07:02+00:00'
slug: not-long-ago-ai-was-being-compared-to-nuclear-tech
source:
  emoji: ğŸ“°
  id: null
  name: Livemint
title: PÅ™ed nedÃ¡vnem se umÄ›lÃ¡ inteligence pÅ™irovnÃ¡vala k jadernÃ© technologii. ProÄ
  se tedy spokojit s vlastnÃ­ regulacÃ­?
url: https://www.livemint.com/opinion/online-views/ai-nuclear-technology-self-regulation-artificial-intelligence-rishi-sunak-elon-musk-economy-11765084943811.html
urlToImage: https://www.livemint.com/lm-img/img/2025/12/07/1600x900/logo/SINGAPORE-NEF-82_1765091783865_1765091799178.jpg
urlToImageBackup: https://www.livemint.com/lm-img/img/2025/12/07/1600x900/logo/SINGAPORE-NEF-82_1765091783865_1765091799178.jpg
---

## Souhrn
BÃ½valÃ½ britskÃ½ premiÃ©r Rishi Sunak zmÄ›nil svÅ¯j postoj k regulaci umÄ›lÃ© inteligence: z organizÃ¡tora prvnÃ­ho globÃ¡lnÃ­ho summitu o bezpeÄnosti AI v roce 2023 se stal zastÃ¡ncem absence legislativy. VlÃ¡dy po celÃ©m svÄ›tÄ› upouÅ¡tÄ›jÃ­ od pÅ™Ã­snÃ½ch pravidel kvÅ¯li ekonomickÃ½m pÅ™Ã­leÅ¾itostem, pÅ™estoÅ¾e rychlÃ½ rÅ¯st technologiÃ­ jako ChatGPT zvyÅ¡uje rizika. ÄŒlÃ¡nek varuje pÅ™ed ÄekÃ¡nÃ­m na katastrofu.

## KlÃ­ÄovÃ© body
- Rishi Sunak poÅ™Ã¡dal AI Safety Summit v roce 2023, nynÃ­ tvrdÃ­, Å¾e regulace nenÃ­ potÅ™eba, protoÅ¾e firmy jako OpenAI dobrovolnÄ› spolupracujÃ­ s vÃ½zkumnÃ­ky.
- GlobÃ¡lnÃ­ trend: vlÃ¡dy prioritizujÃ­ ekonomickÃ½ rÅ¯st pÅ™ed bezpeÄnostÃ­, bez dÅ¯kazÅ¯ o Å¡irokÃ©m poÅ¡kozenÃ­.
- ChatGPT dosÃ¡hl 10 % globÃ¡lnÃ­ populace uÅ¾ivatelÅ¯ bÄ›hem tÅ™Ã­ let, coÅ¾ je nejrychlejÅ¡Ã­ rÅ¯st softwaru v historii.
- OpenAI ÄelÃ­ Å¾alobÃ¡m od rodin obÄ›tÃ­ duÅ¡evnÃ­ch poruch spojenÃ½ch s pouÅ¾Ã­vÃ¡nÃ­m AI.
- Kritika: ÄekÃ¡nÃ­ na katastrofu je hazard s rychle se Å¡Ã­Å™Ã­cÃ­ technologiÃ­.

## Podrobnosti
ÄŒlÃ¡nek od Parmy OlsonovÃ© analyzuje zmÄ›nu postoje klÃ­ÄovÃ½ch politikÅ¯ k regulaci umÄ›lÃ© inteligence. Rishi Sunak, kterÃ½ v roce 2023 hostil v Bletchley Parku prvnÃ­ globÃ¡lnÃ­ AI Safety Summit za ÃºÄasti Elona Muska a dalÅ¡Ã­ch expertÅ¯ na rizika AI, nynÃ­ argumentuje, Å¾e firmy jako OpenAI, vÃ½vojÃ¡Å™ ChatGPT, se dobrovolnÄ› nechÃ¡vajÃ­ testovat bezpeÄnostnÃ­mi vÃ½zkumnÃ­ky v LondÃ½nÄ›. Tyto testy zahrnujÃ­ audity modelÅ¯ na potenciÃ¡lnÃ­ Å¡kody, jako je Å¡Ã­Å™enÃ­ dezinformacÃ­ nebo podporu nebezpeÄnÃ½ch aktivit. Sunak tvrdÃ­, Å¾e dosud nedoÅ¡lo k situacÃ­m, kdy by firmy spoluprÃ¡ci odmÃ­tly, coÅ¾ povaÅ¾uje za pozitivnÃ­.

Tento obrat odhaluje Å¡irÅ¡Ã­ trend mezi vlÃ¡dami. NapÅ™Ã­klad BritÃ¡nie, kterÃ¡ se chtÄ›la stÃ¡t â€domovem regulace bezpeÄnosti AIâ€œ, nynÃ­ legislativu opouÅ¡tÃ­. PodobnÄ› USA a EU zpomalujÃ­ plÃ¡ny na pÅ™Ã­snÃ© normy, protoÅ¾e AI slibuje oÅ¾ivenÃ­ stagnujÃ­cÃ­ch ekonomik â€“ od automatizace prÅ¯myslu po novÃ© sluÅ¾by. ChatGPT, spuÅ¡tÄ›nÃ½ v listopadu 2022, se stal nejrychleji rostoucÃ­m softwarem: bÄ›hem tÅ™Ã­ let ho pouÅ¾Ã­vÃ¡ 10 % svÄ›tovÃ© populace, coÅ¾ pÅ™ekonÃ¡vÃ¡ rÅ¯st sociÃ¡lnÃ­ch sÃ­tÃ­. Tento model large language model (LLM) generuje texty, odpovÃ­dÃ¡ na otÃ¡zky a pomÃ¡hÃ¡ v programovÃ¡nÃ­, ale zÃ¡roveÅˆ mÄ›nÃ­ kognitivnÃ­ procesy uÅ¾ivatelÅ¯ â€“ studie ukazujÃ­ na zÃ¡vislost a snÃ­Å¾enÃ­ kritickÃ©ho myÅ¡lenÃ­.

ProblÃ©my se projevujÃ­: OpenAI ÄelÃ­ Å¾alobÃ¡m od rodin lidÃ­, kteÅ™Ã­ po interakci s ChatGPT upadli do bludÅ¯ nebo sebevraÅ¾ednÃ½ch myÅ¡lenek. Firmy jako Anthropic (vÃ½vojÃ¡Å™ Claude) nebo xAI (Elon Musk) slibujÃ­ dobrovolnÃ© audity, ale bez prÃ¡vnÃ­ zÃ¡vaznosti mohou zmÄ›nit politiku. HistorickÃ© analogy s jadernou energiÃ­ ukazujÃ­, Å¾e bez regulace (jako MezinÃ¡rodnÃ­ agentura pro atomovou energii) dochÃ¡zÃ­ k nehodÃ¡m â€“ proÄ s AI Äekat?

## ProÄ je to dÅ¯leÅ¾itÃ©
Regulace AI ovlivÅˆuje celÃ½ technologickÃ½ ekosystÃ©m. Bez nezÃ¡vislÃ©ho dohledu hrozÃ­ zneuÅ¾itÃ­ modelÅ¯ k dezinformacÃ­m, kyberÃºtokÅ¯m nebo zbranÃ­m â€“ napÅ™Ã­klad generovÃ¡nÃ­ kÃ³du pro malware. Pro uÅ¾ivatele znamenÃ¡ self-regulace riziko nekontrolovanÃ½ch dopadÅ¯ na duÅ¡evnÃ­ zdravÃ­ a soukromÃ­, protoÅ¾e modely sbÃ­rajÃ­ data bez dostateÄnÃ© ochrany. PrÅ¯mysl profituje z absence pravidel (funding OpenAI pÅ™ekroÄil 10 miliard dolarÅ¯), ale dlouhodobÄ› to brzdÃ­ dÅ¯vÄ›ru veÅ™ejnosti. V kontextu pokrokÅ¯ jako GPT-5 nebo Gemini 2.0 je nezbytnÃ© legislativnÃ­ rÃ¡mce, podobnÄ› jako u datovÃ© ochrany GDPR. ÄŒekÃ¡nÃ­ na katastrofu, jako u sociÃ¡lnÃ­ch sÃ­tÃ­, by mohlo zpÅ¯sobit globÃ¡lnÃ­ Å¡kody v miliardÃ¡ch dolarÅ¯ a ztrÃ¡tu Å¾ivotÅ¯.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.livemint.com/opinion/online-views/ai-nuclear-technology-self-regulation-artificial-intelligence-rishi-sunak-elon-musk-economy-11765084943811.html)

**Zdroj:** ğŸ“° Livemint
