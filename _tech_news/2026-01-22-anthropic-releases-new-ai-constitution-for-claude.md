---
author: Marisa Aigen
category: ai
companies:
- Anthropic
date: '2026-01-22 01:01:44'
description: SpoleÄnost Anthropic PBC zveÅ™ejnila novou verzi konstituce pro modely
  Claude, dokumentu, kterÃ½ stanovuje pravidla pro zpracovÃ¡nÃ­ dotazÅ¯ velkÃ½mi jazykovÃ½mi
  modely. Tato aktualizace Å™eÅ¡Ã­ limity pÅ¯vodnÃ­ verze z kvÄ›tna 2023 tÃ­m, Å¾e pÅ™idÃ¡vÃ¡
  vysvÄ›tlenÃ­ dÅ¯vodÅ¯ chovÃ¡nÃ­.
importance: 4
layout: tech_news_article
original_title: Anthropic releases new AI â€˜constitutionâ€™ for Claude
publishedAt: '2026-01-22T01:01:44+00:00'
slug: anthropic-releases-new-ai-constitution-for-claude
source:
  emoji: ğŸ“°
  id: null
  name: SiliconANGLE News
title: Anthropic vydÃ¡vÃ¡ novou 'konstituci' pro umÄ›lou inteligenci Claude
url: https://siliconangle.com/2026/01/21/anthropic-releases-new-ai-constitution-claude/
urlToImage: https://d15shllkswkct0.cloudfront.net/wp-content/blogs.dir/1/files/2026/01/Anthropic-1.png
urlToImageBackup: https://d15shllkswkct0.cloudfront.net/wp-content/blogs.dir/1/files/2026/01/Anthropic-1.png
---

## Souhrn
SpoleÄnost Anthropic, specializujÃ­cÃ­ se na vÃ½voj bezpeÄnÃ½ch velkÃ½ch jazykovÃ½ch modelÅ¯, vydala 21. ledna 2026 aktualizovanou verzi konstituce pro Å™adu modelÅ¯ Claude. Tento dokument slouÅ¾Ã­ jako systÃ©movÃ½ nÃ¡vod pro zpracovÃ¡nÃ­ uÅ¾ivatelskÃ½ch dotazÅ¯ a Å™eÅ¡Ã­ problÃ©my s aplikacÃ­ pravidel na novÃ© situace. Novinka obsahuje nejen instrukce, ale i odÅ¯vodnÄ›nÃ­, proÄ se modely majÃ­ chovat urÄitÃ½m zpÅ¯sobem, coÅ¾ zlepÅ¡uje jejich generalizaci.

## KlÃ­ÄovÃ© body
- **SkuteÄnÄ› uÅ¾iteÄnÃ½**: Modely majÃ­ pÅ™esnÄ› splÅˆovat poÅ¾adavky uÅ¾ivatele, napÅ™Ã­klad generovat kÃ³d pouze v poÅ¾adovanÃ©m programovacÃ­m jazyce.
- **Å iroce bezpeÄnÃ½**: ZabrÃ¡nit provÃ¡dÄ›nÃ­ zakÃ¡zanÃ½ch akcÃ­ a zajistit transparentnost rozhodovÃ¡nÃ­.
- **Å iroce etickÃ½**: DodrÅ¾ovÃ¡nÃ­ etickÃ½ch principÅ¯ v Å¡irÅ¡Ã­m kontextu.
- **SpecifickÃ© pokyny**: Ochrana pÅ™ed pokusy o prolomenÃ­ omezenÃ­ (jailbreaking) a pravidla pro interakci s aplikacemi tÅ™etÃ­ch stran.
- **VylepÅ¡enÃ­ generalizace**: PÅ™idÃ¡nÃ­ vysvÄ›tlenÃ­ dÅ¯vodÅ¯ umoÅ¾Åˆuje modelÅ¯m aplikovat pravidla na neznÃ¡mÃ© scÃ©nÃ¡Å™e.

## Podrobnosti
Konstituce Claude je v podstatÄ› dlouhÃ½ systÃ©movÃ½ prompt, kterÃ½ definuje chovÃ¡nÃ­ velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) pÅ™i odpovÃ­dÃ¡nÃ­ na dotazy. PÅ¯vodnÃ­ verze z kvÄ›tna 2023 obsahovala pÅ™Ã­mÃ© instrukce proti Å¡kodlivÃ©mu nebo neuÅ¾iteÄnÃ©mu vÃ½stupu, ale ukÃ¡zalo se, Å¾e modely je nedokÃ¡Å¾ou spolehlivÄ› aplikovat na neoÄekÃ¡vanÃ© situace. Pokud instrukce explicitnÄ› neÅ™eÅ¡Ã­ konkrÃ©tnÃ­ typ dotazu, model mohl vygenerovat nesprÃ¡vnou odpovÄ›Ä. Anthropic, firma zaloÅ¾enÃ¡ bÃ½valÃ½mi vÃ½zkumnÃ­ky OpenAI s dÅ¯razem na bezpeÄnost AI, proto pÅ™epracoval dokument.

NovÃ¡ verze se toÄÃ­ kolem ÄtyÅ™ hlavnÃ­ch principÅ¯. PrvnÃ­ zdÅ¯razÅˆuje bÃ½t "skuteÄnÄ› uÅ¾iteÄnÃ½m", coÅ¾ znamenÃ¡ pÅ™izpÅ¯sobit vÃ½stup pÅ™esnÄ› poÅ¾adavkÅ¯m â€“ napÅ™Ã­klad pokud vÃ½vojÃ¡Å™ Å¾Ã¡dÃ¡ kÃ³d v Pythonu, model ho nebude psÃ¡t v JavÄ›. DruhÃ½ princip, "Å¡irokÃ¡ bezpeÄnost", zakazuje akce, kterÃ© uÅ¾ivatel zakÃ¡zal, a vyÅ¾aduje transparentnost: model mÃ¡ vysvÄ›tlit, proÄ odmÃ­tne dotaz nebo jak dospÄ›l k rozhodnutÃ­. TÅ™etÃ­ je "Å¡irokÃ¡ etika", kterÃ¡ pokrÃ½vÃ¡ morÃ¡lnÃ­ dilemata mimo striktnÃ­ bezpeÄnost. ÄŒtvrtÃ½ princip zahrnuje specifickÃ© pokyny Anthropicu, vÄetnÄ› obrany proti jailbreakingu â€“ technikÃ¡m, kdy uÅ¾ivatelÃ© snaÅ¾Ã­ obejÃ­t omezenÃ­ role-playingem nebo hypotetickÃ½mi scÃ©nÃ¡Å™i â€“ a pravidel pro integraci s externÃ­mi API nebo aplikacemi.

KlÃ­Äovou inovacÃ­ je pÅ™idÃ¡nÃ­ racionÃ¡le: nejen "nedÄ›lej tohle", ale "nedÄ›lej tohle, protoÅ¾e to vede k rizikÅ¯m X, Y, Z". To umoÅ¾Åˆuje LLM lÃ©pe extrapolovat pravidla na novÃ© kontexty, coÅ¾ je problÃ©m znÃ¡mÃ½ z vÃ½zkumu AI safety. Dokument je veÅ™ejnÄ› dostupnÃ½, takÅ¾e vÃ½vojÃ¡Å™i ho mohou studovat nebo upravovat pro vlastnÃ­ modely. Aktualizace pÅ™ichÃ¡zÃ­ v dobÄ› rostoucÃ­ho tlaku na bezpeÄnost AI, kdy regulaÄnÃ­ orgÃ¡ny jako EU AI Act vyÅ¾adujÃ­ robustnÃ­ ochrany.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tato aktualizace posiluje pozici Claude v soutÄ›Å¾i s modely jako GPT od OpenAI nebo Gemini od Google, kde bezpeÄnostnÃ­ selhÃ¡nÃ­ vedou k reputaÄnÃ­m ztrÃ¡tÃ¡m. Pro uÅ¾ivatele znamenÃ¡ spolehlivÄ›jÅ¡Ã­ nÃ¡stroje pro programovÃ¡nÃ­, analÃ½zu dat nebo kreativnÃ­ Ãºkoly bez rizika Å¡kodlivÃ©ho obsahu. V Å¡irÅ¡Ã­m ekosystÃ©mu AI podtrhuje potÅ™ebu konfigurovatelnÃ½ch safety mechanismÅ¯, kterÃ© se adaptujÃ­ na rychlÃ½ vÃ½voj modelÅ¯. Pokud se principy osvÄ›dÄÃ­, mohou inspirovat standardy pro otevÅ™enÃ© modely jako Llama od Meta, ÄÃ­mÅ¾ pÅ™ispÄ›jÃ­ k prevenci zneuÅ¾itÃ­ v oblastech jako kybernetickÃ¡ bezpeÄnost nebo dezinformace. CelkovÄ› jde o pragmatickÃ½ krok k robustnÄ›jÅ¡Ã­ AI, i kdyÅ¾ neÅ™eÅ¡Ã­ hlubÅ¡Ã­ problÃ©my jako halucinace nebo bias v trÃ©novacÃ­ch datech.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://siliconangle.com/2026/01/21/anthropic-releases-new-ai-constitution-claude/)

**Zdroj:** ğŸ“° SiliconANGLE News
