---
author: Marisa Aigen
category: ai bezpeÄnost
companies:
- OpenAI
date: '2026-02-20 12:00:00'
description: OpenAI odstranila slovo 'safely' ze svÃ© mise v daÅˆovÃ©m pÅ™iznÃ¡nÃ­ za rok
  2024 a nedÃ¡vno rozpustila tÃ½m pro sladÄ›nÃ­ mise. Tyto zmÄ›ny vyvolÃ¡vajÃ­ otÃ¡zky o priority
  spoleÄnosti v oblasti bezpeÄnosti AI.
importance: 4
layout: tech_news_article
original_title: Is OpenAI Intentionally Distancing Itself from Safety?
publishedAt: '2026-02-20T12:00:00+00:00'
slug: is-openai-intentionally-distancing-itself-from-saf
source:
  emoji: ğŸ“°
  id: null
  name: Annielytics.com
title: Oddaluje se OpenAI zÃ¡mÄ›rnÄ› od bezpeÄnosti?
url: https://www.annielytics.com/blog/ai/is-openai-intentionally-distancing-itself-from-safety/
urlToImage: https://www.annielytics.com/wp-content/uploads/2026/02/openai-safety-feature-1024x647.png
urlToImageBackup: https://www.annielytics.com/wp-content/uploads/2026/02/openai-safety-feature-1024x647.png
---

### Souhrn
OpenAI upravila svou misi v daÅˆovÃ©m pÅ™iznÃ¡nÃ­ za rok 2024 tÃ­m, Å¾e odstranila zmÃ­nku o bezpeÄnÃ©m vÃ½voji AI. K tomu se pÅ™idalo rozpustÄ›nÃ­ tÃ½mu pro sladÄ›nÃ­ mise, kterÃ½ mÄ›l zajiÅ¡Å¥ovat soulad s pÅ¯vodnÃ­mi hodnotami. Tyto kroky naznaÄujÃ­ moÅ¾nÃ½ posun prioritu smÄ›rem k rychlejÅ¡Ã­mu vÃ½voji na Ãºkor bezpeÄnostnÃ­ch opatÅ™enÃ­.

### KlÃ­ÄovÃ© body
- V daÅˆovÃ©m pÅ™iznÃ¡nÃ­ za 2023 stÃ¡la mise: â€vybudovat obecnÄ› ÃºÄelovou umÄ›lou inteligenci (AI), kterÃ¡ bezpeÄnÄ› prospÃ­vÃ¡ lidstvuâ€œ; v roce 2024 se zkrÃ¡tila na â€zajistit, aby obecnÃ¡ umÄ›lÃ¡ inteligence prospÃ­vala celÃ©mu lidstvuâ€œ.
- TÃ½m pro sladÄ›nÃ­ mise (Mission Alignment team) byl vytvoÅ™en v zÃ¡Å™Ã­ 2024 a mÄ›l sedm ÄlenÅ¯; byl nedÃ¡vno rozpusten a ÄlenovÃ© pÅ™eÅ™azeni jinam.
- TÃ½m vznikl po odchodu klÃ­ÄovÃ½ch lÃ­drÅ¯ jako Mira Murati (Å¡Ã©f technologiÃ­), Bob McGrew (Å¡Ã©f vÃ½zkumu) a Barret Zoph (viceprezident vÃ½zkumu).
- ZmÄ›ny byly zaznamenÃ¡ny v ÄlÃ¡nku na The Conversation a reportÃ¡Å¾i Platformeru z anonymnÃ­ch zdrojÅ¯.

### Podrobnosti
OpenAI, spoleÄnost zamÄ›Å™enÃ¡ na vÃ½voj AGI (umÄ›lÃ© obecnÃ© inteligence), dlouhodobÄ› zdÅ¯razÅˆovala bezpeÄnost jako klÃ­ÄovÃ½ prvek svÃ©ho poslÃ¡nÃ­. V daÅˆovÃ©m pÅ™iznÃ¡nÃ­ za rok 2023 byla mise formulovÃ¡na takto: â€OpenAI's mission is to build general-purpose artificial intelligence (AI) that safely benefits humanity, unconstrained by a need to generate financial return.â€œ Tato verze explicitnÄ› spojovala vÃ½voj AI s bezpeÄnostÃ­ a Å¡irokÃ½m distribucÃ­ pÅ™Ã­nosÅ¯. V pÅ™iznÃ¡nÃ­ za rok 2024 doÅ¡lo k vÃ½raznÃ©mu zkrÃ¡cenÃ­: â€OpenAI's mission is to ensure that artificial general intelligence benefits all of humanity.â€œ Slovo â€safelyâ€œ zmizelo, stejnÄ› jako zmÃ­nky o odpovÄ›dnÃ©m nasazenÃ­ a absenci tlaku na finanÄnÃ­ zisky.

Tato zmÄ›na by sama o sobÄ› nemusela bÃ½t alarmujÃ­cÃ­, ale v kontextu dalÅ¡Ã­ch udÃ¡lostÃ­ nabÃ½vÃ¡ na vÃ½znamu. Podle reportÃ¡Å¾e Platformeru z minulÃ©ho tÃ½dne OpenAI rozpustila tÃ½m pro sladÄ›nÃ­ mise, kterÃ½ mÄ›l pomÃ¡hat zamÄ›stnancÅ¯m i veÅ™ejnosti chÃ¡pat misi spoleÄnosti a dopady AI. TÃ½m byl zÅ™Ã­zen v zÃ¡Å™Ã­ 2024, krÃ¡tce po masivnÃ­m odchodu vrchnÃ­ch manaÅ¾erÅ¯: Miry MuratiovÃ©, Boba McGrewa a Barreta Zopha, kteÅ™Ã­ odeÅ¡li v jeden den. MluvÄÃ­ OpenAI pro TechCrunch potvrdil, Å¾e tÃ½m slouÅ¾il k zajiÅ¡tÄ›nÃ­ stability bÄ›hem organizaÄnÃ­ch zmÄ›n a viditelnÄ› demonstroval zÃ¡vazek k hodnotÃ¡m. S pouhÃ½mi sedmi Äleny byl tÃ½m malÃ½, ale jeho rozpustÄ›nÃ­ a pÅ™eÅ™azenÃ­ zamÄ›stnancÅ¯ do jinÃ½ch tÃ½mÅ¯ signalizuje pÅ™ehodnocenÃ­ internÃ­ch struktur.

Tyto udÃ¡losti se odehrÃ¡vajÃ­ v obdobÃ­ intenzivnÃ­ho vÃ½voje modelÅ¯ jako GPT sÃ©rie, kde bezpeÄnostnÃ­ otÃ¡zky â€“ napÅ™Ã­klad rizika misalignmentu nebo nechtÄ›nÃ½ch chovÃ¡nÃ­ â€“ jsou stÃ¡le aktuÃ¡lnÃ­. OpenAI v minulosti Äelila kritice za nedostateÄnÃ© bezpeÄnostnÃ­ testy, coÅ¾ vedlo k odchodu zakladatelÅ¯ jako Ilya Sutskever.

### ProÄ je to dÅ¯leÅ¾itÃ©
ZmÄ›ny v misi a rozpustÄ›nÃ­ tÃ½mu odrÃ¡Å¾ejÃ­ Å¡irÅ¡Ã­ trendy v AI prÅ¯myslu, kde rychlost vÃ½voje Äasto pÅ™evaÅ¾uje nad bezpeÄnostÃ­. Pro uÅ¾ivatele to znamenÃ¡ potenciÃ¡lnÄ› mÃ©nÄ› robustnÃ­ bezpeÄnostnÃ­ mechanismy v modelech jako ChatGPT, coÅ¾ zvyÅ¡uje rizika dezinformacÃ­, biasÅ¯ nebo zneuÅ¾itÃ­. V Å¡irÅ¡Ã­m ekosystÃ©mu posiluje to obavy o AGI, kde absence explicitnÃ­ho dÅ¯razu na bezpeÄnost mÅ¯Å¾e urychlit soutÄ›Å¾ s firmami jako Anthropic nebo xAI, kterÃ© bezpeÄnost priorizujÃ­. Pro regulÃ¡tory, jako EU s AI Actem, to je signÃ¡l k vÄ›tÅ¡Ã­mu dohledu nad OpenAI. CelkovÄ› to podtrhuje napÄ›tÃ­ mezi komerÄnÃ­mi cÃ­li a etickÃ½mi povinnostmi v AGI vÃ½voji, coÅ¾ ovlivnÃ­ budoucÃ­ nasazenÃ­ AI v prÅ¯myslu i spoleÄnosti. (512 slov)

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.annielytics.com/blog/ai/is-openai-intentionally-distancing-itself-from-safety/)

**Zdroj:** ğŸ“° Annielytics.com
