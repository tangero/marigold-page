---
author: Marisa Aigen
category: ai
companies:
- OpenAI
- Anthropic
- DeepMind
- Google
- Microsoft
date: '2025-11-10 23:06:42'
description: NovÃ½ vÃ½zkum startupu Goodfire.ai ukazuje, Å¾e v jazykovÃ½ch modelech existujÃ­
  oddÄ›lenÃ© neuronovÃ© drÃ¡hy pro zapamatovÃ¡nÃ­ textu a pro obecnÃ© uvaÅ¾ovÃ¡nÃ­, coÅ¾ umoÅ¾Åˆuje
  cÃ­lenÄ› omezit memorovÃ¡nÃ­ bez zÃ¡sadnÃ­ho dopadu na schopnost Å™eÅ¡it Ãºlohy.
importance: 3
layout: tech_news_article
original_title: Researchers isolate memorization from reasoning in AI neural networks
  - Ars Technica
publishedAt: '2025-11-10T23:06:42+00:00'
slug: researchers-isolate-memorization-from-reasoning-in
source:
  emoji: ğŸ”¬
  id: ars-technica
  name: Ars Technica
title: VÃ½zkumnÃ­ci oddÄ›lili v AI sÃ­tÃ­ch pamÄ›Å¥ovÃ© a logickÃ© schopnosti
url: https://arstechnica.com/ai/2025/11/study-finds-ai-models-store-memories-and-logic-in-different-neural-regions/
urlToImage: https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-1152x648.jpg
urlToImageBackup: https://cdn.arstechnica.net/wp-content/uploads/2025/07/surprised_robot_2-1152x648.jpg
---

## Souhrn
VÃ½zkumnÃ­ci ze startupu Goodfire.ai publikovali analÃ½zu, kterÃ¡ naznaÄuje vÃ½raznÄ› oddÄ›lenÃ© neuronovÃ© okruhy pro memorovÃ¡nÃ­ a pro obecnÃ© uvaÅ¾ovÃ¡nÃ­ v modernÃ­ch jazykovÃ½ch modelech. ExperimentÃ¡lnÄ› ukazujÃ­, Å¾e lze vÃ½raznÄ› omezit schopnost modelu doslova citovat trÃ©ninkovÃ¡ data, aniÅ¾ by se vÃ½znamnÄ› snÃ­Å¾ila jeho schopnost Å™eÅ¡it Ãºlohy, vÄetnÄ› logickÃ©ho uvaÅ¾ovÃ¡nÃ­ a zÃ¡kladnÃ­ aritmetiky.

## KlÃ­ÄovÃ© body
- Identifikace separovanÃ½ch neuronovÃ½ch sloÅ¾ek pro memorovÃ¡nÃ­ a pro obecnÃ© uvaÅ¾ovÃ¡nÃ­ v modelu OLMo-7B.
- OdstranÄ›nÃ­ pamÄ›Å¥ovÃ½ch komponent vedlo k ~97% snÃ­Å¾enÃ­ schopnosti citovat trÃ©ninkovÃ¡ data, pÅ™i zachovÃ¡nÃ­ vÄ›tÅ¡iny ostatnÃ­ch schopnostÃ­.
- AnalÃ½za ukazuje systematickÃ© rozdÄ›lenÃ­: komponenty spojenÃ© s memorovÃ¡nÃ­m a s Å™eÅ¡enÃ­m obecnÃ½ch Ãºloh se liÅ¡Ã­ podle metriky â€curvatureâ€œ.
- ZjiÅ¡tÄ›nÃ­ naznaÄujÃ­, Å¾e aritmetika a dalÅ¡Ã­ â€tvrdÃ©â€œ dovednosti mohou sdÃ­let stejnÃ© drÃ¡hy jako memorizaÄnÃ­ mechanismy.
- VÃ½sledky majÃ­ pÅ™Ã­mÃ½ dopad na bezpeÄnost, ochranu osobnÃ­ch ÃºdajÅ¯ a regulaci AI modelÅ¯.

## Podrobnosti
Startup Goodfire.ai, kterÃ½ se zamÄ›Å™uje na bezpeÄnost, interpretovatelnost a Å™Ã­zenÃ­ chovÃ¡nÃ­ velkÃ½ch jazykovÃ½ch modelÅ¯, analyzoval otevÅ™enÃ½ jazykovÃ½ model OLMo-7B vyvinutÃ½ Allen Institute for AI. PomocÃ­ metriky oznaÄovanÃ© jako â€curvatureâ€œ seÅ™adili vÃ¡hovÃ© komponenty (parametry modelu) podle toho, jak se podÃ­lejÃ­ na zpracovÃ¡nÃ­ vstupÅ¯. NÃ¡slednÃ© testy ukÃ¡zaly, Å¾e komponenty s nejniÅ¾Å¡Ã­ hodnotou curvature vykazujÃ­ vyÅ¡Å¡Ã­ aktivaci u pÅ™esnÄ› zapamatovanÃ½ch pasÃ¡Å¾Ã­ z trÃ©ninkovÃ½ch dat, zatÃ­mco komponenty s nejvyÅ¡Å¡Ã­ curvature se vÃ­ce aktivujÃ­ pÅ™i prÃ¡ci s obecnÃ½m, netrÃ©ninkovÃ½m textem.

Tato separace umoÅ¾nila provÃ©st cÃ­lenÃ½ zÃ¡sah: po odstranÄ›nÃ­ spodnÃ­ ÄÃ¡sti tÄ›chto vÃ¡hovÃ½ch komponent model ztratil pÅ™ibliÅ¾nÄ› 97 % schopnosti pÅ™esnÄ› reprodukovat trÃ©ninkovÃ¡ data, ale jeho vÃ½kon v ÃºlohÃ¡ch vyÅ¾adujÃ­cÃ­ch logickÃ© uvaÅ¾ovÃ¡nÃ­ a prÃ¡ci s novÃ½mi vstupy zÅ¯stal do znaÄnÃ© mÃ­ry zachovÃ¡n. To je v rozporu s dÅ™Ã­vÄ›jÅ¡Ã­ pÅ™edstavou, Å¾e memorovÃ¡nÃ­ a generalizace jsou v modelech silnÄ› provÃ¡zanÃ© a obtÃ­Å¾nÄ› oddÄ›litelnÃ©.

PozoruhodnÃ½m vÃ½sledkem je, Å¾e zÃ¡kladnÃ­ aritmetickÃ© schopnosti modelu se zdajÃ­ bÃ½t Ãºzce svÃ¡zÃ¡ny s tÄ›mito pamÄ›Å¥ovÃ½mi drahami. To naznaÄuje, Å¾e ÄÃ¡st toho, co je bÄ›Å¾nÄ› interpretovÃ¡no jako â€logickÃ©â€œ nebo â€symbolickÃ©â€œ uvaÅ¾ovÃ¡nÃ­, mÅ¯Å¾e bÃ½t ve skuteÄnosti emergentnÃ­ efekt strukturovanÃ©ho memorovÃ¡nÃ­ a specifickÃ½ch reprezentacÃ­ v parametrech modelu. Pokud se tento vÃ½sledek potvrdÃ­ i u vÄ›tÅ¡Ã­ch a komerÄnÃ­ch modelÅ¯, bude moÅ¾nÃ© pÅ™esnÄ›ji Å™Ã­dit, kterÃ© informace model uchovÃ¡vÃ¡ a jakÃ½m zpÅ¯sobem s nimi pracuje.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tento vÃ½zkum mÃ¡ pÅ™Ã­mÃ© dÅ¯sledky pro bezpeÄnost a regulaci AI. Schopnost cÃ­lenÄ› potlaÄit memorovÃ¡nÃ­ s minimÃ¡lnÃ­m dopadem na uÅ¾iteÄnÃ© schopnosti modelu by mohla pomoci snÃ­Å¾it riziko Ãºniku osobnÃ­ch ÃºdajÅ¯, chrÃ¡nÄ›nÃ½ch textÅ¯ nebo citlivÃ½ch internÃ­ch dokumentÅ¯, kterÃ© se mohou v modelech objevit v dÅ¯sledku trÃ©ninku na nevhodnÃ½ch datech. Z pohledu poskytovatelÅ¯ sluÅ¾eb AI a provozovatelÅ¯ API to otevÃ­rÃ¡ cestu k technickÃ½m opatÅ™enÃ­m, kterÃ¡ lÃ©pe vyhovÃ­ evropskÃ© regulaci a internÃ­m compliance poÅ¾adavkÅ¯m.

ZÃ¡roveÅˆ jde o vÃ½znamnÃ½ krok v interpretovatelnosti neuronovÃ½ch sÃ­tÃ­: pokud lze oddÄ›lovat pamÄ›Å¥ovÃ© a â€logickÃ©â€œ komponenty, zvyÅ¡uje to moÅ¾nost modely systematicky ladit, auditovat a omezovat. Pro prÅ¯mysl to znamenÃ¡ potenciÃ¡l vytvÃ¡Å™et modely navrÅ¾enÃ© tak, aby byly mÃ©nÄ› nÃ¡chylnÃ© k neÅ™Ã­zenÃ©mu citovÃ¡nÃ­ trÃ©ninkovÃ½ch dat, aniÅ¾ by se zÃ¡sadnÄ› zhorÅ¡ila jejich pouÅ¾itelnost v podnikovÃ©m prostÅ™edÃ­, software nÃ¡strojÃ­ch Äi asistenÄnÃ­ch aplikacÃ­ch. SouÄasnÄ› vÃ½sledky zpochybÅˆujÃ­ jednoduchÃ© marketingovÃ© interpretace â€schopnosti uvaÅ¾ovÃ¡nÃ­â€œ v AI a nutÃ­ vÃ½robce modelÅ¯ transparentnÄ›ji vysvÄ›tlovat, jak k tÄ›mto schopnostem dochÃ¡zÃ­.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://arstechnica.com/ai/2025/11/study-finds-ai-models-store-memories-and-logic-in-different-neural-regions/)

**Zdroj:** ğŸ”¬ Ars Technica
