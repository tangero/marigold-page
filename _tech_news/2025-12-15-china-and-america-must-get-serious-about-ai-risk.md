---
author: Marisa Aigen
category: ai rizika
date: '2025-12-15 00:05:00'
description: ÄŒÃ­na a Amerika budou ostÅ™e soutÄ›Å¾it o pÅ™evahu v umÄ›lÃ© inteligenci. Jako
  jedinÃ© dvÄ› supervelmoci v tÃ©to oblasti vÅ¡ak obÄ› zemÄ› nalÃ©havÄ› potÅ™ebujÃ­ pÅ™Ã­mou spoluprÃ¡ci
  pÅ™i Å™eÅ¡enÃ­ rostoucÃ­ch rizik, kterÃ¡ technologie pÅ™inÃ¡Å¡Ã­ pro trhy, infrastrukturu,
  sociÃ¡lnÃ­ stabilitu a lidstvo samotnÃ©.
importance: 5
layout: tech_news_article
original_title: China and America Must Get Serious About AI Risk
publishedAt: '2025-12-15T00:05:00+00:00'
slug: china-and-america-must-get-serious-about-ai-risk
source:
  emoji: ğŸ“°
  id: null
  name: Project Syndicate
title: ÄŒÃ­na a Amerika se musÃ­ vÃ¡Å¾nÄ› vypoÅ™Ã¡dat s riziky AI
url: https://www.project-syndicate.org/magazine/china-us-ai-risks-call-for-urgent-diplomacy-by-jake-sullivan-2025-12
urlToImage: https://webapi.project-syndicate.org/library/d81ebdb5116e514a07cfbee45ee61a12.2-1-super.1.jpg
urlToImageBackup: https://webapi.project-syndicate.org/library/d81ebdb5116e514a07cfbee45ee61a12.2-1-super.1.jpg
---

## Souhrn
ÄŒÃ­na a SpojenÃ© stÃ¡ty budou nadÃ¡le soutÄ›Å¾it o vedenÃ­ v oblasti umÄ›lÃ© inteligence, ale jako jedinÃ© globÃ¡lnÃ­ supervelmoci v AI musÃ­ zahÃ¡jit pÅ™Ã­mÃ½ dialog o rizicÃ­ch. V listopadu 2024 vydali prezidenti Biden a Xi Jinping prvnÃ­ podstatnÃ© spoleÄnÃ© prohlÃ¡Å¡enÃ­ o nÃ¡rodnÄ› bezpeÄnostnÃ­ch hrozbÃ¡ch AI, vÄetnÄ› potÅ™eby udrÅ¾et lidskou kontrolu nad rozhodovÃ¡nÃ­m o pouÅ¾itÃ­ jadernÃ½ch zbranÃ­. ÄŒlÃ¡nek Jakea Sullivana, bÃ½valÃ©ho poradce USA pro nÃ¡rodnÃ­ bezpeÄnost, volÃ¡ po intenzivnÄ›jÅ¡Ã­ spoluprÃ¡ci.

## KlÃ­ÄovÃ© body
- PrvnÃ­ spoleÄnÃ© prohlÃ¡Å¡enÃ­ Bidena a Xi Jinpinga z listopadu 2024 zdÅ¯razÅˆuje rizika AI pro nÃ¡rodnÃ­ bezpeÄnost, zejmÃ©na v souvislosti s jadernÃ½mi zbranÄ›mi.
- USA a ÄŒÃ­na jsou jedinÃ© dvÄ› zemÄ› schopnÃ© dominovat v AI, coÅ¾ vyÅ¾aduje jejich pÅ™Ã­mou angaÅ¾ovanost pÅ™i mitigaci rizik.
- Rizika zahrnujÃ­ naruÅ¡enÃ­ trhÅ¯, infrastruktury, sociÃ¡lnÃ­ stability a existenciÃ¡lnÃ­ hrozby pro lidstvo.
- SoutÄ›Å¾ o AI vedenÃ­ nesmÃ­ brÃ¡nit spoluprÃ¡ci na bezpeÄnostnÃ­ch standardech.
- Autor navrhuje strukturovanÃ½ dialog mezi obÄ›ma mocnostmi.

## Podrobnosti
ÄŒlÃ¡nek vychÃ¡zÃ­ z kontextu listopadovÃ©ho summitu v roce 2024, kde se Biden a Xi shodli na nutnosti zachovat lidskou kontrolu nad jadernÃ½mi zbranÄ›mi v Ã©Å™e AI. Tato deklarace pÅ™edstavuje prvnÃ­ explicitnÃ­ uznÃ¡nÃ­, Å¾e pokroÄilÃ© AI systÃ©my, jako velkÃ© jazykovÃ© modely (LLM) nebo autonomnÃ­ rozhodovacÃ­ systÃ©my, mohou ovlivnit kritickÃ© vojenskÃ© procesy. NapÅ™Ã­klad AI algoritmy pro detekci hrozeb nebo optimalizaci odpovÄ›dÃ­ by mohly vÃ©st k eskalaci konfliktÅ¯, pokud nebudou pod lidskÃ½m dohledem.

Jake Sullivan, kterÃ½ slouÅ¾il jako nÃ¡rodnÃ­ bezpeÄnostnÃ­ poradce USA do roku 2025, argumentuje, Å¾e soutÄ›Å¾ mezi USA a ÄŒÃ­nou o AI pÅ™evahu â€“ viditelnÃ¡ v investicÃ­ch do ÄipÅ¯, datovÃ½ch center a modelÅ¯ jako GPT nebo ÄÃ­nskÃ½ch ekvivalentÅ¯ â€“ nesmÃ­ ignorovat rizika. Mezi nimi patÅ™Ã­ ekonomickÃ¡ destabilizace trhÅ¯ masivnÃ­ automatizacÃ­, kybernetickÃ© Ãºtoky zesÃ­lenÃ© AI (napÅ™. generovÃ¡nÃ­ phishingu nebo exploitÅ¯ zero-day), naruÅ¡enÃ­ kritickÃ© infrastruktury jako elektrickÃ© sÃ­tÄ› nebo dopravnÃ­ systÃ©my a sociÃ¡lnÃ­ nestabilita zpÅ¯sobenÃ¡ dezinformacemi z AI generovanÃ©ho obsahu. ExistenciÃ¡lnÃ­ rizika, jako nesprÃ¡vnÃ© zacÃ­lenÃ­ AI v autonomnÃ­ch zbranÃ­ch nebo nekontrolovanÃ½ pokrok smÄ›rem k AGI (umÄ›lÃ© obecnÃ© inteligenci), vyÅ¾adujÃ­ globÃ¡lnÃ­ koordinaci.

Sullivan navrhuje pÅ™Ã­mÃ½ bilaterÃ¡lnÃ­ dialog, podobnÃ½ jednÃ¡nÃ­m o jadernÃ© zbranÄ› bÄ›hem studenÃ© vÃ¡lky. To by zahrnovalo sdÃ­lenÃ­ standardÅ¯ pro AI bezpeÄnost, testovÃ¡nÃ­ robustness modelÅ¯ proti jailbreakÅ¯m nebo halucinacÃ­m a mechanismy pro prevenci zneuÅ¾itÃ­ v asymetrickÃ© vÃ¡lce. V praxi by to znamenalo pro prÅ¯mysl povinnÃ© audity AI systÃ©mÅ¯, mezinÃ¡rodnÃ­ protokoly pro nasazenÃ­ autonomnÃ­ch systÃ©mÅ¯ a omezenÃ­ exportu kritickÃ½ch technologiÃ­ jako GPU pro trÃ©nink modelÅ¯. Pro uÅ¾ivatele a firmy to pÅ™ineslo by vÄ›tÅ¡Ã­ jistotu v nasazenÃ­ AI nÃ¡strojÅ¯, jako jsou chatboti nebo prediktivnÃ­ analytics, bez rizika neoÄekÃ¡vanÃ½ch selhÃ¡nÃ­.

## ProÄ je to dÅ¯leÅ¾itÃ©
Toto volÃ¡nÃ­ po spoluprÃ¡ci pÅ™ichÃ¡zÃ­ v dobÄ›, kdy USA zavÃ¡dÄ›jÃ­ exportnÃ­ kontroly na AI Äipy do ÄŒÃ­ny a Peking investuje miliardy do domÃ¡cÃ­ch alternativ. Bez koordinace hrozÃ­ zÃ¡vod na dno v bezpeÄnostnÃ­ch standardech, kde soutÄ›Å¾Ã­cÃ­ strany upÅ™ednostnÃ­ rychlost pÅ™ed bezpeÄnostÃ­. V Å¡irÅ¡Ã­m ekosystÃ©mu to ovlivnÃ­ vÃ½voj robotiky, autonomnÃ­ch vozidel a brain-computer interfaces, kde selhÃ¡nÃ­ AI mÅ¯Å¾e mÃ­t fatÃ¡lnÃ­ dÅ¯sledky. Pro Evropu a dalÅ¡Ã­ regiony to znamenÃ¡ tlak na vlastnÃ­ regulace, jako EU AI Act, aby se zapojily do globÃ¡lnÃ­ho rÃ¡mce. Pokud dialog selÅ¾e, rizika jako AI-sposobenÃ© krize by mohly pÅ™ekroÄit nÃ¡rodnÃ­ hranice a destabilizovat celÃ½ svÄ›tovÃ½ Å™Ã¡d.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.project-syndicate.org/magazine/china-us-ai-risks-call-for-urgent-diplomacy-by-jake-sullivan-2025-12)

**Zdroj:** ğŸ“° Project Syndicate
