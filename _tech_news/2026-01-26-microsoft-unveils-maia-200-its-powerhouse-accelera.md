---
author: Marisa Aigen
category: ai hardware
companies:
- Microsoft
date: '2026-01-26 16:00:00'
description: Maia 200 s vÃ­ce neÅ¾ 100 miliardami tranzistorÅ¯ nabÃ­zÃ­ vysokÃ½ vÃ½kon pro
  AI inferencing, pÅ™ekonÃ¡vÃ¡ konkurenty od AWS a Google Cloud.
importance: 4
layout: tech_news_article
original_title: Microsoft unveils Maia 200, its 'powerhouse' accelerator looking to
  unlock the power of large-scale AI
publishedAt: '2026-01-26T16:00:00+00:00'
slug: microsoft-unveils-maia-200-its-powerhouse-accelera
source:
  emoji: ğŸ“°
  id: techradar
  name: TechRadar
title: Microsoft pÅ™edstavuje Maia 200, svÅ¯j 'powerhouse' akcelerÃ¡tor pro velkoÅ¡kÃ¡lovou
  AI
url: https://www.techradar.com/pro/microsoft-unveils-maia-200-its-powerhouse-accelerator-looking-to-unlock-the-power-of-large-scale-ai
urlToImage: https://cdn.mos.cms.futurecdn.net/BSVBEAGEWfwryuSnecEAQm-970-80.jpg
urlToImageBackup: https://cdn.mos.cms.futurecdn.net/BSVBEAGEWfwryuSnecEAQm-970-80.jpg
---

## Souhrn
Microsoft pÅ™edstavil Maia 200, nÃ¡stupce Äipu Maia 100, kterÃ½ je urÄen pro zpracovÃ¡nÃ­ inference u velkÃ½ch AI modelÅ¯. Tento akcelerÃ¡tor s vÃ­ce neÅ¾ 100 miliardami tranzistorÅ¯ na TSMC 3nm procesu dosahuje vÃ½konu pÅ™es 10 PFLOPS v 4bitovÃ© pÅ™esnosti (FP4) a kolem 5 PFLOPS v 8bitovÃ© (FP8), coÅ¾ umoÅ¾Åˆuje efektivnÃ­ provoz i nejvÄ›tÅ¡Ã­ch souÄasnÃ½ch modelÅ¯.

## KlÃ­ÄovÃ© body
- VÃ­ce neÅ¾ 100 miliard tranzistorÅ¯ na TSMC 3nm procesu s nativnÃ­mi FP8/FP4 tensor cores pro rychlÃ© vektorovÃ© operace v AI.
- PÅ™epracovanÃ½ pamÄ›Å¥ovÃ½ systÃ©m: 216 GB HBM3e pÅ™i pÅ™enosovÃ© rychlosti 7 TB/s a 272 MB on-chip SRAM pro lokÃ¡lnÃ­ uklÃ¡dÃ¡nÃ­ dat.
- VÃ½kon: pÅ™es 10 PFLOPS (FP4) a asi 5 PFLOPS (FP8), coÅ¾ je 3x vÃ­ce v FP4 neÅ¾ tÅ™etÃ­ generace Amazon Trainium a lepÅ¡Ã­ v FP8 neÅ¾ sedmÃ¡ generace Google TPU.
- Optimalizace pro inference: specializovanÃ½ DMA engine, NoC fabric pro vysokorychlostnÃ­ pÅ™enos dat a zamÄ›Å™enÃ­ na ÃºzkopÅ™esnÃ© datovÃ© typy.
- PouÅ¾itÃ­: internÄ› pro zlepÅ¡enÃ­ Copilotu v Azure, dostupnÃ© i pro zÃ¡kaznÃ­ky.

## Podrobnosti
Maia 200 pÅ™edstavuje vÃ½znamnÃ½ krok v hardwarovÃ© infrastruktuÅ™e Microsoftu pro umÄ›lou inteligenci, konkrÃ©tnÄ› pro fÃ¡zi inference, kdy se trÃ©novanÃ© AI modely pouÅ¾Ã­vajÃ­ k generovÃ¡nÃ­ odpovÄ›dÃ­ v reÃ¡lnÃ©m Äase. Na rozdÃ­l od trÃ©novacÃ­ch ÄipÅ¯ jako Nvidia H100 se Maia 200 soustÅ™edÃ­ na efektivitu pÅ™i opakovanÃ©m spouÅ¡tÄ›nÃ­ modelÅ¯, coÅ¾ je klÃ­ÄovÃ© pro sluÅ¾by jako ChatGPT nebo Microsoft Copilot. ÄŒip je vyroben na pokroÄilÃ©m 3nm procesu od TSMC, coÅ¾ umoÅ¾Åˆuje vysokou hustotu tranzistorÅ¯ a niÅ¾Å¡Ã­ spotÅ™ebu energie.

KlÃ­Äovou inovacÃ­ je pamÄ›Å¥ovÃ½ subsystÃ©m optimalizovanÃ½ pro ÃºzkopÅ™esnÃ© datovÃ© typy jako FP4 a FP8, kterÃ© sniÅ¾ujÃ­ velikost dat bez vÃ½raznÃ© ztrÃ¡ty pÅ™esnosti u velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM). HBM3e pamÄ›Å¥ o objemu 216 GB s bandwidth 7 TB/s zajiÅ¡Å¥uje rychlÃ½ pÅ™Ã­stup k velkÃ½m objemÅ¯m dat, zatÃ­mco 272 MB SRAM na Äipu minimalizuje latenci tÃ­m, Å¾e drÅ¾Ã­ vÄ›tÅ¡inu vah modelÅ¯ lokÃ¡lnÄ›. SpecializovanÃ½ DMA engine a NoC (Network-on-Chip) fabric usnadÅˆujÃ­ pÅ™enos dat mezi jÃ¡dry, coÅ¾ vede k potÅ™ebÄ› mÃ©nÄ› ÄipÅ¯ pro spuÅ¡tÄ›nÃ­ stejnÃ©ho modelu â€“ napÅ™Ã­klad oproti pÅ™edchozÃ­ generaci nebo konkurenÄnÃ­m Å™eÅ¡enÃ­m.

Microsoft tvrdÃ­, Å¾e Maia 200 dramaticky mÄ›nÃ­ ekonomiku velkoÅ¡kÃ¡lovÃ© AI tÃ­m, Å¾e zvyÅ¡uje vÃ½kon a sniÅ¾uje nÃ¡klady na provoz v cloudu Azure. Bude slouÅ¾it k internÃ­mu zlepÅ¡enÃ­ Copilotu, coÅ¾ je AI asistent integrovanÃ½ do Office a Windows, ale stane se dostupnÃ½m i pro zÃ¡kaznÃ­ky Azure, kteÅ™Ã­ chtÄ›jÃ­ rychleji a levnÄ›ji spouÅ¡tÄ›t vlastnÃ­ modely. V porovnÃ¡nÃ­ s konkurencÃ­: 3x vyÅ¡Å¡Ã­ FP4 vÃ½kon neÅ¾ Amazon Trainium3 (urÄenÃ½ pro trÃ©nink i inference v AWS) a vyÅ¡Å¡Ã­ FP8 neÅ¾ Google TPUv7 (Tensor Processing Unit pro Google Cloud). To posiluje pozici Azure v soutÄ›Å¾i o AI workloady, kde Nvidia stÃ¡le dominuje, ale cloudovÃ­ giganti budujÃ­ vlastnÃ­ Äipy pro nezÃ¡vislost.

## ProÄ je to dÅ¯leÅ¾itÃ©
V Ã©Å™e explodujÃ­cÃ­ch nÃ¡rokÅ¯ na vÃ½poÄetnÃ­ vÃ½kon pro AI inference, kde modely jako GPT-4o nebo Llama 3 vyÅ¾adujÃ­ obrovskÃ© zdroje, Maia 200 umoÅ¾Åˆuje Å¡kÃ¡lovat sluÅ¾by bez proporcionalnÃ­ho rÅ¯stu nÃ¡kladÅ¯. Pro prÅ¯mysl znamenÃ¡ vÄ›tÅ¡Ã­ efektivitu v cloudu, mÃ©nÄ› zÃ¡vislosti na Nvidia a konkurenÄnÃ­ tlak na AWS a GCP k inovacÃ­m. Pro uÅ¾ivatele Azure to pÅ™inese rychlejÅ¡Ã­ AI aplikace, jako lepÅ¡Ã­ Copilot v Teams nebo Edge, a potenciÃ¡lnÄ› niÅ¾Å¡Ã­ ceny. DlouhodobÄ› posiluje Microsoft v zÃ¡vodÄ› o AI dominanci, kde hardware rozhoduje o marÅ¾Ã­ch a rychlosti nasazenÃ­ novÃ½ch modelÅ¯. CelkovÄ› pÅ™ispÃ­vÃ¡ k demokratizaci velkÃ© AI tÃ­m, Å¾e sniÅ¾uje bariÃ©ry pro enterprise nasazenÃ­.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.techradar.com/pro/microsoft-unveils-maia-200-its-powerhouse-accelerator-looking-to-unlock-the-power-of-large-scale-ai)

**Zdroj:** ğŸ“° TechRadar
