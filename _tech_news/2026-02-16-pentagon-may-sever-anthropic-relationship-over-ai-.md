---
author: Marisa Aigen
category: ai etika
companies:
- Anthropic
date: '2026-02-16 18:35:00'
description: Anthropic vÃ¡hÃ¡ s povolenÃ­m Pentagonu pokraÄovat v pouÅ¾Ã­vÃ¡nÃ­ modelu Claude
  kvÅ¯li obavÃ¡m z poruÅ¡enÃ­ svÃ½ch zÃ¡sad pouÅ¾Ã­vÃ¡nÃ­. Pentagon hrozÃ­ zruÅ¡enÃ­m 200milionovÃ©ho
  kontraktu kvÅ¯li nesouhlasu s omezenÃ­mi na autonomnÃ­ zbranÄ› a masovÃ© sledovÃ¡nÃ­.
importance: 4
layout: tech_news_article
original_title: Pentagon may sever Anthropic relationship over AI safeguards - Claude
  maker expresses concerns over 'hard limits around fully autonomous weapons and mass
  domestic surveillance'
publishedAt: '2026-02-16T18:35:00+00:00'
slug: pentagon-may-sever-anthropic-relationship-over-ai-
source:
  emoji: ğŸ“°
  id: techradar
  name: TechRadar
title: Pentagon mÅ¯Å¾e ukonÄit vztahy s Anthropic kvÅ¯li bezpeÄnostnÃ­m omezenÃ­m AI -
  VÃ½robce Claude vyjadÅ™uje obavy z 'tvrdÃ½ch limitÅ¯ kolem plnÄ› autonomnÃ­ch zbranÃ­ a
  masovÃ©ho domÃ¡cÃ­ho sledovÃ¡nÃ­'
url: https://www.techradar.com/pro/security/pentagon-may-sever-anthropic-relationship-over-ai-safeguards-claude-maker-expresses-concerns-over-hard-limits-around-fully-autonomous-weapons-and-mass-domestic-surveillance
urlToImage: https://cdn.mos.cms.futurecdn.net/scxFkyfYSQbrtGvqrmFqgU-2560-80.jpg
urlToImageBackup: https://cdn.mos.cms.futurecdn.net/scxFkyfYSQbrtGvqrmFqgU-2560-80.jpg
---

## Souhrn
Pentagon a Anthropic jsou v konfliktu ohlednÄ› pouÅ¾itÃ­ modelu Claude v armÃ¡dnÃ­ch operacÃ­ch. Anthropic odmÃ­tÃ¡ povolit vyuÅ¾itÃ­ svÃ©ho AI pro plnÄ› autonomnÃ­ zbranÄ› nebo masovÃ© domÃ¡cÃ­ sledovÃ¡nÃ­, coÅ¾ vede k hrozbÄ› zruÅ¡enÃ­ 200milionovÃ©ho kontraktu. JinÃ© AI firmy jako OpenAI, Google a xAI ÄelÃ­ podobnÃ©mu tlaku na uvolnÄ›nÃ­ omezenÃ­ pro "vÅ¡echny zÃ¡konnÃ© ÃºÄely".

## KlÃ­ÄovÃ© body
- Pentagon poÅ¾aduje od AI dodavatelÅ¯ povolenÃ­ pouÅ¾Ã­vÃ¡nÃ­ modelÅ¯ pro vÅ¡echny zÃ¡konnÃ© ÃºÄely, vÄetnÄ› potenciÃ¡lnÄ› citlivÃ½ch armÃ¡dnÃ­ch aplikacÃ­.
- Anthropic striktnÄ› zakazuje pouÅ¾itÃ­ Claude v plnÄ› autonomnÃ­ch zbranÃ­ch a masovÃ©m domÃ¡cÃ­m sledovÃ¡nÃ­ podle svÃ½ch zÃ¡sad pouÅ¾Ã­vÃ¡nÃ­.
- Model Claude byl podle Wall Street Journal pouÅ¾it v americkÃ© vojenskÃ© operaci na dopadenÃ­ venezuelskÃ©ho prezidenta NicolÃ¡se Madura.
- AnonymnÃ­ zdroj z Trumpovy administrativy uvedl, Å¾e jedna firma souhlasila s plnÃ½m pÅ™Ã­stupem, dvÄ› dalÅ¡Ã­ projevily flexibilitu.
- Konflikt eskaloval od ledna, Pentagon hrozÃ­ ukonÄenÃ­m spoluprÃ¡ce s Anthropic.

## Podrobnosti
Anthropic, firma specializujÃ­cÃ­ se na vÃ½voj velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) s dÅ¯razem na bezpeÄnost a zarovnÃ¡nÃ­ s lidskÃ½mi hodnotami, vyvinula model Claude jako konkurenÄnÃ­ alternativu k GPT od OpenAI. Claude slouÅ¾Ã­ k zpracovÃ¡nÃ­ textu, analÃ½ze dat a generovÃ¡nÃ­ odpovÄ›dÃ­, ale Anthropic do nÄ›j implementovalo tvrdÃ© bezpeÄnostnÃ­ limity, kterÃ© brÃ¡nÃ­ aplikacÃ­m v rizikovÃ½ch oblastech. Tyto limity explicitnÄ› zahrnujÃ­ zÃ¡kaz podpory pro plnÄ› autonomnÃ­ zbranÄ› â€“ systÃ©my, kterÃ© samostatnÄ› rozhodujÃ­ o ÃºtocÃ­ch bez lidskÃ©ho zÃ¡sahu â€“ a masovÃ© domÃ¡cÃ­ sledovÃ¡nÃ­, coÅ¾ by mohlo zahrnovat analÃ½zu dat o obÄanech bez souhlasu.

Pentagon, ministerstvo obrany USA, mÃ¡ s Anthropic kontrakt v hodnotÄ› 200 milionÅ¯ dolarÅ¯ na vyuÅ¾itÃ­ Claude pro rÅ¯znÃ© operace. Podle zprÃ¡v Wall Street Journal byl Claude nasazen v operaci na dopadenÃ­ NicolÃ¡se Madura, bÃ½valÃ©ho venezuelskÃ©ho prezidenta, kde pravdÄ›podobnÄ› pomÃ¡hal s analÃ½zou dat nebo plÃ¡novÃ¡nÃ­m. Tento incident od ledna otÅ™Ã¡sl vztahy. Pentagon nynÃ­ tlaÄÃ­ na vÅ¡echny hlavnÃ­ AI dodavatele â€“ Anthropic, OpenAI, Google a xAI (firmu Elona Muska zamÄ›Å™enou na pokroÄilÃ© AI modely) â€“ aby povolily Å¡irokÃ© pouÅ¾itÃ­. Podle anonymnÃ­ho poradce Trumpovy administrativy pro Axios jedna z tÄ›chto firem jiÅ¾ souhlasila s neomezenÃ½m pÅ™Ã­stupem, dvÄ› dalÅ¡Ã­ ukazujÃ­ flexibilitu, ale Anthropic trvÃ¡ na svÃ½ch zÃ¡sadÃ¡ch. MluvÄÃ­ Anthropic potvrdil, Å¾e firma nediskutovala podrobnosti, ale zdÅ¯raznil dÅ¯raz na etickÃ© pouÅ¾itÃ­.

Tento spor odhaluje Å¡irÅ¡Ã­ napÄ›tÃ­ mezi technologickÃ½mi firmami a vlÃ¡dou. AI modely jako Claude lze pouÅ¾Ã­t k automatizaci rozhodovÃ¡nÃ­ v reÃ¡lnÃ©m Äase, napÅ™Ã­klad v dronovÃ½ch systÃ©mech nebo sledovacÃ­ch sÃ­tÃ­ch, coÅ¾ zvyÅ¡uje rizika chyb nebo zneuÅ¾itÃ­. Pentagon argumentuje nÃ¡rodnÃ­ bezpeÄnostÃ­, zatÃ­mco firmy se obÃ¡vajÃ­ odpovÄ›dnosti za Å¡kodlivÃ© dÅ¯sledky. Pokud dojde k ukonÄenÃ­ kontraktu, Anthropic pÅ™ijde o vÃ½znamnÃ½ pÅ™Ã­jem, ale posÃ­lÃ­ svou reputaci v etickÃ© AI komunitÄ›.

## ProÄ je to dÅ¯leÅ¾itÃ©
Tento konflikt nastavuje precedent pro budoucÃ­ regulaci AI v armÃ¡dnÃ­ch aplikacÃ­ch. V Å¡irÅ¡Ã­m ekosystÃ©mu AI, kde modely jako Claude, GPT nebo Gemini dosahujÃ­ ÃºrovnÄ›, kterÃ¡ umoÅ¾Åˆuje sofistikovanou analÃ½zu, etickÃ¡ omezenÃ­ brÃ¡nÃ­ rychlÃ©mu nasazenÃ­ v konfliktech. Pro prÅ¯mysl to znamenÃ¡ rostoucÃ­ tlak na vyvaÅ¾ovÃ¡nÃ­ komerÄnÃ­ch zÃ¡jmÅ¯ s morÃ¡lnÃ­mi standardy â€“ firmy jako Anthropic, kterÃ© priorizujÃ­ bezpeÄnost (constitutional AI), mohou inspirovat dalÅ¡Ã­, zatÃ­mco souhlas xAI ukazuje rozdÃ­lnÃ© pÅ™Ã­stupy. Pro uÅ¾ivatele a spoleÄnost to podtrhuje rizika: autonomnÃ­ zbranÄ› by mohly eskalovat vÃ¡lky bez lidskÃ©ho dohledu, zatÃ­mco masovÃ© sledovÃ¡nÃ­ ohroÅ¾uje soukromÃ­. V USA, kde AI vÃ½zkum vede svÄ›t, tento spor ovlivnÃ­ globÃ¡lnÃ­ standardy a mÅ¯Å¾e vÃ©st k legislativÄ› jako EU AI Act. DlouhodobÄ› to urÄÃ­, zda AI zÅ¯stane nÃ¡strojem pro obranu nebo se stane neregulovanou zbranÃ­. (Celkem 528 slov)

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://www.techradar.com/pro/security/pentagon-may-sever-anthropic-relationship-over-ai-safeguards-claude-maker-expresses-concerns-over-hard-limits-around-fully-autonomous-weapons-and-mass-domestic-surveillance)

**Zdroj:** ğŸ“° TechRadar
