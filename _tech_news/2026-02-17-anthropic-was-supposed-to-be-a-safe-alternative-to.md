---
author: Marisa Aigen
category: ai bezpeÄnost
companies:
- Anthropic
- OpenAI
date: '2026-02-17 19:25:08'
description: GenerÃ¡lnÃ­ Å™editel Anthropicu Å™Ã­kÃ¡, Å¾e intenzivnÃ­ komerÄnÃ­ tlak testuje
  misi spoleÄnosti zamÄ›Å™enou na bezpeÄnou AI.
importance: 4
layout: tech_news_article
original_title: Anthropic was supposed to be a â€˜safeâ€™ alternative to OpenAI, but CEO
  Dario Amodei admits his company struggles to balance safety with profits
people:
- Dario Amodei
publishedAt: '2026-02-17T19:25:08+00:00'
slug: anthropic-was-supposed-to-be-a-safe-alternative-to
source:
  emoji: ğŸ“°
  id: fortune
  name: Fortune
title: Anthropic mÄ›l bÃ½t 'bezpeÄnou' alternativou k OpenAI, ale CEO Dario Amodei pÅ™iznÃ¡vÃ¡,
  Å¾e jeho spoleÄnost bojuje s vyvaÅ¾ovÃ¡nÃ­m bezpeÄnosti se zisky
url: https://fortune.com/2026/02/17/anthropic-ceo-dario-amodei-balancing-safety-commercial-pressure-ai-race-openai/
urlToImage: https://fortune.com/img-assets/wp-content/uploads/2026/02/GettyImages-2261514463-e1771354847602.jpg?resize=1200,600
urlToImageBackup: https://fortune.com/img-assets/wp-content/uploads/2026/02/GettyImages-2261514463-e1771354847602.jpg?resize=1200,600
---

### Souhrn
GenerÃ¡lnÃ­ Å™editel Anthropicu Dario Amodei v rozhovoru pro podcast Dwarkesh Patel pÅ™iznal, Å¾e jeho spoleÄnost ÄelÃ­ enormnÃ­mu komerÄnÃ­mu tlaku, kterÃ½ ztÄ›Å¾uje soulad pÅ¯vodnÃ­ mise na bezpeÄnou AI s potÅ™ebou inovacÃ­ a zisku. Anthropic byl zaloÅ¾en v roce 2019 jako bezpeÄnÄ›jÅ¡Ã­ alternativa k OpenAI, odkud Amodei odeÅ¡el spoleÄnÄ› s dalÅ¡Ã­mi zamÄ›stnanci, vÄetnÄ› svÃ© sestry Daniely AmodeiovÃ©. MinulÃ½ tÃ½den spoleÄnost oznÃ¡mila fundraising ve vÃ½Å¡i 30 miliard dolarÅ¯ pÅ™i post-money valuaci 380 miliard dolarÅ¯, coÅ¾ ji Å™adÃ­ mezi nejcennÄ›jÅ¡Ã­ soukromÃ© firmy svÄ›ta.

### KlÃ­ÄovÃ© body
- Amodei odeÅ¡el z OpenAI, kde byl viceprezidentem pro vÃ½zkum zamÄ›Å™enÃ½ na bezpeÄnost, kvÅ¯li nedostateÄnÃ©mu dÅ¯razu na rizika AI.
- Anthropic vyvÃ­jÃ­ velkÃ½ jazykovÃ½ model Claude s dÅ¯razem na bezpeÄnost prostÅ™ednictvÃ­m Constitutional AI, kde model zÃ­skÃ¡vÃ¡ hodnoty mÃ­sto striktnÃ­ch pravidel.
- SpoleÄnost mÃ¡ Responsible Scaling Policy, podle kterÃ© nepublikuje modely schopnÃ© katastrofickÃ© Å¡kody.
- CEO zdÅ¯razÅˆuje tlak na udrÅ¾enÃ­ 10nÃ¡sobnÃ©ho rÅ¯stu pÅ™Ã­jmÅ¯ pÅ™i zachovÃ¡nÃ­ bezpeÄnostnÃ­ch opatÅ™enÃ­, kterÃ¡ jsou nÃ¡roÄnÄ›jÅ¡Ã­ neÅ¾ u konkurentÅ¯.
- Fundraising 30 miliard dolarÅ¯ potvrzuje rychlÃ½ rÅ¯st, ale zÃ¡roveÅˆ zvyÅ¡uje oÄekÃ¡vÃ¡nÃ­ investorÅ¯.

### Podrobnosti
Anthropic, firma zamÄ›Å™enÃ¡ na vÃ½voj velkÃ½ch jazykovÃ½ch modelÅ¯ s dÅ¯razem na bezpeÄnost, vznikla v roce 2019 jako odpovÄ›Ä na Amodeiho nespokojenost s pÅ™Ã­stupem OpenAI. OpenAI, pÅ¯vodnÄ› neziskovÃ¡ organizace vyvÃ­jejÃ­cÃ­ modely jako GPT sÃ©rie pro obecnÃ© Ãºkoly zpracovÃ¡nÃ­ textu a generovÃ¡nÃ­ obsahu, podle Amodeiho pÅ™Ã­liÅ¡ rychle priorizovala komerÄnÃ­ expanzi na Ãºkor rizikovÃ©ho managementu. Amodei, tehdejÅ¡Ã­ viceprezident pro vÃ½zkum, opustil firmu spoleÄnÄ› s kolegy vÄetnÄ› Daniely AmodeiovÃ©, kterÃ¡ se stala spoluzakladatelkou a COO. SpoleÄnÄ› vybudovali Claude, model navrÅ¾enÃ½ pro bezpeÄnÃ© chovÃ¡nÃ­.

KlÃ­ÄovÃ½m prvkem je Constitutional AI, pÅ™Ã­stup, kde model nenÃ­ omezen tisÃ­ci pravidel, ale nauÄÃ­ se hodnoty jako upÅ™Ã­mnost, nÃ¡paditost nebo respekt k lidskÃ½m prÃ¡vÅ¯m. To umoÅ¾Åˆuje Claude reagovat flexibilnÄ›ji na sloÅ¾itÃ© scÃ©nÃ¡Å™e, napÅ™Ã­klad v konverzacÃ­ch nebo analÃ½zÃ¡ch textu, bez rizika generovÃ¡nÃ­ Å¡kodlivÃ©ho obsahu. DalÅ¡Ã­ vrstvou je Responsible Scaling Policy, kterÃ¡ stanovuje hranice pro nasazenÃ­ modelÅ¯ podle jejich schopnostÃ­ â€“ napÅ™Ã­klad modely s potenciÃ¡lem pro chemickÃ© zbranÄ› nebo kybernetickÃ© Ãºtoky nejsou uvolnÄ›ny.

Navzdory tÄ›mto opatÅ™enÃ­m Amodei v podcastu pÅ™iznÃ¡vÃ¡ realitu: "Jsme pod obrovskÃ½m komerÄnÃ­m tlakem a jeÅ¡tÄ› si to ztÄ›Å¾ujeme naÅ¡Ã­ bezpeÄnostÃ­, kterou dÄ›lÃ¡me vÃ­c neÅ¾ ostatnÃ­." Mezi "Velkou ÄtveÅ™ici" AI firem (OpenAI, Google, Meta, xAI) panuje zÃ¡vod o rychlÃ© vydÃ¡vÃ¡nÃ­ novÃ½ch verzÃ­ modelÅ¯, coÅ¾ vyÅ¾aduje masivnÃ­ investice do vÃ½poÄetnÃ­ch zdrojÅ¯ jako GPU clustery. Anthropic musÃ­ udrÅ¾et 10nÃ¡sobnÃ½ rÅ¯st pÅ™Ã­jmÅ¯, aby pÅ™eÅ¾il, pÅ™iÄemÅ¾ bezpeÄnostnÃ­ testy zpÅ¯sobujÃ­ zpoÅ¾dÄ›nÃ­. NedÃ¡vnÃ½ fundraising, vedenÃ½ pravdÄ›podobnÄ› investory jako Amazon nebo Google (kterÃ© jiÅ¾ spolupracujÃ­ na API pÅ™Ã­stupu k Claude), posÃ­lil pozici firmy, ale zÃ¡roveÅˆ zvÃ½Å¡il oÄekÃ¡vÃ¡nÃ­ na ziskovost. Amodei srovnÃ¡vÃ¡ situaci s tlakem v biotech sektoru, kde bezpeÄnost brzdÃ­ rychlost, ale bez nÃ­ hrozÃ­ selhÃ¡nÃ­.

### ProÄ je to dÅ¯leÅ¾itÃ©
Toto pÅ™iznÃ¡nÃ­ odhaluje systÃ©movÃ½ problÃ©m v AI prÅ¯myslu: firmy se profilujÃ­cÃ­ jako bezpeÄnostnÃ­ lÃ­dÅ™i ÄelÃ­ stejnÃ½m ekonomickÃ½m tlakÅ¯m jako konkurence, coÅ¾ mÅ¯Å¾e vÃ©st k oslabenÃ­ bezpeÄnostnÃ­ch standardÅ¯. Pro uÅ¾ivatele znamenÃ¡ riziko, Å¾e i modely jako Claude, urÄenÃ© pro podnikovÃ© aplikace jako analÃ½za dat nebo automatizace, mohou v honbÄ› za ziskem pÅ™ijmout kompromisy v etice. V Å¡irÅ¡Ã­m kontextu to podtrhuje nutnost regulacÃ­ â€“ napÅ™Ã­klad evropskÃ©ho AI Actu nebo americkÃ½ch smÄ›rnic â€“ aby investorÃ© neprosazovali rychlost na Ãºkor rizik jako dezinformace nebo zneuÅ¾itÃ­ v kybernetice. Pokud i Anthropic ustoupÃ­, celÃ½ sektor mÅ¯Å¾e ztratit dÅ¯vÄ›ryhodnost, coÅ¾ ovlivnÃ­ adopci AI v kritickÃ½ch oblastech jako zdravotnictvÃ­ nebo obrana.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://fortune.com/2026/02/17/anthropic-ceo-dario-amodei-balancing-safety-commercial-pressure-ai-race-openai/)

**Zdroj:** ğŸ“° Fortune
