---
author: Marisa Aigen
category: umÄ›lÃ¡ inteligence
companies:
- Anthropic
date: '2026-02-24 03:34:54'
description: Mrinank Sharma, vedoucÃ­ tÃ½mu pro bezpeÄnostnÃ­ opatÅ™enÃ­ v Anthropic, jednÃ©
  z pÅ™ednÃ­ch firem v oblasti umÄ›lÃ© inteligence, rezignoval po dvou letech v Äele Safeguards
  Research Team. Ve svÃ©m virÃ¡lnÃ­m dopise varoval pÅ™ed globÃ¡lnÃ­m nebezpeÄÃ­m spojenÃ½m
  s vÃ½vojem AI a odchÃ¡zÃ­ studovat poezii.
importance: 4
layout: tech_news_article
original_title: 'Artificial Intelligence - "The World Is In Peril": Anthropic''s Safety
  Boss Quits'
people:
- Mrinank Sharma
publishedAt: '2026-02-24T03:34:54+00:00'
slug: artificial-intelligence-the-world-is-in-peril-anth
source:
  emoji: ğŸ“°
  id: null
  name: Freerepublic.com
title: 'UmÄ›lÃ¡ inteligence â€“ â€SvÄ›t je v nebezpeÄÃ­â€œ: Å Ã©f bezpeÄnosti Anthropic rezignuje'
url: https://freerepublic.com/focus/f-chat/4368066/posts
---

## Souhrn
Mrinank Sharma, dosud Å¡Ã©f Safeguards Research Team v Anthropic, rezignoval z pozice, kde dohlÃ­Å¾el na prevenci zneuÅ¾itÃ­ AI pro tvorbu biologickÃ½ch zbranÃ­. Jeho rezignaÄnÃ­ dopis, kterÃ½ zÃ­skal pÅ™es 14 milionÅ¯ zobrazenÃ­ na platformÄ› X, zaÄÃ­nÃ¡ slovy â€svÄ›t je v nebezpeÄÃ­â€œ a konÄÃ­ citacÃ­ bÃ¡snÄ› Williama Stafforda o morÃ¡lnÃ­ niti, kterou nesmÃ­me pustit. Sharma kritizuje, jak v praxi selhÃ¡vajÃ­ hodnoty pÅ™i rozhodovÃ¡nÃ­ o vÃ½voji AI.

## KlÃ­ÄovÃ© body
- Sharma vedl tÃ½m odpovÄ›dnÃ½ za bezpeÄnostnÃ­ mechanismy AI v Anthropic, vÄetnÄ› ochrany proti nÃ¡vrhÅ¯m biologickÃ½ch zbranÃ­.
- Jeho poslednÃ­ projekt zkoumal, jak AI systÃ©my zkreslujÃ­ vnÃ­mÃ¡nÃ­ reality uÅ¾ivateli.
- Dopis zdÅ¯razÅˆuje selhÃ¡nÃ­ v uplatÅˆovÃ¡nÃ­ hodnot v internÃ­ch rozhodnutÃ­ch firmy.
- Sharma odchÃ¡zÃ­ z klÃ­ÄovÃ© role v AI bezpeÄnosti do studia poezie.
- Text dopisu cituje bÃ¡seÅˆ â€The Way It Isâ€œ, symbolizujÃ­cÃ­ morÃ¡lku jako nemÄ›nnou nit Å¾ivota.

## Podrobnosti
Anthropic je americkÃ¡ firma zaloÅ¾enÃ¡ v roce 2021 bÃ½valÃ½mi vÃ½vojÃ¡Å™i OpenAI, zamÄ›Å™enÃ¡ na vÃ½voj velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) jako Claude, kterÃ© majÃ­ bÃ½t bezpeÄnÄ›jÅ¡Ã­ neÅ¾ konkurence jako GPT od OpenAI nebo Gemini od Google. Firma klade dÅ¯raz na tzv. Constitutional AI, kde modely dostÃ¡vajÃ­ vestavÄ›nÃ© principy etiky a bezpeÄnosti, aby minimalizovaly rizika jako dezinformace nebo Å¡kodlivÃ© rady. Sharma pÅ™evzal vedenÃ­ Safeguards Research Team v roce 2022 a jeho tÃ½m testoval modely na schopnost pomoci pÅ™i nÃ¡vrhu biologickÃ½ch zbranÃ­ â€“ konkrÃ©tnÄ› simuloval scÃ©nÃ¡Å™e, kde AI poskytuje pokyny k syntÃ©ze toxinÅ¯ nebo patogenÅ¯. Tyto testy ukazovaly, Å¾e i pokroÄilÃ© modely jako Claude 3.5 mohou bÃ½t obejdeny chytrÃ½mi jailbreaky, coÅ¾ vede k ÃºnikÅ¯m citlivÃ½ch informacÃ­.

Jeho finÃ¡lnÃ­ prÃ¡ce se soustÅ™edila na zkreslenÃ­ reality: AI systÃ©my, trÃ©novanÃ© na obrovskÃ½ch datech z internetu, Äasto zesilujÃ­ biasy, halucinace nebo selektivnÃ­ fakta, coÅ¾ ovlivÅˆuje uÅ¾ivatele v politice, vÄ›dÄ› i kaÅ¾dodennÃ­m rozhodovÃ¡nÃ­. NapÅ™Ã­klad modely mohou prezentovat zkreslenÃ© historickÃ© udÃ¡losti nebo podporovat konspiraÄnÃ­ teorie pod rouÅ¡kou neutrality. Sharma ve dopise popisuje, jak v uzavÅ™enÃ½ch schÅ¯zÃ­ch Anthropic vidÄ›l, Å¾e ekonomickÃ½ tlak a soutÄ›Å¾ s OpenAI a Google vedou k oslabenÃ­ bezpeÄnostnÃ­ch standardÅ¯. â€VidÄ›l jsem to v sobÄ› i v tÃ½mu,â€œ pÃ­Å¡e, a poukazuje na dilemata, kdy rychlost vÃ½voje pÅ™evaÅ¾uje nad riziky. Dopis, publikovanÃ½ 23. Ãºnora 2026 na X (dÅ™Ã­ve Twitter), okamÅ¾itÄ› vyvolal debatu mezi AI etiky, s komentÃ¡Å™i od expertÅ¯ jako Yoshua Bengio nebo Elona Muska, kteÅ™Ã­ podobnÃ© varovÃ¡nÃ­ opakujÃ­. Sharma neuvÃ¡dÃ­ konkrÃ©tnÃ­ incidenty, ale implikuje systÃ©movÃ© selhÃ¡nÃ­ v korporÃ¡tnÃ­m prostÅ™edÃ­, kde priorita je skalovat modely na miliardy parametrÅ¯ za cenu bezpeÄnosti.

## ProÄ je to dÅ¯leÅ¾itÃ©
Odchod Sharma pÅ™edstavuje trhlinu v jednÃ© z nejtvrdÅ¡Ã­ch liniÃ­ AI bezpeÄnosti â€“ Anthropic se prezentuje jako etickÃ¡ alternativa k profitovÄ› Å™Ã­zenÃ½m gigantÅ¯m, ale tento krok naznaÄuje vnitÅ™nÃ­ konflikty. V Å¡irÅ¡Ã­m kontextu posiluje debatu o regulaci AI: EvropskÃ¡ unie pracuje na AI Actu, kterÃ½ klasifikuje modely jako Claude jako vysoce rizikovÃ©, zatÃ­mco USA vÃ¡hÃ¡ s federÃ¡lnÃ­mi pravidly. Pro prÅ¯mysl to znamenÃ¡ riziko reputaÄnÃ­ch ztrÃ¡t a investorÅ¯, kteÅ™Ã­ vyÅ¾adujÃ­ dÅ¯kazy o bezpeÄnosti (Anthropic zÃ­skal funding pÅ™es 7 miliard USD). Pro uÅ¾ivatele to podtrhuje nutnost skeptickÃ©ho pÅ™Ã­stupu k AI vÃ½stupÅ¯m â€“ zkreslenÃ­ reality mÅ¯Å¾e ovlivnit volby nebo veÅ™ejnÃ© zdravÃ­. Pokud i Anthropic selhÃ¡vÃ¡ v morÃ¡lce, zvyÅ¡uje to tlak na globÃ¡lnÃ­ standardy, jako ty od Partnership on AI, a mÅ¯Å¾e zpÅ¯sobit zpomalenÃ­ vÃ½voje AGI. SharmaÅ¯v odchod do poezie ironicky kontrastuje s komercializacÃ­ AI, kde humanitnÃ­ aspekty ustupujÃ­ technologii.

---

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek](https://freerepublic.com/focus/f-chat/4368066/posts)

**Zdroj:** ğŸ“° Freerepublic.com
