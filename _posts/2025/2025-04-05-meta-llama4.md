---
author: Patrick Zandl
post_excerpt: Nov√° verze open source AI LLM Llama 4 m√° v√°lcovat konkurenty jak cenou, tak v√Ωkonem a p≈ôedev≈°√≠m neskuteƒçn√Ωm kontextov√Ωm oknem 10 milion≈Ø token≈Ø. Meta se tak r√°zem posouv√° na ≈°pici peletonu. 
audiooff: true
categories:
- AI
- Meta
- Llama
date: 2025-04-05
layout: post
title: "Meta p≈ôedstavuje Llama 4 - open source LLM s obrovsk√Ωm kontextem i v√Ωkonem"
thumbnail: https://www.marigold.cz/assets/llama4.jpeg
---

Meta pr√°vƒõ vydala novou verzi sv√©ho open source LLM, kter√Ω se jmenuje Llama 4. Ve skuteƒçnosti jde o t≈ôi nov√© modely: Llama 4 Scout, Llama 4 Maverick a Llama 4 Behemoth, kter√© se li≈°√≠ poƒçtem aktivn√≠ch parametr≈Ø a schopnostmi - a samoz≈ôejmƒõ tak√© po≈æadovanou hardware v√Ωbavou. A to je to d≈Øle≈æit√©: nov√° Llama 4 um√≠ bƒõ≈æet na pomƒõrnƒõ p≈ô√≠zniv√© sestavƒõ hardware, p≈ôitom nab√≠z√≠ skvƒõl√Ω v√Ωkon a open source prost≈ôed√≠. A co je naprosto fam√≥zn√≠, je kontextov√© okno na 10 milion≈Ø token≈Ø, co≈æ je 10x v√≠ce ne≈æ dnes nab√≠z√≠ nejpokroƒçilej≈°√≠ LLM Google Gemini. Jistƒõ, rozsah kontextu se projevuje na v√Ωkonu, proto bude zaj√≠mav√© sledovat, jak se to projev√≠ v praxi, pr√°vƒõ p≈ôi pou≈æit√≠ velk√©ho kontextu, kter√Ω je dnes d≈Øle≈æit√Ω nap≈ô√≠klad p≈ôi programov√°n√≠. K dlouh√©mu kontextu si pov√≠me detailn√≠ vysvƒõtlen√≠ na z√°vƒõr. 


üìä **T≈ôi nov√© modely:**
- **Llama 4 Scout:** 17B aktivn√≠ch parametr≈Ø s 16 experty, dok√°≈æe bƒõ≈æet na jedin√© H100 GPU
- **Llama 4 Maverick:** 17B aktivn√≠ch parametr≈Ø se 128 experty, v√Ωjimeƒçn√° multimod√°ln√≠ schopnost
- **Llama 4 Behemoth:** 288B aktivn√≠ch parametr≈Ø, st√°le ve v√Ωvoji, ji≈æ nyn√≠ p≈ôedƒç√≠ GPT-4.5, Claude Sonnet 3.7 a Gemini 2.0 Pro v STEM benchmarc√≠ch

üí° **Kl√≠ƒçov√© technologick√© pr≈Ølomy:**
- Prvn√≠ nativnƒõ multimod√°ln√≠ modely Mety vyu≈æ√≠vaj√≠c√≠ architekturu mixture-of-experts (MoE)
- Pr≈Ølomov√© kontextov√© okno 10M token≈Ø u Llama 4 Scout (10x v√≠ce ne≈æ nab√≠z√≠ Google)
- Tr√©nink na v√≠ce ne≈æ 30 bilionech token≈Ø (dvojn√°sobek oproti Llama 3)
- Podpora pro 200 jazyk≈Ø s 10x v√≠ce multilingv√°ln√≠mi tokeny ne≈æ p≈ôedchoz√≠ verze
- Zpracov√°n√≠ r≈Øznorod√Ωch dat vƒçetnƒõ textu, obrazu a videa

‚öñÔ∏è **V√Ωrazn√© zlep≈°en√≠ v oblasti vyv√°≈æen√≠ a bezpeƒçnosti:**
- Sn√≠≈æen√≠ m√≠ry odm√≠tnut√≠ odpovƒõd√≠ na kontroverzn√≠ t√©mata ze 7% na m√©nƒõ ne≈æ 2%
- Dosa≈æen√≠ politick√© vyv√°≈æenosti srovnateln√© s modelem Grok, s v√Ωraznƒõ men≈°√≠m poƒçtem nevyv√°≈æen√Ωch odpovƒõd√≠
- Open-source bezpeƒçnostn√≠ n√°stroje vƒçetnƒõ Llama Guard, Prompt Guard a CyberSecEval
- V√Ωvoj√°≈ôi mohou integrovat ochrann√© prvky proti potenci√°lnƒõ ≈°kodliv√Ωm vstup≈Øm a v√Ωstup≈Øm

üî• **V√Ωkonnostn√≠ p≈ôednosti:**
- Llama 4 Scout p≈ôekon√°v√° Gemma 3, Gemini 2.0 Flash-Lite a Mistral 3.1
- Llama 4 Maverick p≈ôedƒç√≠ GPT-4o a Gemini 2.0 Flash v ≈ôadƒõ benchmark≈Ø
- Srovnateln√© v√Ωsledky s [DeepSeek](/item/deepseek/) v3 v oblasti uva≈æov√°n√≠ a k√≥dov√°n√≠ - s polovinou aktivn√≠ch parametr≈Ø
- Bezkonkurenƒçn√≠ pomƒõr v√Ωkonu a n√°klad≈Ø, chatovac√≠ verze sk√≥ruje 1417 ELO na LMArena

![Llama 4 a v√Ωkonnostn√≠ benchmarky](https://www.marigold.cz/assets/llama4-vykon.png)

V√Ωborn√© jsou tak√© cenov√© parametry pro p≈ô√≠pad, ≈æe chcete pou≈æ√≠t Llama 4 p≈ôes API a nechcete ji instalovat na vlastn√≠ servery:

![Cenov√© parametry Llama 4](https://www.marigold.cz/assets/llama4-ceny-parametry.jpg)

Mus√≠m ≈ô√≠ct, ≈æe je to velmi p≈ô√≠jemn√© p≈ôekvapen√≠. I kdy≈æ jsou to zat√≠m jen pap√≠rov√° data a osobn√≠ zku≈°enost chyb√≠, vypad√° to velmi slibnƒõ a Meta jistƒõ nebude slibovat nƒõco, co alespo≈à p≈ôibli≈ænƒõ nen√≠ pravda. Na vƒõt≈°√≠ testov√°n√≠ si mus√≠m poƒçkat na zaƒç√°tek t√Ωdne, a≈æ si trochu uvoln√≠m m√≠sto na serverech :)

[V√≠ce informac√≠ o modelech Llama 4 vƒçetnƒõ detail≈Ø o tr√©ninku a benchmarc√≠ch.](https://go.fb.me/gmjohs).

‚¨áÔ∏è [St√°hnout Llama 4 m≈Ø≈æete zde](https://go.fb.me/bwwhe9).

## Architektura iRoPE ƒçili jak se Meta dostala k desetimilionov√©mu kontextu. 

Deset milion≈Ø token≈Ø nen√≠ v≈Øbec maliƒçkost, to je mimo jin√© t≈ôeba 20 hodin videa, kter√© si m≈Ø≈æe Llama 4 Scout nacpat do pamƒõti. Za t√≠mto pr≈Ølomem stoj√≠ architektura iRoPE. 

Architektura iRoPE, kterou vyvinul t√Ωm Meta pro modely Llama 4, p≈ôedstavuje inovativn√≠ p≈ô√≠stup k ≈ôe≈°en√≠ jednoho z nejvƒõt≈°√≠ch probl√©m≈Ø souƒçasn√Ωch jazykov√Ωch model≈Ø - efektivn√≠ pr√°ce s extr√©mnƒõ dlouh√Ωm kontextem. N√°zev iRoPE znamen√° _"interleaved Rotary Position Embedding"_, tedy prokl√°dan√© rotaƒçn√≠ poziƒçn√≠ k√≥dov√°n√≠.

Tradiƒçn√≠ transformerov√© architektury maj√≠ probl√©m se zpracov√°n√≠m velmi dlouh√Ωch text≈Ø ze dvou d≈Øvod≈Ø. Za prv√©, standardn√≠ attention mechanismus m√° kvadratickou slo≈æitost, co≈æ znamen√°, ≈æe pamƒõ≈•ov√© a v√Ωpoƒçetn√≠ n√°roky dramaticky rostou s d√©lkou vstupu. Za druh√©, poziƒçn√≠ k√≥dov√°n√≠, kter√© umo≈æ≈àuje modelu rozli≈°ovat po≈ôad√≠ slov, se obt√≠≈ænƒõ extrapoluje na d√©lky v√Ωraznƒõ p≈ôesahuj√≠c√≠ tr√©novac√≠ data.

Architektura iRoPE elegantnƒõ ≈ôe≈°√≠ tyto probl√©my kombinac√≠ dvou typ≈Ø pozornostn√≠ch vrstev, kter√© se v modelu vz√°jemnƒõ prokl√°daj√≠ (odtud "interleaved" v n√°zvu). 

*Prvn√≠ typ tvo≈ô√≠ lok√°ln√≠ vrstvy*, kter√© pou≈æ√≠vaj√≠ tradiƒçn√≠ rotaƒçn√≠ poziƒçn√≠ k√≥dov√°n√≠ (RoPE). Tyto vrstvy zpracov√°vaj√≠ pouze kr√°tk√© √∫seky textu, typicky do 8K token≈Ø, a jsou zodpovƒõdn√© za zachycen√≠ jemn√Ωch m√≠stn√≠ch souvislost√≠ a jazykov√Ωch vzor≈Ø. Kl√≠ƒçovou optimalizac√≠ je, ≈æe text rozdƒõluj√≠ na men≈°√≠ ƒç√°sti, kter√© zpracov√°vaj√≠ paralelnƒõ, co≈æ v√Ωraznƒõ zvy≈°uje efektivitu.

*Druh√Ω typ p≈ôedstavuj√≠ glob√°ln√≠ vrstvy*, kter√© na rozd√≠l od lok√°ln√≠ch vrstev zpracov√°vaj√≠ cel√Ω dlouh√Ω kontext bez pou≈æit√≠ poziƒçn√≠ch embeding≈Ø. To je revoluƒçn√≠ my≈°lenka - tyto vrstvy se nesna≈æ√≠ rozli≈°ovat konkr√©tn√≠ pozice, ale soust≈ôed√≠ se na s√©mantick√© vztahy mezi r≈Øzn√Ωmi ƒç√°stmi textu. T√≠m, ≈æe se model nemus√≠ spol√©hat na poziƒçn√≠ informace, dok√°≈æe l√©pe generalizovat na d√©lky daleko p≈ôesahuj√≠c√≠ tr√©novac√≠ data.

S√≠la architektury spoƒç√≠v√° pr√°vƒõ v prokl√°d√°n√≠ tƒõchto dvou typ≈Ø vrstev. Lok√°ln√≠ vrstvy poskytuj√≠ p≈ôesn√© modelov√°n√≠ bl√≠zk√Ωch vztah≈Ø, zat√≠mco glob√°ln√≠ vrstvy umo≈æ≈àuj√≠ modelu "vidƒõt" a propojovat vzd√°len√© ƒç√°sti kontextu. Takto dok√°≈æe model efektivnƒõ pracovat s kontextem o d√©lce 10 milion≈Ø token≈Ø, i kdy≈æ byl tr√©nov√°n na mnohem krat≈°√≠ch sekvenc√≠ch (maxim√°lnƒõ 256K token≈Ø).

Dal≈°√≠m d≈Øle≈æit√Ωm aspektem je ≈ôe≈°en√≠ probl√©mu "zplo≈°tƒõn√≠" pozornosti. S rostouc√≠ d√©lkou kontextu toti≈æ mechanismus pozornosti p≈ôirozenƒõ ztr√°c√≠ schopnost zamƒõ≈ôit se na d≈Øle≈æit√© informace - pozornost se "rozptyluje" nap≈ô√≠ƒç mnoha tokeny. T√Ωm Meta vyvinul speci√°ln√≠ techniku teplotn√≠ho ≈°k√°lov√°n√≠, kterou aplikuj√≠ pouze bƒõhem inference a pouze na glob√°ln√≠ vrstvy. Tato technika pom√°h√° modelu udr≈æet "ostrou" pozornost i p≈ôi pr√°ci s extr√©mnƒõ dlouh√Ωmi kontexty, ani≈æ by to negativnƒõ ovlivnilo jeho v√Ωkon na kr√°tk√Ωch textech.

Kl√≠ƒçov√Ωm vhledem cel√©ho p≈ô√≠stupu je zmƒõna perspektivy - m√≠sto snahy tr√©novat model p≈ô√≠mo na velmi dlouh√Ωch sekvenc√≠ch (co≈æ by bylo extr√©mnƒõ n√°roƒçn√© na v√Ωpoƒçetn√≠ zdroje), t√Ωm Meta p≈ôeformuloval probl√©m jako dosa≈æen√≠ "nekoneƒçn√©ho kontextu". To vedlo k n√°vrhu architektury, kter√° dok√°≈æe elegantnƒõ extrapolovat z kr√°tk√Ωch tr√©novac√≠ch sekvenc√≠ na mnohem del≈°√≠ vstupy p≈ôi re√°ln√©m pou≈æit√≠. Tento p≈ô√≠stup je nejen praktiƒçtƒõj≈°√≠ z hlediska tr√©ninku, ale tak√© l√©pe ≈°k√°luje smƒõrem k budouc√≠m model≈Øm s je≈°tƒõ del≈°√≠m kontextem.

A tady si to je≈°tƒõ pro jistotu uk√°≈æeme v grafu:

```mermaid
flowchart TD
    subgraph Architecture["Architektura iRoPE"]
        Input[/"Vstupn√≠ sekvence\n(a≈æ 10M token≈Ø)"/] --> Split["Rozdƒõlen√≠ zpracov√°n√≠"]
        
        Split --> LocalLayers["Lok√°ln√≠ vrstvy s RoPE"]
        Split --> GlobalLayers["Glob√°ln√≠ vrstvy bez poziƒçn√≠ch embeding≈Ø"]
        
        subgraph LocalProcessing["Zpracov√°n√≠ lok√°ln√≠ch vrstev"]
            LocalLayers --> Chunking["Rozdƒõlen√≠ na chunky\n(typicky 8K token≈Ø)"]
            Chunking --> ParallelProc["Paraleln√≠ zpracov√°n√≠ chunk≈Ø"]
            ParallelProc --> LocalPatterns["Zachycen√≠ lok√°ln√≠ch vzor≈Ø\na z√°vislost√≠"]
        end
        
        subgraph GlobalProcessing["Zpracov√°n√≠ glob√°ln√≠ch vrstev"]
            GlobalLayers --> FullContext["Zpracov√°n√≠ cel√©ho kontextu"]
            FullContext --> SemanticRel["Zachycen√≠ s√©mantick√Ωch vztah≈Ø\nbez ohledu na pozici"]
            SemanticRel --> TempScaling["Teplotn√≠ ≈°k√°lov√°n√≠ p≈ôi inferenci\n(kompenzace zplo≈°tƒõn√≠ pozornosti)"]
        end
        
        LocalPatterns --> Interleaving["Prokl√°d√°n√≠ (Interleaving)\nlok√°ln√≠ch a glob√°ln√≠ch reprezentac√≠"]
        TempScaling --> Interleaving
        
        Interleaving --> Output[/"V√Ωstupn√≠ reprezentace\ns efektivn√≠m dlouh√Ωm kontextem"/]
    end
    
    style Architecture fill:#f5f5f5,stroke:#333,stroke-width:1px
    style LocalProcessing fill:#e1f5fe,stroke:#0288d1,stroke-width:1px
    style GlobalProcessing fill:#e8f5e9,stroke:#388e3c,stroke-width:1px
    
    style Input fill:#bbdefb,stroke:#1976d2,stroke-width:1px
    style Output fill:#c8e6c9,stroke:#43a047,stroke-width:1px
    style Split fill:#fff9c4,stroke:#fbc02d,stroke-width:1px
    style Interleaving fill:#f8bbd0,stroke:#e91e63,stroke-width:1px
```    

# Omezen√≠ nov√© licence Llama 4

Nov√° licence Llama 4 p≈ôich√°z√≠ s nƒõkolika omezen√≠mi:

- Spoleƒçnosti s v√≠ce ne≈æ 700 miliony mƒõs√≠ƒçnƒõ aktivn√≠ch u≈æivatel≈Ø mus√≠ po≈æ√°dat o speci√°ln√≠ licenci od spoleƒçnosti Meta, kterou Meta m≈Ø≈æe udƒõlit nebo odm√≠tnout dle sv√©ho v√Ωhradn√≠ho uv√°≈æen√≠.
- Mus√≠te viditelnƒõ zobrazit "Built with Llama" (Vytvo≈ôeno s Llama) na webov√Ωch str√°nk√°ch, rozhran√≠ch, dokumentaci atd.
- Jak√Ωkoliv AI model, kter√Ω vytvo≈ô√≠te s pou≈æit√≠m modelu Llama, mus√≠ obsahovat "Llama" na zaƒç√°tku sv√©ho n√°zvu.
- Mus√≠te zahrnout specifick√© ozn√°men√≠ o autorsk√Ωch pr√°vech v textov√©m souboru "Notice" p≈ôi jak√©koliv distribuci.
- Va≈°e pou≈æ√≠v√°n√≠ mus√≠ b√Ωt v souladu se samostatn√Ωmi [Z√°sadami p≈ôijateln√©ho pou≈æit√≠ spoleƒçnosti Meta](http://llama.com/llama4/use-policy).
- Omezen√° licence k pou≈æ√≠v√°n√≠ n√°zvu "Llama" pouze pro √∫ƒçely splnƒõn√≠ po≈æadavk≈Ø na oznaƒçen√≠.