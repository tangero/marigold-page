---
author: Patrick Zandl
categories:
- AI
- Moonshot AI
- Kimi K2
- DeepSeek
layout: post
post_excerpt: ÄŒÃ­na mÃ¡ bÃ½t hony daleko za USA v AI, jenÅ¾e vÅ¯bec. Na pÅ™elomu roku ten dojem rozbil DeepSeek, nynÃ­ pÅ™ichÃ¡zÃ­ Kimi K2, kterÃ½ se jako open-source model mÃ¡ vyrovnat nejsilnÄ›jÅ¡Ã­m americkÃ½m modelÅ¯m. PouÅ¾il na to nÄ›kolik chytrÃ½ch trikÅ¯...
summary_points:
- Kimi K2 mÃ¡ bilion parametrÅ¯ s 32 miliardami aktivnÃ­ch, pouÅ¾Ã­vÃ¡ MoE architekturu
- VÃ½konnost 97,4% na MATH-500 pÅ™ekonÃ¡vÃ¡ GPT-4.1 s 92,4%
- CenovÃ¡ politika 0,15 dolaru za milion vstupnÃ­ch tokenÅ¯ je o 30% levnÄ›jÅ¡Ã­ neÅ¾ Gemini 2.5 Flash
- Architektura pÅ™Ã­mo vychÃ¡zÃ­ z open-source DeepSeek V3 se ÄtyÅ™mi klÃ­ÄovÃ½mi Ãºpravami
- TrÃ©novÃ¡nÃ­ probÄ›hlo na 15,5 trilionech tokenÅ¯ s nÃ¡klady kolem 5 milionÅ¯ dolarÅ¯
- Model zamÄ›Å™enÃ½ na automatizovanÃ© Ãºkoly bez chain-of-thought uvaÅ¾ovÃ¡nÃ­
thumbnail: https://www.geeky-gadgets.com/wp-content/uploads/2025/07/kimi-k2-ai-model-review_optimized.jpg
title: "ğŸ‡¨ğŸ‡³ Je Kimi K2 dalÅ¡Ã­ ÄÃ­nskÃ½ prÅ¯lom v AI?"
---

ÄŒÃ­na mÃ¡ bÃ½t hony daleko za USA v AI, jenÅ¾e vÅ¯bec. Na pÅ™elomu roku ten dojem rozbil DeepSeek, nynÃ­ pÅ™ichÃ¡zÃ­ Kimi K2, kterÃ½ se jako open-source model mÃ¡ vyrovnat nejsilnÄ›jÅ¡Ã­m americkÃ½m modelÅ¯m. PouÅ¾il na to nÄ›kolik chytrÃ½ch trikÅ¯...

ÄŒÃ­nskÃ½ startup Moonshot AI vydal 11. Äervence 2025 open-source model Kimi K2, kterÃ½ mÃ¡ pÅ™edstavovat dalÅ¡Ã­ milnÃ­k v rozvoji ÄÃ­nskÃ½ch jazykovÃ½ch modelÅ¯. S bilionem celkovÃ½ch parametrÅ¯ se jednÃ¡ o dosud nejvÄ›tÅ¡Ã­ veÅ™ejnÄ› dostupnÃ½ open-source model, kterÃ½ podle testÅ¯ dosahuje vÃ½konnosti srovnatelnÃ© s komerÄnÃ­mi Å™eÅ¡enÃ­mi od OpenAI a Anthropic.

HlavnÃ­ technologickÃ½ prÅ¯lom tkvÃ­ v optimalizÃ¡toru MuonClip. Ten jako prvnÃ­ umoÅ¾nil stabilnÃ­ trÃ©novÃ¡nÃ­ obÅ™Ã­ho modelu MoE (mixture-of-experts) na omezenÃ½ch Äipech Nvidia H800 bez jedinÃ©ho vÃ½padku. VÃ½sledkem je kÅ™ivka ztrÃ¡t, kterou vÃ½zkumnÃ­ci nazÃ½vajÃ­ â€nejkrÃ¡snÄ›jÅ¡Ã­ v historii strojovÃ©ho uÄenÃ­â€œ[^1].

### TechnickÃ¡ architektura a specifikace

Kimi K2 vyuÅ¾Ã­vÃ¡ architekturu Mixture-of-Experts (MoE) s celkem 384 experty, z nichÅ¾ se pro kaÅ¾dÃ½ dotaz aktivuje osm, coÅ¾ pÅ™edstavuje zhruba 32 miliard parametrÅ¯. Tento pÅ™Ã­stup umoÅ¾Åˆuje zpracovÃ¡nÃ­ rozsÃ¡hlÃ½ch Ãºloh pÅ™i zachovÃ¡nÃ­ vÃ½poÄetnÃ­ efektivity - aktivuje se pouze potÅ™ebnÃ¡ ÄÃ¡st modelu namÃ­sto celÃ© struktury - zapojujÃ­ se do Å™eÅ¡enÃ­ problÃ©mu pouze ti experti, kteÅ™Ã­ k tomu majÃ­ co Å™Ã­ct... 

| Parametr                  | Kimi K2          | DeepSeek V3     | GPT-4.1          |
| ------------------------- | ---------------- | --------------- | ---------------- |
| CelkovÃ½ poÄet vah         | 1,0 bilionu      | 685 miliard     | neuveÅ™ejnÄ›no     |
| AktivnÃ­ch vah na dotaz    | 32 miliardy      | 37 miliardy     | ~220 miliard     |
| TrÃ©novacÃ­ tokeny          | 15,5 bilionu     | 14,8 bilionu    | odhad 13 bilionÅ¯ |
| Optimizer (optimalizÃ¡tor) | MuonClip 2. Å™Ã¡du | AdamW           | neuveÅ™ejnÄ›no     |
| OdhadovanÃ© nÃ¡klady        | ~120 milionÅ¯ KÄ  | ~140 milionÅ¯ KÄ | ~2,3 miliardy KÄ |


Model pÅ™Ã­mo navazuje na open-source architekturu DeepSeek V3, kterou tÃ½m Moonshot upravil ve ÄtyÅ™ech klÃ­ÄovÃ½ch bodech. PoÄet expertÅ¯ zvÃ½Å¡ili z pÅ¯vodnÃ­ch 256 na 384 po zjiÅ¡tÄ›nÃ­, Å¾e scaling laws platÃ­ i pro Å™Ã­dkost _(sparsity)_. SouÄasnÄ› snÃ­Å¾ili poÄet "hlav pozornosti" _(attention heads)_ pro kompenzaci vyÅ¡Å¡Ã­ho poÄtu expertÅ¯. Pouze prvnÃ­ vrstvu ponechali jako hustou _(dense)_, zatÃ­mco vÅ¡echny ostatnÃ­ pouÅ¾Ã­vajÃ­ MoE pro maximalizaci efektivity. VÅ¡echny experty umÃ­stili do jednÃ© skupiny namÃ­sto rozdÄ›lenÃ­ do vÃ­ce skupin.

> TermÃ­n attention heads (Äesky â€hlavy pozornostiâ€œ) pochÃ¡zÃ­ z mechanismu self-attention, kterÃ½ je klÃ­Äovou souÄÃ¡stÃ­ architektury transofmÃ¡torÅ¯, na nÃ­Å¾ jsou postaveny dneÅ¡nÃ­ jazykovÃ© modely jako Kimiâ€¯K2, GPT nebo DeepSeek. Jedna attention head je samostatnÃ¡ vÃ½poÄetnÃ­ vÄ›tev v mechanismu self-attention, kterÃ¡:
> - analyzuje vztahy mezi vÅ¡emi tokeny v sekvenci (napÅ™. slovy ve vÄ›tÄ›),
> - mÃ¡ vlastnÃ­ vÃ¡hy (matice pro klÃ­Äe, dotazy, hodnoty: Q, K, V),
> - se zamÄ›Å™uje (attention) na jinÃ© aspekty vstupu neÅ¾ ostatnÃ­ hlavy.
> VÃ­ce attention heads umoÅ¾Åˆuje modelu â€dÃ­vat seâ€œ na vstup z rÅ¯znÃ½ch perspektiv souÄasnÄ›:
> - jedna hlava mÅ¯Å¾e detekovat syntaktickÃ© vazby,
> - jinÃ¡ mÅ¯Å¾e sledovat vÃ½znamovÃ© souvislosti,
> - dalÅ¡Ã­ mÅ¯Å¾e sledovat strukturu vÄ›t nebo zÃ¡vislosti napÅ™Ã­Ä vÄ›tami.
> V klasickÃ© architektuÅ™e transfomÃ¡torÅ¯ (napÅ™. BERT, GPT-2) je bÄ›Å¾nÃ© mÃ­t 12â€“96 attention heads, podle velikosti modelu.

### PrÅ¯lom v optimalizaci trÃ©ninku

KlÃ­Äovou technickou inovacÃ­ Kimi K2 je pouÅ¾itÃ­ [optimalizÃ©ru MuonClip](https://app.studyraid.com/en/read/30083/1291932/muonclip-optimizer-for-training-stability) mÃ­sto [standardnÃ­ho AdamW](https://optimization.cbe.cornell.edu/index.php?title=AdamW). MuonClip pÅ™idÃ¡vÃ¡ druhou ÃºroveÅˆ pozorovÃ¡nÃ­, kterÃ© sleduje nejen jak se model uÄÃ­ prostÅ™ednictvÃ­m gradientÅ¯, ale takÃ© jak se tyto gradienty mÄ›nÃ­. Tato metoda umoÅ¾Åˆuje ostÅ™ejÅ¡Ã­ a stabilnÄ›jÅ¡Ã­ aktualizace bÄ›hem trÃ©ninku.

DruhÃ½m prvkem je mechanismus QK-clipping v pozornostnÃ­ vrstvÄ›, kterÃ½ funguje jako pojistka proti destabilizaci systÃ©mu. KdyÅ¾ model kalkuluje vztahy mezi slovy nÃ¡sobenÃ­m 'query' a 'key' vah, QK-clipping omezuje tyto hodnoty pÅ™ed jejich pÅ™Ã­padnou spirÃ¡lou mimo kontrolu.

VÃ½sledkem je trÃ©novÃ¡nÃ­ na 15,5 trilionech tokenÅ¯ - zhruba padesÃ¡tkrÃ¡t vÃ­ce neÅ¾ GPT-3 - bez jedinÃ©ho loss spike, katastrofickÃ©ho pÃ¡du nebo restartu. Stabilita trÃ©ninku vÃ½znamnÄ› snÃ­Å¾ila nÃ¡klady na pÅ™ibliÅ¾nÄ› 5 milionÅ¯ dolarÅ¯, coÅ¾ je zlomek bÄ›Å¾nÃ½ch nÃ¡kladÅ¯ na modely tÃ©to velikosti.

## VÃ½konnostnÃ­ parametry a srovnÃ¡nÃ­

V benchmark testech Kimi K2 dosahuje 97,4% ÃºspÄ›Å¡nosti na MATH-500, zatÃ­mco GPT-4.1 dosahuje 92,4%. Na LiveCodeBench dosahuje 53,7% oproti 44,7% u GPT-4.1. V oblasti automatizovanÃ©ho programovÃ¡nÃ­ (SWE-bench Verified) dosahuje 65,8% a na MultiPL-E coding tasks 85,7%.

PraktickÃ© testovÃ¡nÃ­ ukÃ¡zalo schopnosti srovnatelnÃ© s Claude 4. PÅ™i vytvÃ¡Å™enÃ­ kompletnÃ­ PDF chat aplikace s FastAPI backendem a JavaScript frontendem Kimi K2 dodal funkÄnÃ­ Å™eÅ¡enÃ­, pÅ™estoÅ¾e trvalo dvakrÃ¡t dÃ©le neÅ¾ u Claude. VÃ½slednÃ¡ aplikace byla bez chyb a nevyÅ¾adovala ladÄ›nÃ­.

Model vynikÃ¡ v kreativnÃ­m psanÃ­, kde podle benchmarkÅ¯ dosahuje nejlepÅ¡Ã­ch vÃ½sledkÅ¯ mezi souÄasnÃ½mi modely. Na rozdÃ­l od reasoning modelÅ¯ negeneruje dlouhÃ© Å™etÄ›zce uvaÅ¾ovÃ¡nÃ­ pÅ™ed odpovÄ›dÃ­, ale byl rozsÃ¡hle trÃ©novÃ¡n pomocÃ­ posilujÃ­cÃ­ho uÄenÃ­.

A ÄeÅ¡tina? Provedl jsem pÃ¡r pokusÅ¯, musÃ­m Å™Ã­ct, Å¾e vÃ½sledky jsou hodnÄ› impresivnÃ­. NapÅ™Ã­klad jsem pouÅ¾il svÅ¯j prompt "patrikizÃ¡tor", kterÃ½ pouÅ¾Ã­vÃ¡m na ladÄ›nÃ­ textÅ¯. Jeho smyslem je vytahat technickÃ© informace a udÄ›lat z nich ÄitelnÃ½ text, coÅ¾ se mu o sobÄ› samÃ©m (Kimi K2) podaÅ™ilo lÃ©pe, neÅ¾ se daÅ™Ã­ Claude Sonet 4. 


## CenovÃ¡ strategie a dostupnost

Moonshot AI stanovil ceny na 0,15 dolaru za milion vstupnÃ­ch tokenÅ¯ a 2,50 dolaru za milion vÃ½stupnÃ­ch tokenÅ¯. To pÅ™edstavuje 30% Ãºsporu oproti Gemini 2.5 Flash a Å™Ã¡dovÃ© snÃ­Å¾enÃ­ nÃ¡kladÅ¯ oproti Claude 4 Opus (15/75 dolarÅ¯) nebo GPT-4.1 (2/8 dolarÅ¯).

VyÅ¡Å¡Ã­ verbÃ³znost modelu vÅ¡ak zvyÅ¡uje praktickÃ© nÃ¡klady na provoz. Podle analÃ½zy Artificial Analysis pouÅ¾Ã­vÃ¡ Kimi K2 aÅ¾ o 30% mÃ©nÄ› tokenÅ¯ neÅ¾ Claude 4 Sonnet v maximÃ¡lnÃ­m reÅ¾imu, ale tÃ©mÄ›Å™ trojnÃ¡sobek pÅ™i vypnutÃ©m uvaÅ¾ovÃ¡nÃ­.

## ObchodnÃ­ pozadÃ­ Moonshot AI

SpoleÄnost zaloÅ¾il Yang Zhilin, absolvent Tsinghua University s PhD z Carnegie Mellon, kterÃ½ pracoval v Meta AI a Google Brain. Na rozdÃ­l od DeepSeek, kterÃ½ financuje hedge fund High Flyer, Moonshot operuje s tradiÄnÃ­m VC financovÃ¡nÃ­m v hodnotÄ› 1 miliardy dolarÅ¯ s Alibaba jako nejvÄ›tÅ¡Ã­m investorem. MusÃ­ tedy rychleji ukÃ¡zat zisk. Jejich recept je pÅ™ekvapivÄ› prozaickÃ½: nabÃ­dnout vÃ½kon drahÃ½ch modelÅ¯ za desetinu ceny.


Moonshot se zamÄ›Å™uje vÃ½hradnÄ› na individuÃ¡lnÃ­ uÅ¾ivatele prostÅ™ednictvÃ­m [chatbota Kimi](https://www.kimi.com/), kterÃ½ pouÅ¾Ã­vÃ¡ pÅ™es 100 milionÅ¯ ÄÃ­nskÃ½ch uÅ¾ivatelÅ¯. 

PÅ™i pohledu [na cenÃ­k API](https://platform.moonshot.ai/docs/pricing/chat) je rozdÃ­l markantnÃ­. Kimi K2 ÃºÄtuje 3,50 KÄ za milion vstupnÃ­ch tokenÅ¯ a 58 KÄ za milion vÃ½stupnÃ­ch. Claude 3.5 Sonnet pÅ™itom Å¾Ã¡dÃ¡ 29 KÄ/290 KÄ. Pro firmy, kterÃ© dennÄ› zpracovÃ¡vajÃ­ miliony tokenÅ¯, jde o Ãºsporu ve statisÃ­cÃ­ch mÄ›sÃ­ÄnÄ›.

**Jak mÄ›nÃ­ ekosystÃ©m?**

PrvnÃ­ tÃ½den po vydÃ¡nÃ­ se Kimi K2 vyÅ¡plhalo na prvnÃ­ mÃ­sto v Å¾ebÅ™Ã­Äku vyuÅ¾itÃ­ na platformÄ› OpenRouter, pÅ™ed Grok 4 a Llama 3.1. Za tÃ­m stojÃ­ kombinace tÅ™Ã­ faktorÅ¯:
- KomerÄnÃ­ licence Apache 2.0 umoÅ¾Åˆuje firmÃ¡m model pÅ™Ã­mo hostovat ve vlastnÃ­ infrastruktuÅ™e bez obav z prÃ¡vnÃ­ch postihÅ¯.
- Velikost aktivnÃ­ch vah 32 miliard znamenÃ¡, Å¾e se vejde na ÄtyÅ™i A100 80 GB â€“ konfigurace dostupnÃ¡ i menÅ¡Ã­m ÄeskÃ½m firmÃ¡m.
- ZamÄ›Å™enÃ­ na agentnÃ­ Ãºlohy â€“ model je trÃ©novanÃ½ pÅ™edevÅ¡Ã­m na volÃ¡nÃ­ funkcÃ­, prÃ¡ci s nÃ¡stroji a kÃ³dovÃ¡nÃ­, coÅ¾ jsou oblasti, kde se dÃ¡ snadno mÄ›Å™it ROI.

Pokud Kimi K2 pÅ™idÃ¡ v pÅ™Ã­Å¡tÃ­ verzi Å™etÄ›zovÃ© myÅ¡lenÃ­ (chain-of-thought) a vyladÃ­ politickou cenzuru (coÅ¾ Å™eÅ¡Ã­ napÅ™Ã­klad Perplexity svou Ãºpravou), mÅ¯Å¾e se stÃ¡t prvnÃ­m open-source modelem, kterÃ½ opravdu nahradÃ­ Claude Sonnet ve firmÃ¡ch. ZvlÃ¡Å¡Å¥ kdyÅ¾ jeho provoznÃ­ nÃ¡klady jsou srovnatelnÃ© s provozem internÃ­ho systÃ©mu, nikoli s placenÃ­m dolarÅ¯ za kaÅ¾dÃ½ dotaz.

Silicon Valley zatÃ­m reaguje zdrÅ¾enlivÄ›. OpenAI stÃ¡le odklÃ¡dÃ¡ vydÃ¡nÃ­ vlastnÃ­ho open-source modelu a Anthropic drÅ¾Ã­ ceny vysoko. MezitÃ­m ÄÃ­nskÃ© laboratoÅ™e dokazujÃ­, Å¾e se dÃ¡ jet i rychleji a levnÄ›ji. Kimi K2 nenÃ­ jen technologickÃ½ skok, ale varovnÃ½ signÃ¡l: algoritmickÃ© inovace zaÄÃ­najÃ­ pÅ™evÃ¡Å¾ovat nad ÄistÃ½m vÃ½konem hardwaru.


### Vztah k DeepSeek a open-source kultuÅ™e

Kimi K2 pÅ™Ã­mo stavÃ­ na architektuÅ™e DeepSeek V3. InÅ¾enÃ½r Liu Shaowei z Moonshot na platformÄ› Zhihu vysvÄ›tlil, Å¾e pÅ™ed zahÃ¡jenÃ­m trÃ©ninku K2 provedli rozsÃ¡hlÃ© experimenty s architekturou modelÅ¯, ale Å¾Ã¡dnÃ¡ navrÅ¾enÃ¡ architektura nedokÃ¡zala pÅ™ekonat DeepSeek V3.

"DÅ¯vod je jednoduchÃ½: architektura V3 byla validovÃ¡na a zÅ¯stÃ¡vÃ¡ efektivnÃ­ ve velkÃ©m mÄ›Å™Ã­tku, zatÃ­mco naÅ¡e 'novÃ© architektury' jeÅ¡tÄ› neproÅ¡ly dostateÄnou large-scale validacÃ­," uvedl Liu.

Tento pÅ™Ã­stup ilustruje sÃ­lu open-source kultury v ÄÃ­nskÃ©m AI vÃ½zkumu. Jak poznamenal inÅ¾enÃ½r Su Jianlin: "Rozhodli jsme se vzdÃ¡t hold DeepSeek replikovÃ¡nÃ­m jejich MLA designu."

ÃšspÄ›ch DeepSeek zmÄ›nil postoj Yang Zhilina k open source. JeÅ¡tÄ› v Ãºnoru 2024 tvrdil, Å¾e open-source modely nemohou rychle dohnat closed-source Å™eÅ¡enÃ­. Po ÃºspÄ›chu DeepSeek vÅ¡ak Moonshot vydal K2 jako open-source.

### Technologie pro automatizovanÃ© Ãºkoly

Moonshot oznaÄuje K2 jako "open agentic intelligence" s dÅ¯razem na automatizovanÃ© schopnosti. Model byl trÃ©novÃ¡n na masivnÃ­ch objemech syntetickÃ½ch dat vytvoÅ™enÃ½ch speciÃ¡lnÄ› pro napodobenÃ­ real-life aplikacÃ­ a optimalizovÃ¡n pro tool-calling.

VÃ½vojovÃ½ proces zahrnoval generovÃ¡nÃ­ stovek scÃ©nÃ¡Å™Å¯ (food delivery, sociÃ¡lnÃ­ sÃ­tÄ›), tisÃ­cÅ¯ nÃ¡strojÅ¯ a stovek tisÃ­c rÅ¯znÃ½ch agentÅ¯ s rÅ¯znÃ½mi system prompty a sadami nÃ¡strojÅ¯. NÃ¡slednÄ› probÄ›hly large-scale simulace agentÅ¯ s hodnocenÃ­m podle kritÃ©riÃ­ ÃºspÄ›Å¡nosti.

Na rozdÃ­l od _chain-of-thought_ modelÅ¯, K2 nevyÅ¾aduje dlouhÃ© uvaÅ¾ovacÃ­ sekvence pÅ™ed odpovÄ›dÃ­, coÅ¾ ho ÄinÃ­ vhodnÃ½m pro praktickÃ© automatizovanÃ© Ãºkoly vyÅ¾adujÃ­cÃ­ rychlÃ© execution.

### Vliv na globÃ¡lnÃ­ AI ekosystÃ©m

VydÃ¡nÃ­ Kimi K2 potvrzuje, Å¾e ÄÃ­nskÃ© laboratoÅ™e mohou vytvÃ¡Å™et konkurenceschopnÃ© modely i rÅ¯znÃ½mi jinÃ½mi cestami neÅ¾ DeepSeek. Moonshot prokÃ¡zal, Å¾e i s tradiÄnÃ­m VC financovÃ¡nÃ­m a investorskÃ½mi tlaky lze dosÃ¡hnout Å¡piÄkovÃ½ch vÃ½sledkÅ¯.

ZÃ¡padnÃ­ laboratoÅ™e jako OpenAI, kterÃ© odloÅ¾ily vydÃ¡nÃ­ vlastnÃ­ch open-source modelÅ¯, ztrÃ¡cejÃ­ kontrolu nad narrativem v open-source oblasti. Perplexity jiÅ¾ oznÃ¡mila plÃ¡ny na post-training K2 pro svÃ© uÅ¾ivatele, coÅ¾ umoÅ¾nÃ­ necenzurovanou verzi modelu podobnÄ› jako u DeepSeek R1.

Model je v souladu s ÄÃ­nskÃ½mi zÃ¡kony, coÅ¾ znamenÃ¡ omezenÃ­ pÅ™i dotazech na citlivÃ¡ politickÃ¡ tÃ©mata. Tato omezenÃ­ vÅ¡ak platformy jako Perplexity obchÃ¡zejÃ­ vlastnÃ­m post-trainingem. Proto je dÅ¯leÅ¾itÃ©, Å¾e je fork Kimi K2 dostupnÃ½ pÅ™es americkÃ© firmy. Podle dashboardu Openrouteru mÃ¡ K2 provoz vyÅ¡Å¡Ã­, neÅ¾ nedÃ¡vno pÅ™edstavenÃ½ Grok 4. 

Kimi K2 pÅ™edstavuje dÅ¯kaz, Å¾e inovace v oblasti AI efektivity se pÅ™esunuly do Pekingu namÃ­sto Palo Alta. Dva vÃ½znamnÃ© algoritmickÃ© prÅ¯lomy - Group-Relative PPO od DeepSeek a MuonClip od Moonshot - publikovanÃ© pod permisivnÃ­mi licencemi bÄ›hem Å¡esti mÄ›sÃ­cÅ¯ posunuly tÄ›Å¾iÅ¡tÄ› vÃ½zkumu smÄ›rem k ÄŒÃ­nÄ›.

## TechnickÃ© odkazy a dostupnost

Model Kimi K2 je dostupnÃ½ prostÅ™ednictvÃ­m [Moonshot AI API](https://platform.moonshot.cn) a [Hugging Face](https://huggingface.co/moonshot-ai). TechnickÃ¡ dokumentace a implementaÄnÃ­ detaily jsou publikovÃ¡ny na [Moonshot AI blogu](https://www.moonshot.ai/blog) a [GitHubu](https://github.com/moonshot-ai).

KompletnÃ­ kÃ³dovÃ¡ zÃ¡kladna vÄetnÄ› MuonClip optimizÃ©ru bude uvolnÄ›na v nÃ¡sledujÃ­cÃ­ch tÃ½dnech podle oznÃ¡menÃ­ vÃ½vojovÃ©ho tÃ½mu na platformÄ› Zhihu.

[^1]: Jde o velmi hladkÃ½, monotÃ³nnÄ› klesajÃ­cÃ­ graf, kterÃ½ zobrazuje, jak se chyba modelu (ztrÃ¡tovÃ¡ funkce) bÄ›hem trÃ©novÃ¡nÃ­ neustÃ¡le zmenÅ¡uje â€“ bez vÃ½kyvÅ¯, stagnacÃ­ Äi nÃ¡hlÃ½ch skokÅ¯. TakovÃ¡ dokonale pravidelnÃ¡ kÅ™ivka je v praxi obÅ™Ã­ch jazykovÃ½ch modelÅ¯ vÃ½jimeÄnÃ¡; obvykle se grafy â€kousajÃ­â€œ a trÃ©nink musÃ­ restartovat. KdyÅ¾ se tohle nestane, pÅ¯sobÃ­ to na vÃ½zkumnÃ­ky esteticky a technicky â€“ odtud ta nadsÃ¡zka â€nejkrÃ¡snÄ›jÅ¡Ã­â€œ. _(toto vysvÄ›tlenÃ­ pro vÃ¡s napsal Kimi K2 a vysvÄ›tlil mi, jak dÃ¡t poznÃ¡mku pod Äarou v markdown formÃ¡tovÃ¡nÃ­)_