---
author: Patrick Zandl
audio: true
categories:
- AI
- DeepSeek
- open-source
- LLM
- Čína
layout: post
post_excerpt: Čínský startup DeepSeek vydal model V3.1 s 685 miliardami parametrů, který konkuruje americkým systémům za zlomek ceny a je dostupný jako open-source.
summary_points:
- DeepSeek-V3.1 má 685 miliard parametrů a kontextové okno 128 000 tokenů
- Model dosahuje výkonu srovnatelného s proprietárními systémy jako Claude Opus 4
- Cena za API je 0,56 USD za milion vstupních tokenů oproti 70 USD u konkurence
- Využívá hybridní architekturu kombinující běžné úlohy s logickým uvažováním
- Model je dostupný pod licencí MIT pro komerční využití
- Trénování předchozí verze stálo pouze 5,6 milionu USD
title: "🇨🇳 DeepSeek-V3.1 je příliš dobrý čínský model pro  umělou inteligenci"
thumbnail: https://www.marigold.cz/assets/Deepseek-V31.png
---

Čínská společnost DeepSeek, která se specializuje na vývoj pokročilých jazykových modelů umělé inteligence, právě uvedla na trh model DeepSeek-V3.1. Jde o masivní systém s 685 miliardami parametrů, který je k dispozici jako otevřený model pod licencí MIT. Model byl tiše publikován na platformě [Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3), kde rychle získal pozornost vývojářské komunity a dostal se mezi nejpopulárnější modely.

## Technické parametry a architektura

DeepSeek-V3.1 představuje významný technologický skok oproti své předchozí verzi. Model disponuje 685 miliardami parametrů, což představuje výrazný nárůst oproti předchůdci, a kontextovým oknem o velikosti 128 000 tokenů. To jej staví na úroveň jiných otevřených modelů jako OpenAI GPT-OSS nebo Google Gemma 3.

Základem modelu je architektura Mixture-of-Experts (MoE), která aktivuje pouze 37 miliard parametrů na jeden token. Tento přístup udržuje náklady na inferenci nízké navzdory celkové velikosti modelu. DeepSeek-V3.1 využívá hybridní design, který integruje standardní konverzační schopnosti, logické uvažování a funkce nástrojů do jediného modelu. Systém funguje standardně v běžném chatovacím režimu a přepíná se do režimu uvažování nebo používání nástrojů pomocí speciálních tokenů.

Model podporuje několik formátů přesnosti včetně BF16, F8_E4M3 a F32, což umožňuje vývojářům optimalizovat výkon podle jejich konkrétního hardwaru. Byl natrénován pro nativní podporu webového vyhledávání a programování, což rozšiřuje jeho praktické využití.

Tento přístup se liší od předchozí generace modelů DeepSeek, která vyžadovala samostatné modely pro běžné úlohy (DeepSeek-V2) a logické uvažování (DeepSeek-R1). Společnost DeepSeek zřejmě vyřešila technické výzvy, které dříve komplikovaly vytvoření funkčních hybridních modelů.

> Formáty přesnosti (anglicky “precision formats” nebo “floating-point formats”) určují, jak jsou v počítači reprezentována desetinná čísla a kolik bitů paměti zabírají. **F32** (Float32, také FP32) je standardní 32bitová přesnost s plovoucí desetinnou čárkou, která poskytuje vysokou přesnost výpočtů, ale vyžaduje více paměti a výpočetního výkonu. **BF16** (Brain Float 16) je 16bitový formát vyvinutý společností Google, který zachovává stejný rozsah hodnot jako F32 (8 bitů pro exponent), ale snižuje přesnost mantisy na 7 bitů, což výrazně zrychluje trénování neuronových sítí při minimální ztrátě přesnosti. **F8_E4M3** je 8bitový formát s plovoucí desetinnou čárkou, kde E4 znamená 4 bity pro exponent a M3 znamená 3 bity pro mantisu - jde o extrémně kompaktní formát, který umožňuje drastické snížení paměťových nároků a zrychlení inference, ale za cenu významného snížení numerické přesnosti, což je přijatelné pro mnoho úloh v již natrénovaných modelech.​​​​​​​​​​​​​​​​

## Výkon srovnatelný s nejlepšími proprietárními systémy

Podle prvních testů dosahuje DeepSeek-V3.1 na benchmarku SWE-bench skóre 71,6 procenta, čímž mírně překonává i proprietární modely jako Claude Opus 4. Model prokázal silné schopnosti logického uvažování při řešení komplexních problémů, například úlohy s míčkem poskakujícím uvnitř rotujícího šestiúhelníku. Matematické dovednosti modelu staví na úspěchu jeho předchůdce, který již překonával ostatní modely v benchmarcích jako AIME 2024 a MATH 500.

Nákladová efektivita modelu je pozoruhodná. Podle analýzy společnosti Venture Beat dokáže DeepSeek-V3.1 dokončit komplexní programovací úlohu za přibližně 1,01 USD, zatímco srovnatelné proprietární systémy vyžadují náklady blížící se 70 USD za ekvivalentní zátěž. Tato dramatická cenová výhoda vychází částečně z nízkých nákladů na trénování - předchozí verze DeepSeek-V3 stála pouhých 5,6 milionu USD za jeden tréninkový běh, což je výrazně méně než obdobné projekty amerických laboratoří.

## Dostupnost a cenová politika

Model je k dispozici prostřednictvím několika kanálů. Na platformě Hugging Face jsou dostupné jak standardní váhy modelu, tak hybridní verze. Pro přímé využití nabízí DeepSeek vlastní API s cenou 0,56 USD za milion vstupních tokenů a 2,19 USD za milion výstupních tokenů. Tato cenová struktura představuje zlomek nákladů konkurenčních služeb při zachování srovnatelného výkonu.

Využití modelu je možné také prostřednictvím webového rozhraní na [chat.deepseek.com](https://chat.deepseek.com), kde mohou uživatelé přepínat mezi standardním režimem a režimem uvažování pomocí funkce “DeepThink”.

## Praktické bariéry a geopolitické aspekty

Přestože je DeepSeek-V3.1 technicky otevřený model dostupný pod licencí MIT, jeho velikost přibližně 700 GB představuje významnou praktickou překážku. Samostatné hostování a přizpůsobení modelu vyžaduje rozsáhlé výpočetní zdroje a odborné znalosti, které většina organizací nemá k dispozici. Pro mnoho uživatelů bude hlavní výhodou “otevřenosti” modelu přístup k levnějším API prostřednictvím poskytovatelů cloudových služeb, nikoli schopnost provozovat model lokálně.

Americké podniky dost možná budou váhat s přijetím modelu kvůli geopolitickému napětí mezi Spojenými státy a Čínou. Preference domácích dodavatelů, kteří nabízejí integrované platformy s podnikovými zárukami podpory a bezpečnosti, může omezit adopci DeepSeek-V3.1 v západních firmách. Generální ředitel OpenAI Sam Altman nedávno v rozhovoru pro CNBC uvedl, že konkurence čínských open-source modelů včetně DeepSeek ovlivnila rozhodnutí jeho společnosti vydat vlastní otevřené modely.

## Dopad na trh s umělou inteligencí

Vydání DeepSeek-V3.1 bylo strategicky načasováno pouhé týdny po uvedení Claude 4.1 od společnosti Anthropic a GPT-5 od OpenAI. Dosažením srovnatelného výkonu s otevřeným modelem DeepSeek přímo zpochybňuje obchodní modely založené na vysokých nákladech a uzavřených systémech amerických konkurentů.

Globální vývojářská komunita reagovala okamžitě - V3.1 se rychle dostal na seznam trendujících projektů na Hugging Face s více než 80 000 sledujícími. Tato rychlá adopce ukazuje, že technický výkon je primárním faktorem přijetí bez ohledu na národní původ modelu.

Pokračující vydávání špičkových otevřených modelů může změnit dynamiku globálního závodu v umělé inteligenci. Přesouvá důraz z otázky, kdo dokáže vytvořit nejvýkonnější systém, na to, kdo dokáže tento systém učinit nejdostupnějším. S rostoucí velikostí modelů se však stává výraznější rozdíl mezi výkonným, přístupným nástrojem a skutečně přizpůsobitelným systémem. Budoucnost umělé inteligence bude definována rovnováhou mezi výkonem a praktickou použitelností.

Pro podnikové aplikace mohou být praktičtější malé modely, které lze snadno přizpůsobit prostřednictvím kurátorovaných datových sad nebo destilace, než systémy typu “jeden model pro všechno”. Současně roste význam malých jazykových modelů, které lze provozovat na široké škále spotřebitelského hardwaru a jednotlivých akcelerátorů.

Sám budu DeepSeek V3.1 testovat přes API přimo na čínském serveru, ještě uvidíme, jak kvalitní budou jeho odpovědi v češtině. Ale již v předchozích verzích byla podpora češtiny na velmi slušné úrovni...

Přečtěte si také článek [DeepSeek jako čínský Sputnik](https://www.marigold.cz/item/deepseek/) o významu geopolitického soupeření Číny a USA v oblasti AI. 