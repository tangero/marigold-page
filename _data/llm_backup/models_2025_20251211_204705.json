[
  {
    "id": "deepseek/deepseek-r1-distill-llama-70b",
    "canonical_slug": "deepseek/deepseek-r1-distill-llama-70b",
    "hugging_face_id": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
    "name": "DeepSeek: R1 Distill Llama 70B",
    "created": 1737663169,
    "description": "DeepSeek R1 Distill Llama 70B is a distilled large language model based on [Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). The model combines advanced distillation techniques to achieve high performance across multiple benchmarks, including:\n\n- AIME 2024 pass@1: 70.0\n- MATH-500 pass@1: 94.5\n- CodeForces Rating: 1633\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
    "context_length": 131072,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Llama3",
      "instruct_type": "deepseek-r1"
    },
    "pricing": {
      "prompt": "0.00000003",
      "completion": "0.00000013",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": 131072,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "logit_bias",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "deepseek/deepseek-r1-distill-qwen-14b",
    "canonical_slug": "deepseek/deepseek-r1-distill-qwen-14b",
    "hugging_face_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    "name": "DeepSeek: R1 Distill Qwen 14B",
    "created": 1738193940,
    "description": "DeepSeek R1 Distill Qwen 14B is a distilled large language model based on [Qwen 2.5 14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\nOther benchmark results include:\n\n- AIME 2024 pass@1: 69.7\n- MATH-500 pass@1: 93.9\n- CodeForces Rating: 1481\n\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
    "context_length": 32768,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Qwen",
      "instruct_type": "deepseek-r1"
    },
    "pricing": {
      "prompt": "0.00000012",
      "completion": "0.00000012",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 32768,
      "max_completion_tokens": 16384,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "max_tokens",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "top_k",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "deepseek/deepseek-r1-distill-qwen-32b",
    "canonical_slug": "deepseek/deepseek-r1-distill-qwen-32b",
    "hugging_face_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    "name": "DeepSeek: R1 Distill Qwen 32B",
    "created": 1738194830,
    "description": "DeepSeek R1 Distill Qwen 32B is a distilled large language model based on [Qwen 2.5 32B](https://huggingface.co/Qwen/Qwen2.5-32B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\n\\nOther benchmark results include:\\n\\n- AIME 2024 pass@1: 72.6\\n- MATH-500 pass@1: 94.3\\n- CodeForces Rating: 1691\\n\\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
    "context_length": 64000,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Qwen",
      "instruct_type": "deepseek-r1"
    },
    "pricing": {
      "prompt": "0.00000024",
      "completion": "0.00000024",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 64000,
      "max_completion_tokens": 32000,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "top_k",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "mistralai/mistral-small-24b-instruct-2501",
    "canonical_slug": "mistralai/mistral-small-24b-instruct-2501",
    "hugging_face_id": "mistralai/Mistral-Small-24B-Instruct-2501",
    "name": "Mistral: Mistral Small 3",
    "created": 1738255409,
    "description": "Mistral Small 3 is a 24B-parameter language model optimized for low-latency performance across common AI tasks. Released under the Apache 2.0 license, it features both pre-trained and instruction-tuned versions designed for efficient local deployment.\n\nThe model achieves 81% accuracy on the MMLU benchmark and performs competitively with larger models like Llama 3.3 70B and Qwen 32B, while operating at three times the speed on equivalent hardware. [Read the blog post about the model here.](https://mistral.ai/news/mistral-small-3/)",
    "context_length": 32768,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000003",
      "completion": "0.00000011",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 32768,
      "max_completion_tokens": 32768,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logit_bias",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.3
    }
  },
  {
    "id": "openai/o3-mini",
    "canonical_slug": "openai/o3-mini-2025-01-31",
    "hugging_face_id": "",
    "name": "OpenAI: o3 Mini",
    "created": 1738351721,
    "description": "OpenAI o3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding.\n\nThis model supports the `reasoning_effort` parameter, which can be set to \"high\", \"medium\", or \"low\" to control the thinking time of the model. The default is \"medium\". OpenRouter also offers the model slug `openai/o3-mini-high` to default the parameter to \"high\".\n\nThe model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities.\n\nThe model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.",
    "context_length": 200000,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000011",
      "completion": "0.0000044",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000055"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 100000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {}
  },
  {
    "id": "google/gemini-2.0-flash-001",
    "canonical_slug": "google/gemini-2.0-flash-001",
    "hugging_face_id": "",
    "name": "Google: Gemini 2.0 Flash",
    "created": 1738769413,
    "description": "Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5). It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.",
    "context_length": 1048576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image",
        "file",
        "audio",
        "video"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000001",
      "completion": "0.0000004",
      "request": "0",
      "image": "0.0000258",
      "audio": "0.0000007",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000025",
      "input_cache_write": "0.0000001833"
    },
    "top_provider": {
      "context_length": 1048576,
      "max_completion_tokens": 8192,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/o3-mini-high",
    "canonical_slug": "openai/o3-mini-high-2025-01-31",
    "hugging_face_id": "",
    "name": "OpenAI: o3 Mini High",
    "created": 1739372611,
    "description": "OpenAI o3-mini-high is the same model as [o3-mini](/openai/o3-mini) with reasoning_effort set to high. \n\no3-mini is a cost-efficient language model optimized for STEM reasoning tasks, particularly excelling in science, mathematics, and coding. The model features three adjustable reasoning effort levels and supports key developer capabilities including function calling, structured outputs, and streaming, though it does not include vision processing capabilities.\n\nThe model demonstrates significant improvements over its predecessor, with expert testers preferring its responses 56% of the time and noting a 39% reduction in major errors on complex questions. With medium reasoning effort settings, o3-mini matches the performance of the larger o1 model on challenging reasoning evaluations like AIME and GPQA, while maintaining lower latency and cost.",
    "context_length": 200000,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000011",
      "completion": "0.0000044",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000055"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 100000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {}
  },
  {
    "id": "meta-llama/llama-guard-3-8b",
    "canonical_slug": "meta-llama/llama-guard-3-8b",
    "hugging_face_id": "meta-llama/Llama-Guard-3-8B",
    "name": "Llama Guard 3 8B",
    "created": 1739401318,
    "description": "Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM – it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.\n\nLlama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls.\n",
    "context_length": 131072,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Llama3",
      "instruct_type": "none"
    },
    "pricing": {
      "prompt": "0.00000002",
      "completion": "0.00000006",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logit_bias",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "temperature",
      "top_k",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "mistralai/mistral-saba",
    "canonical_slug": "mistralai/mistral-saba-2502",
    "hugging_face_id": "",
    "name": "Mistral: Saba",
    "created": 1739803239,
    "description": "Mistral Saba is a 24B-parameter language model specifically designed for the Middle East and South Asia, delivering accurate and contextually relevant responses while maintaining efficient performance. Trained on curated regional datasets, it supports multiple Indian-origin languages—including Tamil and Malayalam—alongside Arabic. This makes it a versatile option for a range of regional and multilingual applications. Read more at the blog post [here](https://mistral.ai/en/news/mistral-saba)",
    "context_length": 32768,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000002",
      "completion": "0.0000006",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 32768,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "presence_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.3
    }
  },
  {
    "id": "anthropic/claude-3.7-sonnet:thinking",
    "canonical_slug": "anthropic/claude-3-7-sonnet-20250219",
    "hugging_face_id": "",
    "name": "Anthropic: Claude 3.7 Sonnet (thinking)",
    "created": 1740422110,
    "description": "Claude 3.7 Sonnet is an advanced large language model with improved reasoning, coding, and problem-solving capabilities. It introduces a hybrid reasoning approach, allowing users to choose between rapid responses and extended, step-by-step processing for complex tasks. The model demonstrates notable improvements in coding, particularly in front-end development and full-stack updates, and excels in agentic workflows, where it can autonomously navigate multi-step processes. \n\nClaude 3.7 Sonnet maintains performance parity with its predecessor in standard mode while offering an extended reasoning mode for enhanced accuracy in math, coding, and instruction-following tasks.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-3-7-sonnet)",
    "context_length": 200000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Claude",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000003",
      "completion": "0.000015",
      "request": "0",
      "image": "0.0048",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000003",
      "input_cache_write": "0.00000375"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 64000,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "stop",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "google/gemini-2.0-flash-lite-001",
    "canonical_slug": "google/gemini-2.0-flash-lite-001",
    "hugging_face_id": "",
    "name": "Google: Gemini 2.0 Flash Lite",
    "created": 1740506212,
    "description": "Gemini 2.0 Flash Lite offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](/google/gemini-flash-1.5), while maintaining quality on par with larger models like [Gemini Pro 1.5](/google/gemini-pro-1.5), all at extremely economical token prices.",
    "context_length": 1048576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image",
        "file",
        "audio",
        "video"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000000075",
      "completion": "0.0000003",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 1048576,
      "max_completion_tokens": 8192,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "google/gemma-3-27b-it",
    "canonical_slug": "google/gemma-3-27b-it",
    "hugging_face_id": "",
    "name": "Google: Gemma 3 27B",
    "created": 1741756359,
    "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 27B is Google's latest open source model, successor to [Gemma 2](google/gemma-2-27b-it)",
    "context_length": 96000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": "gemma"
    },
    "pricing": {
      "prompt": "0.00000004",
      "completion": "0.00000015",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 96000,
      "max_completion_tokens": 96000,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logit_bias",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "openai/gpt-4o-search-preview",
    "canonical_slug": "openai/gpt-4o-search-preview-2025-03-11",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-4o Search Preview",
    "created": 1741817949,
    "description": "GPT-4o Search Previewis a specialized model for web search in Chat Completions. It is trained to understand and execute web search queries.",
    "context_length": 128000,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000025",
      "completion": "0.00001",
      "request": "0.035",
      "image": "0.003613",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 128000,
      "max_completion_tokens": 16384,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "structured_outputs",
      "web_search_options"
    ],
    "default_parameters": {}
  },
  {
    "id": "openai/gpt-4o-mini-search-preview",
    "canonical_slug": "openai/gpt-4o-mini-search-preview-2025-03-11",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-4o-mini Search Preview",
    "created": 1741818122,
    "description": "GPT-4o mini Search Preview is a specialized model for web search in Chat Completions. It is trained to understand and execute web search queries.",
    "context_length": 128000,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000015",
      "completion": "0.0000006",
      "request": "0.0275",
      "image": "0.000217",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 128000,
      "max_completion_tokens": 16384,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "structured_outputs",
      "web_search_options"
    ],
    "default_parameters": {}
  },
  {
    "id": "google/gemma-3-12b-it",
    "canonical_slug": "google/gemma-3-12b-it",
    "hugging_face_id": "google/gemma-3-12b-it",
    "name": "Google: Gemma 3 12B",
    "created": 1741902625,
    "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 12B is the second largest in the family of Gemma 3 models after [Gemma 3 27B](google/gemma-3-27b-it)",
    "context_length": 131072,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": "gemma"
    },
    "pricing": {
      "prompt": "0.00000003",
      "completion": "0.0000001",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": 131072,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logit_bias",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "top_k",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "google/gemma-3-4b-it",
    "canonical_slug": "google/gemma-3-4b-it",
    "hugging_face_id": "google/gemma-3-4b-it",
    "name": "Google: Gemma 3 4B",
    "created": 1741905510,
    "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling.",
    "context_length": 96000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": "gemma"
    },
    "pricing": {
      "prompt": "0.00000001703012",
      "completion": "0.0000000681536",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 96000,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "temperature",
      "top_k",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "mistralai/mistral-small-3.1-24b-instruct",
    "canonical_slug": "mistralai/mistral-small-3.1-24b-instruct-2503",
    "hugging_face_id": "mistralai/Mistral-Small-3.1-24B-Instruct-2503",
    "name": "Mistral: Mistral Small 3.1 24B",
    "created": 1742238937,
    "description": "Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billion parameters with advanced multimodal capabilities. It provides state-of-the-art performance in text-based reasoning and vision tasks, including image analysis, programming, mathematical reasoning, and multilingual support across dozens of languages. Equipped with an extensive 128k token context window and optimized for efficient local inference, it supports use cases such as conversational agents, function calling, long-document comprehension, and privacy-sensitive deployments. The updated version is [Mistral Small 3.2](mistralai/mistral-small-3.2-24b-instruct)",
    "context_length": 131072,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000003",
      "completion": "0.00000011",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": 131072,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.3
    }
  },
  {
    "id": "openai/o1-pro",
    "canonical_slug": "openai/o1-pro",
    "hugging_face_id": "",
    "name": "OpenAI: o1-pro",
    "created": 1742423211,
    "description": "The o1 series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o1-pro model uses more compute to think harder and provide consistently better answers.",
    "context_length": 200000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00015",
      "completion": "0.0006",
      "request": "0",
      "image": "0.21675",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 100000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs"
    ],
    "default_parameters": {}
  },
  {
    "id": "deepseek/deepseek-chat-v3-0324",
    "canonical_slug": "deepseek/deepseek-chat-v3-0324",
    "hugging_face_id": "deepseek-ai/DeepSeek-V3-0324",
    "name": "DeepSeek: DeepSeek V3 0324",
    "created": 1742824755,
    "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks.",
    "context_length": 163840,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "DeepSeek",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000002",
      "completion": "0.00000088",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000106"
    },
    "top_provider": {
      "context_length": 163840,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "meta-llama/llama-4-scout",
    "canonical_slug": "meta-llama/llama-4-scout-17b-16e-instruct",
    "hugging_face_id": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
    "name": "Meta: Llama 4 Scout",
    "created": 1743881519,
    "description": "Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, activating 17 billion parameters out of a total of 109B. It supports native multimodal input (text and image) and multilingual output (text and code) across 12 supported languages. Designed for assistant-style interaction and visual reasoning, Scout uses 16 experts per forward pass and features a context length of 10 million tokens, with a training corpus of ~40 trillion tokens.\n\nBuilt for high efficiency and local or commercial deployment, Llama 4 Scout incorporates early fusion for seamless modality integration. It is instruction-tuned for use in multilingual chat, captioning, and image understanding tasks. Released under the Llama 4 Community License, it was last trained on data up to August 2024 and launched publicly on April 5, 2025.",
    "context_length": 327680,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Llama4",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000008",
      "completion": "0.0000003",
      "request": "0",
      "image": "0.0003342",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 327680,
      "max_completion_tokens": 16384,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logit_bias",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "meta-llama/llama-4-maverick",
    "canonical_slug": "meta-llama/llama-4-maverick-17b-128e-instruct",
    "hugging_face_id": "meta-llama/Llama-4-Maverick-17B-128E-Instruct",
    "name": "Meta: Llama 4 Maverick",
    "created": 1743881822,
    "description": "Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction.\n\nMaverick features early fusion for native multimodality and a 1 million token context window. It was trained on a curated mixture of public, licensed, and Meta-platform data, covering ~22 trillion tokens, with a knowledge cutoff in August 2024. Released on April 5, 2025 under the Llama 4 Community License, Maverick is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.",
    "context_length": 1048576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Llama4",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000015",
      "completion": "0.0000006",
      "request": "0",
      "image": "0.0006684",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 1048576,
      "max_completion_tokens": 16384,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logit_bias",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "x-ai/grok-3-beta",
    "canonical_slug": "x-ai/grok-3-beta",
    "hugging_face_id": "",
    "name": "xAI: Grok 3 Beta",
    "created": 1744240068,
    "description": "Grok 3 is the latest model from xAI. It's their flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science.\n\nExcels in structured tasks and benchmarks like GPQA, LCB, and MMLU-Pro where it outperforms Grok 3 Mini even on high thinking. \n\nNote: That there are two xAI endpoints for this model. By default when using this model we will always route you to the base endpoint. If you want the fast endpoint you can add `provider: { sort: throughput}`, to sort by throughput instead. \n",
    "context_length": 131072,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Grok",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000003",
      "completion": "0.000015",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000075"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logprobs",
      "max_tokens",
      "presence_penalty",
      "response_format",
      "seed",
      "stop",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "x-ai/grok-3-mini-beta",
    "canonical_slug": "x-ai/grok-3-mini-beta",
    "hugging_face_id": "",
    "name": "xAI: Grok 3 Mini Beta",
    "created": 1744240195,
    "description": "Grok 3 Mini is a lightweight, smaller thinking model. Unlike traditional models that generate answers immediately, Grok 3 Mini thinks before responding. It’s ideal for reasoning-heavy tasks that don’t demand extensive domain knowledge, and shines in math-specific and quantitative use cases, such as solving challenging puzzles or math problems.\n\nTransparent \"thinking\" traces accessible. Defaults to low reasoning, can boost with setting `reasoning: { effort: \"high\" }`\n\nNote: That there are two xAI endpoints for this model. By default when using this model we will always route you to the base endpoint. If you want the fast endpoint you can add `provider: { sort: throughput}`, to sort by throughput instead. \n",
    "context_length": 131072,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Grok",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000003",
      "completion": "0.0000005",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000075"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "logprobs",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "openai/gpt-4.1-nano",
    "canonical_slug": "openai/gpt-4.1-nano-2025-04-14",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-4.1 Nano",
    "created": 1744651369,
    "description": "For tasks that demand low latency, GPT‑4.1 nano is the fastest and cheapest model in the GPT-4.1 series. It delivers exceptional performance at a small size with its 1 million token context window, and scores 80.1% on MMLU, 50.3% on GPQA, and 9.8% on Aider polyglot coding – even higher than GPT‑4o mini. It’s ideal for tasks like classification or autocompletion.",
    "context_length": 1047576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000001",
      "completion": "0.0000004",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000025"
    },
    "top_provider": {
      "context_length": 1047576,
      "max_completion_tokens": 32768,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {}
  },
  {
    "id": "openai/gpt-4.1-mini",
    "canonical_slug": "openai/gpt-4.1-mini-2025-04-14",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-4.1 Mini",
    "created": 1744651381,
    "description": "GPT-4.1 Mini is a mid-sized model delivering performance competitive with GPT-4o at substantially lower latency and cost. It retains a 1 million token context window and scores 45.1% on hard instruction evals, 35.8% on MultiChallenge, and 84.1% on IFEval. Mini also shows strong coding ability (e.g., 31.6% on Aider’s polyglot diff benchmark) and vision understanding, making it suitable for interactive applications with tight performance constraints.",
    "context_length": 1047576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000004",
      "completion": "0.0000016",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000001"
    },
    "top_provider": {
      "context_length": 1047576,
      "max_completion_tokens": 32768,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {}
  },
  {
    "id": "openai/gpt-4.1",
    "canonical_slug": "openai/gpt-4.1-2025-04-14",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-4.1",
    "created": 1744651385,
    "description": "GPT-4.1 is a flagship large language model optimized for advanced instruction following, real-world software engineering, and long-context reasoning. It supports a 1 million token context window and outperforms GPT-4o and GPT-4.5 across coding (54.6% SWE-bench Verified), instruction compliance (87.4% IFEval), and multimodal understanding benchmarks. It is tuned for precise code diffs, agent reliability, and high recall in large document contexts, making it ideal for agents, IDE tooling, and enterprise knowledge retrieval.",
    "context_length": 1047576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000002",
      "completion": "0.000008",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000005"
    },
    "top_provider": {
      "context_length": 1047576,
      "max_completion_tokens": 32768,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {}
  },
  {
    "id": "openai/o4-mini",
    "canonical_slug": "openai/o4-mini-2025-04-16",
    "hugging_face_id": "",
    "name": "OpenAI: o4 Mini",
    "created": 1744820942,
    "description": "OpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains.\n\nDespite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.",
    "context_length": 200000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000011",
      "completion": "0.0000044",
      "request": "0",
      "image": "0.0008415",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000275"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 100000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {}
  },
  {
    "id": "openai/o3",
    "canonical_slug": "openai/o3-2025-04-16",
    "hugging_face_id": "",
    "name": "OpenAI: o3",
    "created": 1744823457,
    "description": "o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images. ",
    "context_length": 200000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000002",
      "completion": "0.000008",
      "request": "0",
      "image": "0.00153",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000005"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 100000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {}
  },
  {
    "id": "openai/o4-mini-high",
    "canonical_slug": "openai/o4-mini-high-2025-04-16",
    "hugging_face_id": "",
    "name": "OpenAI: o4 Mini High",
    "created": 1744824212,
    "description": "OpenAI o4-mini-high is the same model as [o4-mini](/openai/o4-mini) with reasoning_effort set to high. \n\nOpenAI o4-mini is a compact reasoning model in the o-series, optimized for fast, cost-efficient performance while retaining strong multimodal and agentic capabilities. It supports tool use and demonstrates competitive reasoning and coding performance across benchmarks like AIME (99.5% with Python) and SWE-bench, outperforming its predecessor o3-mini and even approaching o3 in some domains.\n\nDespite its smaller size, o4-mini exhibits high accuracy in STEM tasks, visual problem solving (e.g., MathVista, MMMU), and code editing. It is especially well-suited for high-throughput scenarios where latency or cost is critical. Thanks to its efficient architecture and refined reinforcement learning training, o4-mini can chain tools, generate structured outputs, and solve multi-step tasks with minimal delay—often in under a minute.",
    "context_length": 200000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Other",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000011",
      "completion": "0.0000044",
      "request": "0",
      "image": "0.0008415",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000275"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 100000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {}
  },
  {
    "id": "meta-llama/llama-guard-4-12b",
    "canonical_slug": "meta-llama/llama-guard-4-12b",
    "hugging_face_id": "meta-llama/Llama-Guard-4-12B",
    "name": "Meta: Llama Guard 4 12B",
    "created": 1745975193,
    "description": "Llama Guard 4 is a Llama 4 Scout-derived multimodal pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM—generating text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.\n\nLlama Guard 4 was aligned to safeguard against the standardized MLCommons hazards taxonomy and designed to support multimodal Llama 4 capabilities. Specifically, it combines features from previous Llama Guard models, providing content moderation for English and multiple supported languages, along with enhanced capabilities to handle mixed text-and-image prompts, including multiple images. Additionally, Llama Guard 4 is integrated into the Llama Moderations API, extending robust safety classification to text and images.",
    "context_length": 163840,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Other",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000018",
      "completion": "0.00000018",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 163840,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logit_bias",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "temperature",
      "top_k",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "deepseek/deepseek-prover-v2",
    "canonical_slug": "deepseek/deepseek-prover-v2",
    "hugging_face_id": "deepseek-ai/DeepSeek-Prover-V2-671B",
    "name": "DeepSeek: DeepSeek Prover V2",
    "created": 1746013094,
    "description": "DeepSeek Prover V2 is a 671B parameter model, speculated to be geared towards logic and mathematics. Likely an upgrade from [DeepSeek-Prover-V1.5](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL) Not much is known about the model yet, as DeepSeek released it on Hugging Face without an announcement or description.",
    "context_length": 163840,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "DeepSeek",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000005",
      "completion": "0.00000218",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 163840,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "temperature",
      "top_k",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "google/gemini-2.5-pro-preview-05-06",
    "canonical_slug": "google/gemini-2.5-pro-preview-03-25",
    "hugging_face_id": "",
    "name": "Google: Gemini 2.5 Pro Preview 05-06",
    "created": 1746578513,
    "description": "Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.",
    "context_length": 1048576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image",
        "file",
        "audio",
        "video"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0.00516",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000031",
      "input_cache_write": "0.000001625"
    },
    "top_provider": {
      "context_length": 1048576,
      "max_completion_tokens": 65535,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "mistralai/mistral-medium-3",
    "canonical_slug": "mistralai/mistral-medium-3",
    "hugging_face_id": "",
    "name": "Mistral: Mistral Medium 3",
    "created": 1746627341,
    "description": "Mistral Medium 3 is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8× lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases.\n\nThe model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.",
    "context_length": 131072,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000004",
      "completion": "0.000002",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "presence_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.3
    }
  },
  {
    "id": "openai/codex-mini",
    "canonical_slug": "openai/codex-mini",
    "hugging_face_id": "",
    "name": "OpenAI: Codex Mini",
    "created": 1747409761,
    "description": "codex-mini-latest is a fine-tuned version of o4-mini specifically for use in Codex CLI. For direct use in the API, we recommend starting with gpt-4.1.",
    "context_length": 200000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000015",
      "completion": "0.000006",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000375"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 100000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {}
  },
  {
    "id": "google/gemma-3n-e4b-it",
    "canonical_slug": "google/gemma-3n-e4b-it",
    "hugging_face_id": "google/gemma-3n-E4B-it",
    "name": "Google: Gemma 3n 4B",
    "created": 1747776824,
    "description": "Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as phones, laptops, and tablets. It supports multimodal inputs—including text, visual data, and audio—enabling diverse tasks such as text generation, speech recognition, translation, and image analysis. Leveraging innovations like Per-Layer Embedding (PLE) caching and the MatFormer architecture, Gemma 3n dynamically manages memory usage and computational load by selectively activating model parameters, significantly reducing runtime resource requirements.\n\nThis model supports a wide linguistic range (trained in over 140 languages) and features a flexible 32K token context window. Gemma 3n can selectively load parameters, optimizing memory and computational efficiency based on the task or device capabilities, making it well-suited for privacy-focused, offline-capable applications and on-device AI solutions. [Read more in the blog post](https://developers.googleblog.com/en/introducing-gemma-3n/)",
    "context_length": 32768,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Other",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000002",
      "completion": "0.00000004",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 32768,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logit_bias",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "stop",
      "temperature",
      "top_k",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "anthropic/claude-sonnet-4",
    "canonical_slug": "anthropic/claude-4-sonnet-20250522",
    "hugging_face_id": "",
    "name": "Anthropic: Claude Sonnet 4",
    "created": 1747930371,
    "description": "Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in both coding and reasoning tasks with improved precision and controllability. Achieving state-of-the-art performance on SWE-bench (72.7%), Sonnet 4 balances capability and computational efficiency, making it suitable for a broad range of applications from routine coding tasks to complex software development projects. Key enhancements include improved autonomous codebase navigation, reduced error rates in agent-driven workflows, and increased reliability in following intricate instructions. Sonnet 4 is optimized for practical everyday use, providing advanced reasoning capabilities while maintaining efficiency and responsiveness in diverse internal and external scenarios.\n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)",
    "context_length": 1000000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Claude",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000003",
      "completion": "0.000015",
      "request": "0",
      "image": "0.0048",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000003",
      "input_cache_write": "0.00000375"
    },
    "top_provider": {
      "context_length": 1000000,
      "max_completion_tokens": 64000,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "stop",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "anthropic/claude-opus-4",
    "canonical_slug": "anthropic/claude-4-opus-20250522",
    "hugging_face_id": "",
    "name": "Anthropic: Claude Opus 4",
    "created": 1747931245,
    "description": "Claude Opus 4 is benchmarked as the world’s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation. \n\nRead more at the [blog post here](https://www.anthropic.com/news/claude-4)",
    "context_length": 200000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Claude",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000015",
      "completion": "0.000075",
      "request": "0",
      "image": "0.024",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000015",
      "input_cache_write": "0.00001875"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 32000,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "stop",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "deepseek/deepseek-r1-0528",
    "canonical_slug": "deepseek/deepseek-r1-0528",
    "hugging_face_id": "deepseek-ai/DeepSeek-R1-0528",
    "name": "DeepSeek: R1 0528",
    "created": 1748455170,
    "description": "May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.\n\nFully open-source model.",
    "context_length": 163840,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "DeepSeek",
      "instruct_type": "deepseek-r1"
    },
    "pricing": {
      "prompt": "0.0000004",
      "completion": "0.00000175",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 163840,
      "max_completion_tokens": 163840,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "deepseek/deepseek-r1-0528-qwen3-8b",
    "canonical_slug": "deepseek/deepseek-r1-0528-qwen3-8b",
    "hugging_face_id": "deepseek-ai/deepseek-r1-0528-qwen3-8b",
    "name": "DeepSeek: DeepSeek R1 0528 Qwen3 8B",
    "created": 1748538543,
    "description": "DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter post-training tricks, pushing its reasoning and inference to the brink of flagship models like O3 and Gemini 2.5 Pro.\nIt now tops math, programming, and logic leaderboards, showcasing a step-change in depth-of-thought.\nThe distilled variant, DeepSeek-R1-0528-Qwen3-8B, transfers this chain-of-thought into an 8 B-parameter form, beating standard Qwen3 8B by +10 pp and tying the 235 B “thinking” giant on AIME 2024.",
    "context_length": 32768,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Qwen",
      "instruct_type": "deepseek-r1"
    },
    "pricing": {
      "prompt": "0.00000002",
      "completion": "0.0000001",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 32768,
      "max_completion_tokens": 32768,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "max_tokens",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "google/gemini-2.5-pro-preview",
    "canonical_slug": "google/gemini-2.5-pro-preview-06-05",
    "hugging_face_id": "",
    "name": "Google: Gemini 2.5 Pro Preview 06-05",
    "created": 1749137257,
    "description": "Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.\n",
    "context_length": 1048576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "file",
        "image",
        "text",
        "audio"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0.00516",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000031",
      "input_cache_write": "0.000001625"
    },
    "top_provider": {
      "context_length": 1048576,
      "max_completion_tokens": 65536,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "x-ai/grok-3",
    "canonical_slug": "x-ai/grok-3",
    "hugging_face_id": "",
    "name": "xAI: Grok 3",
    "created": 1749582908,
    "description": "Grok 3 is the latest model from xAI. It's their flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science.\n\n",
    "context_length": 131072,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Grok",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000003",
      "completion": "0.000015",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000075"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logprobs",
      "max_tokens",
      "presence_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "x-ai/grok-3-mini",
    "canonical_slug": "x-ai/grok-3-mini",
    "hugging_face_id": "",
    "name": "xAI: Grok 3 Mini",
    "created": 1749583245,
    "description": "A lightweight model that thinks before responding. Fast, smart, and great for logic-based tasks that do not require deep domain knowledge. The raw thinking traces are accessible.",
    "context_length": 131072,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Grok",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000003",
      "completion": "0.0000005",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000075"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "logprobs",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "openai/o3-pro",
    "canonical_slug": "openai/o3-pro-2025-06-10",
    "hugging_face_id": "",
    "name": "OpenAI: o3 Pro",
    "created": 1749598352,
    "description": "The o-series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o3-pro model uses more compute to think harder and provide consistently better answers.\n\nNote that BYOK is required for this model. Set up here: https://openrouter.ai/settings/integrations",
    "context_length": 200000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "file",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00002",
      "completion": "0.00008",
      "request": "0",
      "image": "0.0153",
      "web_search": "0.01",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 100000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {}
  },
  {
    "id": "google/gemini-2.5-pro",
    "canonical_slug": "google/gemini-2.5-pro",
    "hugging_face_id": "",
    "name": "Google: Gemini 2.5 Pro",
    "created": 1750169544,
    "description": "Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities.",
    "context_length": 1048576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image",
        "file",
        "audio",
        "video"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0.00516",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000125",
      "input_cache_write": "0.000001625"
    },
    "top_provider": {
      "context_length": 1048576,
      "max_completion_tokens": 65536,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "google/gemini-2.5-flash",
    "canonical_slug": "google/gemini-2.5-flash",
    "hugging_face_id": "",
    "name": "Google: Gemini 2.5 Flash",
    "created": 1750172488,
    "description": "Gemini 2.5 Flash is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
    "context_length": 1048576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "file",
        "image",
        "text",
        "audio",
        "video"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000003",
      "completion": "0.0000025",
      "request": "0",
      "image": "0.001238",
      "audio": "0.000001",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000003",
      "input_cache_write": "0.0000003833"
    },
    "top_provider": {
      "context_length": 1048576,
      "max_completion_tokens": 65535,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "mistralai/mistral-small-3.2-24b-instruct",
    "canonical_slug": "mistralai/mistral-small-3.2-24b-instruct-2506",
    "hugging_face_id": "mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "name": "Mistral: Mistral Small 3.2 24B",
    "created": 1750443016,
    "description": "Mistral-Small-3.2-24B-Instruct-2506 is an updated 24B parameter model from Mistral optimized for instruction following, repetition reduction, and improved function calling. Compared to the 3.1 release, version 3.2 significantly improves accuracy on WildBench and Arena Hard, reduces infinite generations, and delivers gains in tool use and structured output tasks.\n\nIt supports image and text inputs with structured outputs, function/tool calling, and strong performance across coding (HumanEval+, MBPP), STEM (MMLU, MATH, GPQA), and vision benchmarks (ChartQA, DocVQA).",
    "context_length": 131072,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000006",
      "completion": "0.00000018",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": 131072,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logit_bias",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.3
    }
  },
  {
    "id": "x-ai/grok-4",
    "canonical_slug": "x-ai/grok-4-07-09",
    "hugging_face_id": "",
    "name": "xAI: Grok 4",
    "created": 1752087689,
    "description": "Grok 4 is xAI's latest reasoning model with a 256k context window. It supports parallel tool calling, structured outputs, and both image and text inputs. Note that reasoning is not exposed, reasoning cannot be disabled, and the reasoning effort cannot be specified. Pricing increases once the total tokens in a given request is greater than 128k tokens. See more details on the [xAI docs](https://docs.x.ai/docs/models/grok-4-0709)",
    "context_length": 256000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Grok",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000003",
      "completion": "0.000015",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000075"
    },
    "top_provider": {
      "context_length": 256000,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "logprobs",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "mistralai/devstral-small",
    "canonical_slug": "mistralai/devstral-small-2507",
    "hugging_face_id": "mistralai/Devstral-Small-2507",
    "name": "Mistral: Devstral Small 1.1",
    "created": 1752160751,
    "description": "Devstral Small 1.1 is a 24B parameter open-weight language model for software engineering agents, developed by Mistral AI in collaboration with All Hands AI. Finetuned from Mistral Small 3.1 and released under the Apache 2.0 license, it features a 128k token context window and supports both Mistral-style function calling and XML output formats.\n\nDesigned for agentic coding workflows, Devstral Small 1.1 is optimized for tasks such as codebase exploration, multi-file edits, and integration into autonomous development agents like OpenHands and Cline. It achieves 53.6% on SWE-Bench Verified, surpassing all other open models on this benchmark, while remaining lightweight enough to run on a single 4090 GPU or Apple silicon machine. The model uses a Tekken tokenizer with a 131k vocabulary and is deployable via vLLM, Transformers, Ollama, LM Studio, and other OpenAI-compatible runtimes.\n",
    "context_length": 128000,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000007",
      "completion": "0.00000028",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 128000,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.3
    }
  },
  {
    "id": "mistralai/devstral-medium",
    "canonical_slug": "mistralai/devstral-medium-2507",
    "hugging_face_id": "",
    "name": "Mistral: Devstral Medium",
    "created": 1752161321,
    "description": "Devstral Medium is a high-performance code generation and agentic reasoning model developed jointly by Mistral AI and All Hands AI. Positioned as a step up from Devstral Small, it achieves 61.6% on SWE-Bench Verified, placing it ahead of Gemini 2.5 Pro and GPT-4.1 in code-related tasks, at a fraction of the cost. It is designed for generalization across prompt styles and tool use in code agents and frameworks.\n\nDevstral Medium is available via API only (not open-weight), and supports enterprise deployment on private infrastructure, with optional fine-tuning capabilities.",
    "context_length": 131072,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000004",
      "completion": "0.000002",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "presence_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.3
    }
  },
  {
    "id": "google/gemini-2.5-flash-lite",
    "canonical_slug": "google/gemini-2.5-flash-lite",
    "hugging_face_id": "",
    "name": "Google: Gemini 2.5 Flash Lite",
    "created": 1753200276,
    "description": "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ",
    "context_length": 1048576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image",
        "file",
        "audio",
        "video"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000001",
      "completion": "0.0000004",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000001",
      "input_cache_write": "0.0000001833"
    },
    "top_provider": {
      "context_length": 1048576,
      "max_completion_tokens": 65535,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "mistralai/codestral-2508",
    "canonical_slug": "mistralai/codestral-2508",
    "hugging_face_id": "",
    "name": "Mistral: Codestral 2508",
    "created": 1754079630,
    "description": "Mistral's cutting-edge language model for coding released end of July 2025. Codestral specializes in low-latency, high-frequency tasks such as fill-in-the-middle (FIM), code correction and test generation.\n\n[Blog Post](https://mistral.ai/news/codestral-25-08)",
    "context_length": 256000,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000003",
      "completion": "0.0000009",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 256000,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "presence_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.3
    }
  },
  {
    "id": "anthropic/claude-opus-4.1",
    "canonical_slug": "anthropic/claude-4.1-opus-20250805",
    "hugging_face_id": "",
    "name": "Anthropic: Claude Opus 4.1",
    "created": 1754411591,
    "description": "Claude Opus 4.1 is an updated version of Anthropic’s flagship model, offering improved performance in coding, reasoning, and agentic tasks. It achieves 74.5% on SWE-bench Verified and shows notable gains in multi-file code refactoring, debugging precision, and detail-oriented reasoning. The model supports extended thinking up to 64K tokens and is optimized for tasks involving research, data analysis, and tool-assisted reasoning.",
    "context_length": 200000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Claude",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000015",
      "completion": "0.000075",
      "request": "0",
      "image": "0.024",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000015",
      "input_cache_write": "0.00001875"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 32000,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-oss-20b",
    "canonical_slug": "openai/gpt-oss-20b",
    "hugging_face_id": "openai/gpt-oss-20b",
    "name": "OpenAI: gpt-oss-20b",
    "created": 1754414229,
    "description": "gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.",
    "context_length": 131072,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000003",
      "completion": "0.00000014",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "logit_bias",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-oss-120b",
    "canonical_slug": "openai/gpt-oss-120b",
    "hugging_face_id": "openai/gpt-oss-120b",
    "name": "OpenAI: gpt-oss-120b",
    "created": 1754414231,
    "description": "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.",
    "context_length": 131072,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000000039",
      "completion": "0.00000019",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5-nano",
    "canonical_slug": "openai/gpt-5-nano-2025-08-07",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5 Nano",
    "created": 1754587402,
    "description": "GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, optimized for developer tools, rapid interactions, and ultra-low latency environments. While limited in reasoning depth compared to its larger counterparts, it retains key instruction-following and safety features. It is the successor to GPT-4.1-nano and offers a lightweight option for cost-sensitive or real-time applications.",
    "context_length": 400000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000005",
      "completion": "0.0000004",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000005"
    },
    "top_provider": {
      "context_length": 400000,
      "max_completion_tokens": 128000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {}
  },
  {
    "id": "openai/gpt-5-mini",
    "canonical_slug": "openai/gpt-5-mini-2025-08-07",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5 Mini",
    "created": 1754587407,
    "description": "GPT-5 Mini is a compact version of GPT-5, designed to handle lighter-weight reasoning tasks. It provides the same instruction-following and safety-tuning benefits as GPT-5, but with reduced latency and cost. GPT-5 Mini is the successor to OpenAI's o4-mini model.",
    "context_length": 400000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000025",
      "completion": "0.000002",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000025"
    },
    "top_provider": {
      "context_length": 400000,
      "max_completion_tokens": 128000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {}
  },
  {
    "id": "openai/gpt-5",
    "canonical_slug": "openai/gpt-5-2025-08-07",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5",
    "created": 1754587413,
    "description": "GPT-5 is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.",
    "context_length": 400000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000125"
    },
    "top_provider": {
      "context_length": 400000,
      "max_completion_tokens": 128000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5-chat",
    "canonical_slug": "openai/gpt-5-chat-2025-08-07",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5 Chat",
    "created": 1754587837,
    "description": "GPT-5 Chat is designed for advanced, natural, multimodal, and context-aware conversations for enterprise applications.",
    "context_length": 128000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "file",
        "image",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000125"
    },
    "top_provider": {
      "context_length": 128000,
      "max_completion_tokens": 16384,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "seed",
      "structured_outputs"
    ],
    "default_parameters": {}
  },
  {
    "id": "mistralai/mistral-medium-3.1",
    "canonical_slug": "mistralai/mistral-medium-3.1",
    "hugging_face_id": "",
    "name": "Mistral: Mistral Medium 3.1",
    "created": 1755095639,
    "description": "Mistral Medium 3.1 is an updated version of Mistral Medium 3, which is a high-performance enterprise-grade language model designed to deliver frontier-level capabilities at significantly reduced operational cost. It balances state-of-the-art reasoning and multimodal performance with 8× lower cost compared to traditional large models, making it suitable for scalable deployments across professional and industrial use cases.\n\nThe model excels in domains such as coding, STEM reasoning, and enterprise adaptation. It supports hybrid, on-prem, and in-VPC deployments and is optimized for integration into custom workflows. Mistral Medium 3.1 offers competitive accuracy relative to larger models like Claude Sonnet 3.5/3.7, Llama 4 Maverick, and Command R+, while maintaining broad compatibility across cloud environments.",
    "context_length": 131072,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000004",
      "completion": "0.000002",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "presence_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.3
    }
  },
  {
    "id": "openai/gpt-4o-audio-preview",
    "canonical_slug": "openai/gpt-4o-audio-preview",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-4o Audio",
    "created": 1755233061,
    "description": "The gpt-4o-audio-preview model adds support for audio inputs as prompts. This enhancement allows the model to detect nuances within audio recordings and add depth to generated user experiences. Audio outputs are currently not supported. Audio tokens are priced at $40 per million input audio tokens.",
    "context_length": 128000,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "audio",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000025",
      "completion": "0.00001",
      "request": "0",
      "image": "0",
      "audio": "0.00004",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 128000,
      "max_completion_tokens": 16384,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "presence_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "deepseek/deepseek-chat-v3.1",
    "canonical_slug": "deepseek/deepseek-chat-v3.1",
    "hugging_face_id": "deepseek-ai/DeepSeek-V3.1",
    "name": "DeepSeek: DeepSeek V3.1",
    "created": 1755779628,
    "description": "DeepSeek-V3.1 is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes via prompt templates. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. \n\nIt succeeds the [DeepSeek V3-0324](/deepseek/deepseek-chat-v3-0324) model and performs well on a variety of tasks.",
    "context_length": 8192,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "DeepSeek",
      "instruct_type": "deepseek-v3.1"
    },
    "pricing": {
      "prompt": "0.00000015",
      "completion": "0.00000075",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 8192,
      "max_completion_tokens": 7168,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "google/gemini-2.5-flash-image-preview",
    "canonical_slug": "google/gemini-2.5-flash-image-preview",
    "hugging_face_id": "",
    "name": "Google: Gemini 2.5 Flash Image Preview (Nano Banana)",
    "created": 1756218977,
    "description": "Gemini 2.5 Flash Image Preview, a.k.a. \"Nano Banana,\" is a state of the art image generation model with contextual understanding. It is capable of image generation, edits, and multi-turn conversations.",
    "context_length": 32768,
    "architecture": {
      "modality": "text+image->text+image",
      "input_modalities": [
        "image",
        "text"
      ],
      "output_modalities": [
        "image",
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000003",
      "completion": "0.0000025",
      "request": "0",
      "image": "0.001238",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 32768,
      "max_completion_tokens": 32768,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "seed",
      "structured_outputs",
      "temperature",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "x-ai/grok-code-fast-1",
    "canonical_slug": "x-ai/grok-code-fast-1",
    "hugging_face_id": "",
    "name": "xAI: Grok Code Fast 1",
    "created": 1756238927,
    "description": "Grok Code Fast 1 is a speedy and economical reasoning model that excels at agentic coding. With reasoning traces visible in the response, developers can steer Grok Code for high-quality work flows.",
    "context_length": 256000,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Grok",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000002",
      "completion": "0.0000015",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000002"
    },
    "top_provider": {
      "context_length": 256000,
      "max_completion_tokens": 10000,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "logprobs",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {}
  },
  {
    "id": "x-ai/grok-4-fast",
    "canonical_slug": "x-ai/grok-4-fast",
    "hugging_face_id": "",
    "name": "xAI: Grok 4 Fast",
    "created": 1758240090,
    "description": "Grok 4 Fast is xAI's latest multimodal model with SOTA cost-efficiency and a 2M token context window. It comes in two flavors: non-reasoning and reasoning. Read more about the model on xAI's [news post](http://x.ai/news/grok-4-fast).\n\nReasoning can be enabled/disabled using the `reasoning` `enabled` parameter in the API. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)",
    "context_length": 2000000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Grok",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000002",
      "completion": "0.0000005",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000005"
    },
    "top_provider": {
      "context_length": 2000000,
      "max_completion_tokens": 30000,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "logprobs",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "deepseek/deepseek-v3.1-terminus",
    "canonical_slug": "deepseek/deepseek-v3.1-terminus",
    "hugging_face_id": "deepseek-ai/DeepSeek-V3.1-Terminus",
    "name": "DeepSeek: DeepSeek V3.1 Terminus",
    "created": 1758548275,
    "description": "DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language consistency and agent capabilities, further optimizing the model's performance in coding and search agents. It is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. ",
    "context_length": 163840,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "DeepSeek",
      "instruct_type": "deepseek-v3.1"
    },
    "pricing": {
      "prompt": "0.00000021",
      "completion": "0.00000079",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000168"
    },
    "top_provider": {
      "context_length": 163840,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5-codex",
    "canonical_slug": "openai/gpt-5-codex",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5 Codex",
    "created": 1758643403,
    "description": "GPT-5-Codex is a specialized version of GPT-5 optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5, Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)\n\nCodex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically—providing fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications.",
    "context_length": 400000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000125"
    },
    "top_provider": {
      "context_length": 400000,
      "max_completion_tokens": 128000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "google/gemini-2.5-flash-lite-preview-09-2025",
    "canonical_slug": "google/gemini-2.5-flash-lite-preview-09-2025",
    "hugging_face_id": "",
    "name": "Google: Gemini 2.5 Flash Lite Preview 09-2025",
    "created": 1758819686,
    "description": "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. ",
    "context_length": 1048576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image",
        "file",
        "audio",
        "video"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000001",
      "completion": "0.0000004",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 1048576,
      "max_completion_tokens": 65536,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "google/gemini-2.5-flash-preview-09-2025",
    "canonical_slug": "google/gemini-2.5-flash-preview-09-2025",
    "hugging_face_id": "",
    "name": "Google: Gemini 2.5 Flash Preview 09-2025",
    "created": 1758820178,
    "description": "Gemini 2.5 Flash Preview September 2025 Checkpoint is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning).",
    "context_length": 1048576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "file",
        "text",
        "audio",
        "video"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000003",
      "completion": "0.0000025",
      "request": "0",
      "image": "0.001238",
      "audio": "0.000001",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000075",
      "input_cache_write": "0.0000003833"
    },
    "top_provider": {
      "context_length": 1048576,
      "max_completion_tokens": 65536,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "deepseek/deepseek-v3.2-exp",
    "canonical_slug": "deepseek/deepseek-v3.2-exp",
    "hugging_face_id": "deepseek-ai/DeepSeek-V3.2-Exp",
    "name": "DeepSeek: DeepSeek V3.2 Exp",
    "created": 1759150481,
    "description": "DeepSeek-V3.2-Exp is an experimental large language model released by DeepSeek as an intermediate step between V3.1 and future architectures. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism designed to improve training and inference efficiency in long-context scenarios while maintaining output quality. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model was trained under conditions aligned with V3.1-Terminus to enable direct comparison. Benchmarking shows performance roughly on par with V3.1 across reasoning, coding, and agentic tool-use tasks, with minor tradeoffs and gains depending on the domain. This release focuses on validating architectural optimizations for extended context lengths rather than advancing raw task accuracy, making it primarily a research-oriented model for exploring efficient transformer designs.",
    "context_length": 163840,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "DeepSeek",
      "instruct_type": "deepseek-v3.1"
    },
    "pricing": {
      "prompt": "0.00000021",
      "completion": "0.00000032",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000168"
    },
    "top_provider": {
      "context_length": 163840,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.6,
      "top_p": 0.95,
      "frequency_penalty": null
    }
  },
  {
    "id": "anthropic/claude-sonnet-4.5",
    "canonical_slug": "anthropic/claude-4.5-sonnet-20250929",
    "hugging_face_id": "",
    "name": "Anthropic: Claude Sonnet 4.5",
    "created": 1759161676,
    "description": "Claude Sonnet 4.5 is Anthropic’s most advanced Sonnet model to date, optimized for real-world agents and coding workflows. It delivers state-of-the-art performance on coding benchmarks such as SWE-bench Verified, with improvements across system design, code security, and specification adherence. The model is designed for extended autonomous operation, maintaining task continuity across sessions and providing fact-based progress tracking.\n\nSonnet 4.5 also introduces stronger agentic capabilities, including improved tool orchestration, speculative parallel execution, and more efficient context and memory management. With enhanced context tracking and awareness of token usage across tool calls, it is particularly well-suited for multi-context and long-running workflows. Use cases span software engineering, cybersecurity, financial analysis, research agents, and other domains requiring sustained reasoning and tool use.",
    "context_length": 1000000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Claude",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000003",
      "completion": "0.000015",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000003",
      "input_cache_write": "0.00000375"
    },
    "top_provider": {
      "context_length": 1000000,
      "max_completion_tokens": 64000,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 1,
      "top_p": 1,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5-pro",
    "canonical_slug": "openai/gpt-5-pro-2025-10-06",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5 Pro",
    "created": 1759776663,
    "description": "GPT-5 Pro is OpenAI’s most advanced model, offering major improvements in reasoning, code quality, and user experience. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.",
    "context_length": 400000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000015",
      "completion": "0.00012",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 400000,
      "max_completion_tokens": 128000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "google/gemini-2.5-flash-image",
    "canonical_slug": "google/gemini-2.5-flash-image",
    "hugging_face_id": "",
    "name": "Google: Gemini 2.5 Flash Image (Nano Banana)",
    "created": 1759870431,
    "description": "Gemini 2.5 Flash Image, a.k.a. \"Nano Banana,\" is now generally available. It is a state of the art image generation model with contextual understanding. It is capable of image generation, edits, and multi-turn conversations. Aspect ratios can be controlled with the [image_config API Parameter](https://openrouter.ai/docs/features/multimodal/image-generation#image-aspect-ratio-configuration)",
    "context_length": 32768,
    "architecture": {
      "modality": "text+image->text+image",
      "input_modalities": [
        "image",
        "text"
      ],
      "output_modalities": [
        "image",
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000003",
      "completion": "0.0000025",
      "request": "0",
      "image": "0.001238",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 32768,
      "max_completion_tokens": 32768,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "seed",
      "structured_outputs",
      "temperature",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/o4-mini-deep-research",
    "canonical_slug": "openai/o4-mini-deep-research-2025-06-26",
    "hugging_face_id": "",
    "name": "OpenAI: o4 Mini Deep Research",
    "created": 1760129642,
    "description": "o4-mini-deep-research is OpenAI's faster, more affordable deep research model—ideal for tackling complex, multi-step research tasks.\n\nNote: This model always uses the 'web_search' tool which adds additional cost.",
    "context_length": 200000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "file",
        "image",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000002",
      "completion": "0.000008",
      "request": "0",
      "image": "0.00153",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000005"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 100000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "presence_penalty",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/o3-deep-research",
    "canonical_slug": "openai/o3-deep-research-2025-06-26",
    "hugging_face_id": "",
    "name": "OpenAI: o3 Deep Research",
    "created": 1760129661,
    "description": "o3-deep-research is OpenAI's advanced model for deep research, designed to tackle complex, multi-step research tasks.\n\nNote: This model always uses the 'web_search' tool which adds additional cost.",
    "context_length": 200000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00001",
      "completion": "0.00004",
      "request": "0",
      "image": "0.00765",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000025"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 100000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "presence_penalty",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5-image",
    "canonical_slug": "openai/gpt-5-image",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5 Image",
    "created": 1760447986,
    "description": "[GPT-5](https://openrouter.ai/openai/gpt-5) Image combines OpenAI's most advanced language model with state-of-the-art image generation capabilities. It offers major improvements in reasoning, code quality, and user experience while incorporating GPT Image 1's superior instruction following, text rendering, and detailed image editing.",
    "context_length": 400000,
    "architecture": {
      "modality": "text+image->text+image",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "image",
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00001",
      "completion": "0.00001",
      "request": "0",
      "image": "0.00001",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000125"
    },
    "top_provider": {
      "context_length": 400000,
      "max_completion_tokens": 128000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "presence_penalty",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "anthropic/claude-haiku-4.5",
    "canonical_slug": "anthropic/claude-4.5-haiku-20251001",
    "hugging_face_id": "",
    "name": "Anthropic: Claude Haiku 4.5",
    "created": 1760547638,
    "description": "Claude Haiku 4.5 is Anthropic’s fastest and most efficient model, delivering near-frontier intelligence at a fraction of the cost and latency of larger Claude models. Matching Claude Sonnet 4’s performance across reasoning, coding, and computer-use tasks, Haiku 4.5 brings frontier-level capability to real-time and high-volume applications.\n\nIt introduces extended thinking to the Haiku line; enabling controllable reasoning depth, summarized or interleaved thought output, and tool-assisted workflows with full support for coding, bash, web search, and computer-use tools. Scoring >73% on SWE-bench Verified, Haiku 4.5 ranks among the world’s best coding models while maintaining exceptional responsiveness for sub-agents, parallelized execution, and scaled deployment.",
    "context_length": 200000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Claude",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000001",
      "completion": "0.000005",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000001",
      "input_cache_write": "0.00000125"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 64000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "stop",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5-image-mini",
    "canonical_slug": "openai/gpt-5-image-mini",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5 Image Mini",
    "created": 1760624583,
    "description": "GPT-5 Image Mini combines OpenAI's advanced language capabilities, powered by [GPT-5 Mini](https://openrouter.ai/openai/gpt-5-mini), with GPT Image 1 Mini for efficient image generation. This natively multimodal model features superior instruction following, text rendering, and detailed image editing with reduced latency and cost. It excels at high-quality visual creation while maintaining strong text understanding, making it ideal for applications that require both efficient image generation and text processing at scale.",
    "context_length": 400000,
    "architecture": {
      "modality": "text+image->text+image",
      "input_modalities": [
        "file",
        "image",
        "text"
      ],
      "output_modalities": [
        "image",
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000025",
      "completion": "0.000002",
      "request": "0",
      "image": "0.0000025",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000025"
    },
    "top_provider": {
      "context_length": 400000,
      "max_completion_tokens": 128000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "presence_penalty",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-oss-safeguard-20b",
    "canonical_slug": "openai/gpt-oss-safeguard-20b",
    "hugging_face_id": "openai/gpt-oss-safeguard-20b",
    "name": "OpenAI: gpt-oss-safeguard-20b",
    "created": 1761752836,
    "description": "gpt-oss-safeguard-20b is a safety reasoning model from OpenAI built upon gpt-oss-20b. This open-weight, 21B-parameter Mixture-of-Experts (MoE) model offers lower latency for safety tasks like content classification, LLM filtering, and trust & safety labeling.\n\nLearn more about this model in OpenAI's gpt-oss-safeguard [user guide](https://cookbook.openai.com/articles/gpt-oss-safeguard-guide).",
    "context_length": 131072,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000000075",
      "completion": "0.0000003",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000037"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": 65536,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "mistralai/voxtral-small-24b-2507",
    "canonical_slug": "mistralai/voxtral-small-24b-2507",
    "hugging_face_id": "mistralai/Voxtral-Small-24B-2507",
    "name": "Mistral: Voxtral Small 24B 2507",
    "created": 1761835144,
    "description": "Voxtral Small is an enhancement of Mistral Small 3, incorporating state-of-the-art audio input capabilities while retaining best-in-class text performance. It excels at speech transcription, translation and audio understanding. Input audio is priced at $100 per million seconds.",
    "context_length": 32000,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text",
        "audio"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000001",
      "completion": "0.0000003",
      "request": "0",
      "image": "0",
      "audio": "0.0001",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 32000,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "presence_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.2,
      "top_p": 0.95,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5.1-codex-mini",
    "canonical_slug": "openai/gpt-5.1-codex-mini-20251113",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5.1-Codex-Mini",
    "created": 1763057820,
    "description": "GPT-5.1-Codex-Mini is a smaller and faster version of GPT-5.1-Codex",
    "context_length": 400000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000025",
      "completion": "0.000002",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000025"
    },
    "top_provider": {
      "context_length": 400000,
      "max_completion_tokens": 100000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5.1-codex",
    "canonical_slug": "openai/gpt-5.1-codex-20251113",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5.1-Codex",
    "created": 1763060298,
    "description": "GPT-5.1-Codex is a specialized version of GPT-5.1 optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5.1, Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)\n\nCodex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically—providing fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications.",
    "context_length": 400000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000125"
    },
    "top_provider": {
      "context_length": 400000,
      "max_completion_tokens": 128000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5.1-chat",
    "canonical_slug": "openai/gpt-5.1-chat-20251113",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5.1 Chat",
    "created": 1763060302,
    "description": "GPT-5.1 Chat (AKA Instant is the fast, lightweight member of the 5.1 family, optimized for low-latency chat while retaining strong general intelligence. It uses adaptive reasoning to selectively “think” on harder queries, improving accuracy on math, coding, and multi-step tasks without slowing down typical conversations. The model is warmer and more conversational by default, with better instruction following and more stable short-form reasoning. GPT-5.1 Chat is designed for high-throughput, interactive workloads where responsiveness and consistency matter more than deep deliberation.\n",
    "context_length": 128000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "file",
        "image",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000125"
    },
    "top_provider": {
      "context_length": 128000,
      "max_completion_tokens": 16384,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5.1",
    "canonical_slug": "openai/gpt-5.1-20251113",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5.1",
    "created": 1763060305,
    "description": "GPT-5.1 is the latest frontier-grade model in the GPT-5 series, offering stronger general-purpose reasoning, improved instruction adherence, and a more natural conversational style compared to GPT-5. It uses adaptive reasoning to allocate computation dynamically, responding quickly to simple queries while spending more depth on complex tasks. The model produces clearer, more grounded explanations with reduced jargon, making it easier to follow even on technical or multi-step problems.\n\nBuilt for broad task coverage, GPT-5.1 delivers consistent gains across math, coding, and structured analysis workloads, with more coherent long-form answers and improved tool-use reliability. It also features refined conversational alignment, enabling warmer, more intuitive responses without compromising precision. GPT-5.1 serves as the primary full-capability successor to GPT-5",
    "context_length": 400000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000125"
    },
    "top_provider": {
      "context_length": 400000,
      "max_completion_tokens": 128000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "google/gemini-3-pro-preview",
    "canonical_slug": "google/gemini-3-pro-preview-20251117",
    "hugging_face_id": "",
    "name": "Google: Gemini 3 Pro Preview",
    "created": 1763474668,
    "description": "Gemini 3 Pro is Google’s flagship frontier model for high-precision multimodal reasoning, combining strong performance across text, image, video, audio, and code with a 1M-token context window. Reasoning Details must be preserved when using multi-turn tool calling, see our docs here: https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks. It delivers state-of-the-art benchmark results in general reasoning, STEM problem solving, factual QA, and multimodal understanding, including leading scores on LMArena, GPQA Diamond, MathArena Apex, MMMU-Pro, and Video-MMMU. Interactions emphasize depth and interpretability: the model is designed to infer intent with minimal prompting and produce direct, insight-focused responses.\n\nBuilt for advanced development and agentic workflows, Gemini 3 Pro provides robust tool-calling, long-horizon planning stability, and strong zero-shot generation for complex UI, visualization, and coding tasks. It excels at agentic coding (SWE-Bench Verified, Terminal-Bench 2.0), multimodal analysis, and structured long-form tasks such as research synthesis, planning, and interactive learning experiences. Suitable applications include autonomous agents, coding assistants, multimodal analytics, scientific reasoning, and high-context information processing.",
    "context_length": 1048576,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image",
        "file",
        "audio",
        "video"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000002",
      "completion": "0.000012",
      "request": "0",
      "image": "0.008256",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000002",
      "input_cache_write": "0.000002375"
    },
    "top_provider": {
      "context_length": 1048576,
      "max_completion_tokens": 65536,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "x-ai/grok-4.1-fast",
    "canonical_slug": "x-ai/grok-4.1-fast",
    "hugging_face_id": "",
    "name": "xAI: Grok 4.1 Fast",
    "created": 1763587502,
    "description": "Grok 4.1 Fast is xAI's best agentic tool calling model that shines in real-world use cases like customer support and deep research. 2M context window.\n\nReasoning can be enabled/disabled using the `reasoning` `enabled` parameter in the API. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)",
    "context_length": 2000000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Grok",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000002",
      "completion": "0.0000005",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000005"
    },
    "top_provider": {
      "context_length": 2000000,
      "max_completion_tokens": 30000,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "logprobs",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.7,
      "top_p": 0.95,
      "frequency_penalty": null
    }
  },
  {
    "id": "google/gemini-3-pro-image-preview",
    "canonical_slug": "google/gemini-3-pro-image-preview-20251120",
    "hugging_face_id": "",
    "name": "Google: Nano Banana Pro (Gemini 3 Pro Image Preview)",
    "created": 1763653797,
    "description": "Nano Banana Pro is Google’s most advanced image-generation and editing model, built on Gemini 3 Pro. It extends the original Nano Banana with significantly improved multimodal reasoning, real-world grounding, and high-fidelity visual synthesis. The model generates context-rich graphics, from infographics and diagrams to cinematic composites, and can incorporate real-time information via Search grounding.\n\nIt offers industry-leading text rendering in images (including long passages and multilingual layouts), consistent multi-image blending, and accurate identity preservation across up to five subjects. Nano Banana Pro adds fine-grained creative controls such as localized edits, lighting and focus adjustments, camera transformations, and support for 2K/4K outputs and flexible aspect ratios. It is designed for professional-grade design, product visualization, storyboarding, and complex multi-element compositions while remaining efficient for general image creation workflows.",
    "context_length": 65536,
    "architecture": {
      "modality": "text+image->text+image",
      "input_modalities": [
        "image",
        "text"
      ],
      "output_modalities": [
        "image",
        "text"
      ],
      "tokenizer": "Gemini",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000002",
      "completion": "0.000012",
      "request": "0",
      "image": "0.067",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 65536,
      "max_completion_tokens": 32768,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "temperature",
      "top_p"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "anthropic/claude-opus-4.5",
    "canonical_slug": "anthropic/claude-4.5-opus-20251124",
    "hugging_face_id": "",
    "name": "Anthropic: Claude Opus 4.5",
    "created": 1764010580,
    "description": "Claude Opus 4.5 is Anthropic’s frontier reasoning model optimized for complex software engineering, agentic workflows, and long-horizon computer use. It offers strong multimodal capabilities, competitive performance across real-world coding and reasoning benchmarks, and improved robustness to prompt injection. The model is designed to operate efficiently across varied effort levels, enabling developers to trade off speed, depth, and token usage depending on task requirements. It comes with a new parameter to control token efficiency, which can be accessed using the OpenRouter Verbosity parameter with low, medium, or high.\n\nOpus 4.5 supports advanced tool use, extended context management, and coordinated multi-agent setups, making it well-suited for autonomous research, debugging, multi-step planning, and spreadsheet/browser manipulation. It delivers substantial gains in structured reasoning, execution reliability, and alignment compared to prior Opus generations, while reducing token overhead and improving performance on long-running tasks.",
    "context_length": 200000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "file",
        "image",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Claude",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000005",
      "completion": "0.000025",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.0000005",
      "input_cache_write": "0.00000625"
    },
    "top_provider": {
      "context_length": 200000,
      "max_completion_tokens": 32000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "verbosity"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "deepseek/deepseek-v3.2",
    "canonical_slug": "deepseek/deepseek-v3.2-20251201",
    "hugging_face_id": "deepseek-ai/DeepSeek-V3.2",
    "name": "DeepSeek: DeepSeek V3.2",
    "created": 1764594642,
    "description": "DeepSeek-V3.2 is a large language model designed to harmonize high computational efficiency with strong reasoning and agentic tool-use performance. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism that reduces training and inference cost while preserving quality in long-context scenarios. A scalable reinforcement learning post-training framework further improves reasoning, with reported performance in the GPT-5 class, and the model has demonstrated gold-medal results on the 2025 IMO and IOI. V3.2 also uses a large-scale agentic task synthesis pipeline to better integrate reasoning into tool-use settings, boosting compliance and generalization in interactive environments.\n\nUsers can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)",
    "context_length": 163840,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "DeepSeek",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000025",
      "completion": "0.00000038",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.00000019",
      "input_cache_write": "0"
    },
    "top_provider": {
      "context_length": 163840,
      "max_completion_tokens": 65536,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 1,
      "top_p": 0.95,
      "frequency_penalty": null
    }
  },
  {
    "id": "deepseek/deepseek-v3.2-speciale",
    "canonical_slug": "deepseek/deepseek-v3.2-speciale-20251201",
    "hugging_face_id": "deepseek-ai/DeepSeek-V3.2-Speciale",
    "name": "DeepSeek: DeepSeek V3.2 Speciale",
    "created": 1764594837,
    "description": "DeepSeek-V3.2-Speciale is a high-compute variant of DeepSeek-V3.2 optimized for maximum reasoning and agentic performance. It builds on DeepSeek Sparse Attention (DSA) for efficient long-context processing, then scales post-training reinforcement learning to push capability beyond the base model. Reported evaluations place Speciale ahead of GPT-5 on difficult reasoning workloads, with proficiency comparable to Gemini-3.0-Pro, while retaining strong coding and tool-use reliability. Like V3.2, it benefits from a large-scale agentic task synthesis pipeline that improves compliance and generalization in interactive environments.",
    "context_length": 163840,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "DeepSeek",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000027",
      "completion": "0.00000041",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 163840,
      "max_completion_tokens": 65536,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "include_reasoning",
      "logit_bias",
      "logprobs",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "reasoning",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "top_k",
      "top_logprobs",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 1,
      "top_p": 0.95,
      "frequency_penalty": null
    }
  },
  {
    "id": "mistralai/mistral-large-2512",
    "canonical_slug": "mistralai/mistral-large-2512",
    "hugging_face_id": "",
    "name": "Mistral: Mistral Large 3 2512",
    "created": 1764624472,
    "description": "Mistral Large 3 2512 is Mistral’s most capable model to date, featuring a sparse mixture-of-experts architecture with 41B active parameters (675B total), and released under the Apache 2.0 license.",
    "context_length": 262144,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000005",
      "completion": "0.0000015",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 262144,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "presence_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.0645,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "mistralai/ministral-3b-2512",
    "canonical_slug": "mistralai/ministral-3b-2512",
    "hugging_face_id": "mistralai/Ministral-3-3B-Instruct-2512",
    "name": "Mistral: Ministral 3 3B 2512",
    "created": 1764681560,
    "description": "The smallest model in the Ministral 3 family, Ministral 3 3B is a powerful, efficient tiny language model with vision capabilities.",
    "context_length": 131072,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000001",
      "completion": "0.0000001",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 131072,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "presence_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.3,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "mistralai/ministral-8b-2512",
    "canonical_slug": "mistralai/ministral-8b-2512",
    "hugging_face_id": "mistralai/Ministral-3-8B-Instruct-2512",
    "name": "Mistral: Ministral 3 8B 2512",
    "created": 1764681654,
    "description": "A balanced model in the Ministral 3 family, Ministral 3 8B is a powerful, efficient tiny language model with vision capabilities.",
    "context_length": 262144,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000015",
      "completion": "0.00000015",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 262144,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "presence_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.3,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "mistralai/ministral-14b-2512",
    "canonical_slug": "mistralai/ministral-14b-2512",
    "hugging_face_id": "mistralai/Ministral-3-14B-Instruct-2512",
    "name": "Mistral: Ministral 3 14B 2512",
    "created": 1764681735,
    "description": "The largest model in the Ministral 3 family, Ministral 3 14B offers frontier capabilities and performance comparable to its larger Mistral Small 3.2 24B counterpart. A powerful and efficient language model with vision capabilities.",
    "context_length": 262144,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.0000002",
      "completion": "0.0000002",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 262144,
      "max_completion_tokens": null,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "logit_bias",
      "max_tokens",
      "min_p",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.3,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5.1-codex-max",
    "canonical_slug": "openai/gpt-5.1-codex-max-20251204",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5.1-Codex-Max",
    "created": 1764878934,
    "description": "GPT-5.1-Codex-Max is OpenAI’s latest agentic coding model, designed for long-running, high-context software development tasks. It is based on an updated version of the 5.1 reasoning stack and trained on agentic workflows spanning software engineering, mathematics, and research. \nGPT-5.1-Codex-Max delivers faster performance, improved reasoning, and higher token efficiency across the development lifecycle. ",
    "context_length": 400000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "text",
        "image"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000125",
      "completion": "0.00001",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000125"
    },
    "top_provider": {
      "context_length": 400000,
      "max_completion_tokens": 128000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "mistralai/devstral-2512",
    "canonical_slug": "mistralai/devstral-2512",
    "hugging_face_id": "mistralai/Devstral-2-123B-Instruct-2512",
    "name": "Mistral: Devstral 2 2512",
    "created": 1765285419,
    "description": "Devstral 2 is a state-of-the-art open-source model by Mistral AI specializing in agentic coding. It is a 123B-parameter dense transformer model supporting a 256K context window.\n\nDevstral 2 supports exploring codebases and orchestrating changes across multiple files while maintaining architecture-level context. It tracks framework dependencies, detects failures, and retries with corrections—solving challenges like bug fixing and modernizing legacy systems. The model can be fine-tuned to prioritize specific languages or optimize for large enterprise codebases. It is available under a modified MIT license.",
    "context_length": 262144,
    "architecture": {
      "modality": "text->text",
      "input_modalities": [
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "Mistral",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000015",
      "completion": "0.0000006",
      "request": "0",
      "image": "0",
      "web_search": "0",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 262144,
      "max_completion_tokens": 65536,
      "is_moderated": false
    },
    "per_request_limits": null,
    "supported_parameters": [
      "frequency_penalty",
      "max_tokens",
      "presence_penalty",
      "repetition_penalty",
      "response_format",
      "seed",
      "stop",
      "structured_outputs",
      "temperature",
      "tool_choice",
      "tools",
      "top_k",
      "top_p"
    ],
    "default_parameters": {
      "temperature": 0.3,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5.2",
    "canonical_slug": "openai/gpt-5.2-20251211",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5.2",
    "created": 1765389775,
    "description": "GPT-5.2 is the latest frontier-grade model in the GPT-5 series, offering stronger agentic and long context perfomance compared to GPT-5.1. It uses adaptive reasoning to allocate computation dynamically, responding quickly to simple queries while spending more depth on complex tasks.\n\nBuilt for broad task coverage, GPT-5.2 delivers consistent gains across math, coding, sciende, and tool calling workloads, with more coherent long-form answers and improved tool-use reliability.",
    "context_length": 400000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "file",
        "image",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000175",
      "completion": "0.000014",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000175"
    },
    "top_provider": {
      "context_length": 400000,
      "max_completion_tokens": 128000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5.2-pro",
    "canonical_slug": "openai/gpt-5.2-pro-20251211",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5.2 Pro",
    "created": 1765389780,
    "description": "GPT-5.2 Pro is OpenAI’s most advanced model, offering major improvements in agentic coding and long context performance over GPT-5 Pro. It is optimized for complex tasks that require step-by-step reasoning, instruction following, and accuracy in high-stakes use cases. It supports test-time routing features and advanced prompt understanding, including user-specified intent like \"think hard about this.\" Improvements include reductions in hallucination, sycophancy, and better performance in coding, writing, and health-related tasks.",
    "context_length": 400000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "image",
        "text",
        "file"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.000021",
      "completion": "0.000168",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0"
    },
    "top_provider": {
      "context_length": 400000,
      "max_completion_tokens": 128000,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "include_reasoning",
      "max_tokens",
      "reasoning",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  },
  {
    "id": "openai/gpt-5.2-chat",
    "canonical_slug": "openai/gpt-5.2-chat-20251211",
    "hugging_face_id": "",
    "name": "OpenAI: GPT-5.2 Chat",
    "created": 1765389783,
    "description": "GPT-5.2 Chat (AKA Instant) is the fast, lightweight member of the 5.2 family, optimized for low-latency chat while retaining strong general intelligence. It uses adaptive reasoning to selectively “think” on harder queries, improving accuracy on math, coding, and multi-step tasks without slowing down typical conversations. The model is warmer and more conversational by default, with better instruction following and more stable short-form reasoning. GPT-5.2 Chat is designed for high-throughput, interactive workloads where responsiveness and consistency matter more than deep deliberation.",
    "context_length": 128000,
    "architecture": {
      "modality": "text+image->text",
      "input_modalities": [
        "file",
        "image",
        "text"
      ],
      "output_modalities": [
        "text"
      ],
      "tokenizer": "GPT",
      "instruct_type": null
    },
    "pricing": {
      "prompt": "0.00000175",
      "completion": "0.000014",
      "request": "0",
      "image": "0",
      "web_search": "0.01",
      "internal_reasoning": "0",
      "input_cache_read": "0.000000175"
    },
    "top_provider": {
      "context_length": 128000,
      "max_completion_tokens": 16384,
      "is_moderated": true
    },
    "per_request_limits": null,
    "supported_parameters": [
      "max_tokens",
      "response_format",
      "seed",
      "structured_outputs",
      "tool_choice",
      "tools"
    ],
    "default_parameters": {
      "temperature": null,
      "top_p": null,
      "frequency_penalty": null
    }
  }
]