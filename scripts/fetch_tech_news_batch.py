#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import yaml
import json
import os
import requests
import time
import re
from datetime import datetime, timezone, timedelta
from pathlib import Path
import logging
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import hashlib
import unicodedata
from typing import List, Dict, Optional, Tuple

# NaÄÃ­st .env soubor
load_dotenv()

# NastavenÃ­ logovÃ¡nÃ­
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BatchTechNewsManager:
    """PokroÄilÃ½ sprÃ¡vce tech news s batch pÅ™ekladem a LLM vyhodnocenÃ­m"""

    def __init__(self, config_file='_data/tech_news_sources.yaml'):
        """Inicializace s konfiguraÄnÃ­m souborem"""
        self.config_file = Path(config_file)
        self.config = self.load_config()
        self.cache_dir = Path('_cache')
        self.cache_dir.mkdir(exist_ok=True)

        # API klÃ­Äe
        self.openrouter_key = os.environ.get('OPENROUTER_API_KEY')
        if not self.openrouter_key or self.openrouter_key == "skip":
            logger.warning("âš ï¸ OpenRouter API klÃ­Ä nenÃ­ nastaven - pÅ™eklady budou pÅ™eskoÄeny")
            self.openrouter_key = None

    def load_config(self):
        """NaÄte konfiguraci ze YAML souboru"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                logger.info(f"âœ… NaÄtena konfigurace: {len(config['sources'])} zdrojÅ¯")
                return config
        except Exception as e:
            logger.error(f"âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ konfigurace: {e}")
            return {'sources': {}, 'settings': {}}

    def extract_image_from_content(self, entry):
        """Extrahuje obrÃ¡zek z rÅ¯znÃ½ch RSS formÃ¡tÅ¯"""
        image_url = None

        # 1. Media thumbnail (nejÄastÄ›jÅ¡Ã­)
        if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
            image_url = entry.media_thumbnail[0].get('url')

        # 2. Media content
        elif hasattr(entry, 'media_content') and entry.media_content:
            for media in entry.media_content:
                if media.get('type', '').startswith('image'):
                    image_url = media.get('url')
                    break

        # 3. Enclosure
        elif hasattr(entry, 'enclosures') and entry.enclosures:
            for enc in entry.enclosures:
                if enc.get('type', '').startswith('image'):
                    image_url = enc.get('href')
                    break

        # 4. Content HTML parsing
        if not image_url:
            content = entry.get('content', [{}])[0].get('value', '') if hasattr(entry, 'content') else ''
            if not content:
                content = entry.get('summary', '')

            if content:
                soup = BeautifulSoup(content, 'html.parser')
                img = soup.find('img')
                if img:
                    image_url = img.get('src')

        return image_url

    def clean_text(self, text, max_length=500):
        """VyÄistÃ­ text od HTML a zkrÃ¡tÃ­"""
        if not text:
            return ""

        # Odstranit HTML
        soup = BeautifulSoup(text, 'html.parser')
        clean = soup.get_text().strip()

        # Odstranit extra mezery
        clean = ' '.join(clean.split())

        # ZkrÃ¡tit na max dÃ©lku
        if len(clean) > max_length:
            clean = clean[:max_length] + "..."

        return clean

    def transliterate_to_ascii(self, text):
        """PÅ™evede text s diakritikou na ASCII-only verzi pro URL"""
        if not text:
            return ""

        # Normalize unicode characters
        nfkd_form = unicodedata.normalize('NFKD', text)
        # Filter out non-ASCII characters
        ascii_text = ''.join([c for c in nfkd_form if not unicodedata.combining(c)])

        # Replace remaining non-ASCII chars with safe alternatives
        replacements = {
            'Ä': 'c', 'Ä': 'd', 'Ä›': 'e', 'Åˆ': 'n', 'Å™': 'r', 'Å¡': 's', 'Å¥': 't', 'Å¯': 'u', 'Å¾': 'z',
            'ÄŒ': 'C', 'Ä': 'D', 'Äš': 'E', 'Å‡': 'N', 'Å˜': 'R', 'Å ': 'S', 'Å¤': 'T', 'Å®': 'U', 'Å½': 'Z',
            'Ã¡': 'a', 'Ã©': 'e', 'Ã­': 'i', 'Ã³': 'o', 'Ãº': 'u', 'Ã½': 'y',
            'Ã': 'A', 'Ã‰': 'E', 'Ã': 'I', 'Ã“': 'O', 'Ãš': 'U', 'Ã': 'Y'
        }

        for czech_char, ascii_char in replacements.items():
            ascii_text = ascii_text.replace(czech_char, ascii_char)

        return ascii_text

    def fetch_articles_from_all_sources(self) -> List[Dict]:
        """StÃ¡hne ÄlÃ¡nky ze vÅ¡ech RSS zdrojÅ¯"""
        all_articles = []

        for source_id, source_config in self.config['sources'].items():
            if not source_config.get('enabled', True):
                logger.debug(f"â­ï¸ PÅ™eskakuji vypnutÃ½ zdroj: {source_config['name']}")
                continue

            try:
                logger.info(f"ğŸ“¡ Stahuji RSS: {source_config['name']}...")

                feed = feedparser.parse(source_config['url'])

                if feed.bozo and not feed.entries:
                    logger.warning(f"âš ï¸ ProblÃ©m s {source_config['name']}: {feed.bozo_exception}")
                    continue

                max_articles = source_config.get('max_articles', 5)
                max_age_days = self.config['settings']['filters'].get('max_age_days', 7)
                cutoff_date = datetime.now(timezone.utc) - timedelta(days=max_age_days)

                for entry in feed.entries[:max_articles * 2]:
                    try:
                        title = entry.get('title', '').strip()
                        link = entry.get('link', '')

                        if len(title) < self.config['settings']['filters'].get('min_title_length', 10):
                            continue

                        exclude = self.config['settings']['filters'].get('exclude_keywords', [])
                        if any(kw.lower() in title.lower() for kw in exclude):
                            continue

                        # Datum
                        published = entry.get('published_parsed') or entry.get('updated_parsed')
                        if published:
                            pub_date = datetime.fromtimestamp(time.mktime(published), tz=timezone.utc)
                        else:
                            pub_date = datetime.now(timezone.utc)

                        if pub_date < cutoff_date:
                            continue

                        # Popis
                        description = entry.get('description') or entry.get('summary') or ''
                        description = self.clean_text(description)

                        # ObrÃ¡zek
                        image_url = self.extract_image_from_content(entry)

                        article = {
                            'title': title,
                            'description': description,
                            'url': link,
                            'source': {
                                'name': source_config['name'],
                                'id': source_id,
                                'emoji': source_config.get('emoji', 'ğŸ“°')
                            },
                            'publishedAt': pub_date.isoformat(),
                            'urlToImage': image_url,
                            'priority': source_config.get('priority', 3),
                            'language': source_config.get('language', 'en'),
                            'categories': source_config.get('category_focus', [])
                        }

                        all_articles.append(article)

                        if len(all_articles) >= max_articles:
                            break

                    except Exception as e:
                        logger.error(f"Chyba pÅ™i zpracovÃ¡nÃ­ ÄlÃ¡nku z {source_config['name']}: {e}")
                        continue

                logger.info(f"âœ… {source_config['name']}: {len([a for a in all_articles if a['source']['id'] == source_id])} ÄlÃ¡nkÅ¯")

            except Exception as e:
                logger.error(f"âŒ Selhalo stahovÃ¡nÃ­ {source_config['name']}: {e}")

        return all_articles

    def batch_translate_and_evaluate(self, articles: List[Dict]) -> List[Dict]:
        """Batch pÅ™ekladÃ¡ ÄlÃ¡nky a vyhodnocuje jejich dÅ¯leÅ¾itost pomocÃ­ LLM"""
        if not self.openrouter_key:
            logger.warning("âš ï¸ PÅ™eskakuji pÅ™eklad - API klÃ­Ä nenÃ­ nastaven")
            return articles

        # PÅ™ipravit batch data pro LLM
        batch_data = []
        for i, article in enumerate(articles):
            batch_data.append({
                "id": i,
                "title": article['title'],
                "description": article['description']
            })

        # StrukturovanÃ½ prompt pro batch zpracovÃ¡nÃ­
        system_prompt = """Jsi expert na technologickÃ© zprÃ¡vy. TvÃ½m Ãºkolem je:
1. PÅ™eloÅ¾it kaÅ¾dÃ½ ÄlÃ¡nek do ÄeÅ¡tiny (zachovat faktickou pÅ™esnost)
2. Vyhodnotit dÅ¯leÅ¾itost ÄlÃ¡nku na Å¡kÃ¡le 1-5:
   - 1: BÄ›Å¾nÃ© novinky, drobnÃ© aktualizace
   - 2: ZajÃ­mavÃ©, ale ne kritickÃ© informace
   - 3: VÃ½znamnÃ© novinky v oboru
   - 4: Velmi dÅ¯leÅ¾itÃ© udÃ¡losti (velkÃ© akvizice, prÅ¯lomy)
   - 5: PÅ™evratnÃ© udÃ¡losti (zÃ¡sadnÃ­ prÅ¯lomy, mega-akvizice, bezpeÄnostnÃ­ krizÃ­)

3. Identifikovat primÃ¡rnÃ­ kategorii ÄlÃ¡nku:
   - ai: UmÄ›lÃ¡ inteligence a machine learning
   - startups: Startupy, investice, podnikÃ¡nÃ­
   - hardware: Hardware, Äipy, zaÅ™Ã­zenÃ­
   - mobile: MobilnÃ­ technologie, aplikace
   - security: KybernetickÃ¡ bezpeÄnost, soukromÃ­
   - science: VÃ½zkum, vÄ›da, technologie
   - programming: VÃ½voj software, programovÃ¡nÃ­
   - gaming: HernÃ­ prÅ¯mysl
   - social: SociÃ¡lnÃ­ sÃ­tÄ›, platformy
   - business: ObchodnÃ­ dohody, firemnÃ­ strategie

VraÅ¥ odpovÄ›Ä jako JSON array, kde kaÅ¾dÃ½ objekt obsahuje:
{
  "id": ÄÃ­slo_ÄlÃ¡nku,
  "title_cs": "ÄeskÃ½_pÅ™eklad_titulku",
  "description_cs": "ÄeskÃ½_pÅ™eklad_popisu",
  "importance": ÄÃ­slo_1_aÅ¾_5,
  "category": "kategorie"
}"""

        user_prompt = f"""Zpracuj tyto technologickÃ© ÄlÃ¡nky:

{json.dumps(batch_data, ensure_ascii=False, indent=2)}

VraÅ¥ odpovÄ›Ä jako JSON array s pÅ™eklady a vyhodnocenÃ­m."""

        try:
            logger.info(f"ğŸ¤– PosÃ­lÃ¡m {len(articles)} ÄlÃ¡nkÅ¯ na batch zpracovÃ¡nÃ­...")

            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.openrouter_key}",
                    "HTTP-Referer": "https://marigold.cz",
                    "X-Title": "Marigold Tech News",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "qwen/qwen3-max",
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.3,
                    "max_tokens": 8000
                },
                timeout=60
            )

            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content'].strip()

                # Extrahovat JSON z odpovÄ›di
                if content.startswith('```json'):
                    content = content[7:-3].strip()
                elif content.startswith('```'):
                    content = content[3:-3].strip()

                processed_data = json.loads(content)

                logger.info(f"âœ… ÃšspÄ›Å¡nÄ› zpracovÃ¡no {len(processed_data)} ÄlÃ¡nkÅ¯")

                # Aplikovat vÃ½sledky na ÄlÃ¡nky
                for item in processed_data:
                    article_id = item['id']
                    if article_id < len(articles):
                        articles[article_id].update({
                            'title_cs': item.get('title_cs', articles[article_id]['title']),
                            'description_cs': item.get('description_cs', articles[article_id]['description']),
                            'importance': item.get('importance', 3),
                            'category': item.get('category', 'tech')
                        })

                return articles

            elif response.status_code == 429:
                logger.error("âŒ Rate limit pÅ™ekroÄen - zkus pozdÄ›ji")
                wait_time = int(response.headers.get('retry-after', 300))
                logger.info(f"â° ÄŒekÃ¡m {wait_time} sekund...")
                time.sleep(wait_time)
                return articles
            else:
                logger.error(f"âŒ API chyba: {response.status_code} - {response.text}")
                return articles

        except json.JSONDecodeError as e:
            logger.error(f"âŒ Chyba pÅ™i parsovÃ¡nÃ­ JSON odpovÄ›di: {e}")
            return articles
        except Exception as e:
            logger.error(f"âŒ Chyba pÅ™i batch zpracovÃ¡nÃ­: {e}")
            return articles

    def save_articles_to_jekyll(self, articles: List[Dict]):
        """UloÅ¾Ã­ ÄlÃ¡nky do Jekyll collections"""
        output_dir = Path('_tech_news')
        output_dir.mkdir(exist_ok=True)

        # VyÄistit starÃ© ÄlÃ¡nky
        for old_file in output_dir.glob('*.md'):
            old_file.unlink()

        for i, article in enumerate(articles, 1):
            try:
                # PouÅ¾Ã­t ÄeskÃ½ pÅ™eklad pokud existuje
                title = article.get('title_cs', article['title'])
                description = article.get('description_cs', article['description'])

                # VytvoÅ™it filename safe nÃ¡zev bez diakritiky
                ascii_title = self.transliterate_to_ascii(title)
                safe_title = re.sub(r'[^\w\s-]', '', ascii_title).strip()
                safe_title = re.sub(r'[-\s]+', '-', safe_title)[:50]

                date_str = datetime.now().strftime('%Y-%m-%d')
                filename = f"{date_str}-{i:02d}-{safe_title}.md"

                # Front matter
                front_matter = {
                    'layout': 'tech_news_article',
                    'title': title,
                    'description': description,
                    'original_title': article['title'],
                    'url': article['url'],
                    'source': article['source'],
                    'publishedAt': article['publishedAt'],
                    'importance': article.get('importance', 3),
                    'category': article.get('category', 'tech'),
                    'urlToImage': article.get('urlToImage'),
                    'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }

                # Generovat markdown soubor
                content = f"""---
{yaml.dump(front_matter, default_flow_style=False, allow_unicode=True)}---

{description}

[ÄŒÃ­st pÅ¯vodnÃ­ ÄlÃ¡nek]({article['url']})

**Zdroj:** {article['source']['emoji']} {article['source']['name']}
"""

                file_path = output_dir / filename
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)

                logger.debug(f"ğŸ’¾ UloÅ¾en ÄlÃ¡nek: {filename}")

            except Exception as e:
                logger.error(f"âŒ Chyba pÅ™i uklÃ¡dÃ¡nÃ­ ÄlÃ¡nku {i}: {e}")

    def run_full_pipeline(self):
        """SpustÃ­ celÃ½ pipeline: staÅ¾enÃ­ -> batch zpracovÃ¡nÃ­ -> uloÅ¾enÃ­"""
        logger.info("ğŸš€ SpouÅ¡tÃ­m kompletnÃ­ tech news pipeline...")

        # 1. StÃ¡hnout ÄlÃ¡nky
        articles = self.fetch_articles_from_all_sources()
        if not articles:
            logger.warning("âš ï¸ Å½Ã¡dnÃ© ÄlÃ¡nky k zpracovÃ¡nÃ­")
            return

        logger.info(f"ğŸ“š Celkem naÄteno {len(articles)} ÄlÃ¡nkÅ¯")

        # 2. Batch zpracovÃ¡nÃ­ (pÅ™eklad + vyhodnocenÃ­)
        processed_articles = self.batch_translate_and_evaluate(articles)

        # 3. SeÅ™adit podle dÅ¯leÅ¾itosti a data
        processed_articles.sort(key=lambda x: (-x.get('importance', 3), x['publishedAt']), reverse=True)

        # 4. Omezit poÄet ÄlÃ¡nkÅ¯
        max_total = self.config['settings'].get('total_max_articles', 30)
        processed_articles = processed_articles[:max_total]

        # 5. UloÅ¾it do Jekyll
        self.save_articles_to_jekyll(processed_articles)

        logger.info(f"âœ… Pipeline dokonÄen: {len(processed_articles)} ÄlÃ¡nkÅ¯ uloÅ¾eno")

        # Statistiky
        importance_stats = {}
        category_stats = {}
        for article in processed_articles:
            imp = article.get('importance', 3)
            cat = article.get('category', 'tech')
            importance_stats[imp] = importance_stats.get(imp, 0) + 1
            category_stats[cat] = category_stats.get(cat, 0) + 1

        logger.info(f"ğŸ“Š Statistiky dÅ¯leÅ¾itosti: {importance_stats}")
        logger.info(f"ğŸ“Š Statistiky kategoriÃ­: {category_stats}")

def main():
    """HlavnÃ­ funkce"""
    manager = BatchTechNewsManager()
    manager.run_full_pipeline()

if __name__ == "__main__":
    main()