#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import feedparser
import yaml
import json
import os
import requests
import time
import re
from datetime import datetime, timezone, timedelta
from pathlib import Path
import logging
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import hashlib
import unicodedata
from typing import List, Dict, Optional, Tuple

# Naƒç√≠st .env soubor
load_dotenv()

# Nastaven√≠ logov√°n√≠
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BatchTechNewsManager:
    """Pokroƒçil√Ω spr√°vce tech news s batch p≈ôekladem a LLM vyhodnocen√≠m"""

    def __init__(self, config_file='_data/tech_news_sources.yaml'):
        """Inicializace s konfiguraƒçn√≠m souborem"""
        self.config_file = Path(config_file)
        self.config = self.load_config()
        self.cache_dir = Path('_cache')
        self.cache_dir.mkdir(exist_ok=True)

        # API kl√≠ƒçe
        self.openrouter_key = os.environ.get('OPENROUTER_API_KEY')
        if not self.openrouter_key or self.openrouter_key == "skip":
            logger.warning("‚ö†Ô∏è OpenRouter API kl√≠ƒç nen√≠ nastaven - p≈ôeklady budou p≈ôeskoƒçeny")
            self.openrouter_key = None

    def load_config(self):
        """Naƒçte konfiguraci ze YAML souboru"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                logger.info(f"‚úÖ Naƒçtena konfigurace: {len(config['sources'])} zdroj≈Ø")
                return config
        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi naƒç√≠t√°n√≠ konfigurace: {e}")
            return {'sources': {}, 'settings': {}}

    def extract_image_from_content(self, entry):
        """Extrahuje obr√°zek z r≈Øzn√Ωch RSS form√°t≈Ø"""
        image_url = None

        # 1. Media thumbnail (nejƒçastƒõj≈°√≠)
        if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
            image_url = entry.media_thumbnail[0].get('url')

        # 2. Media content
        elif hasattr(entry, 'media_content') and entry.media_content:
            for media in entry.media_content:
                if media.get('type', '').startswith('image'):
                    image_url = media.get('url')
                    break

        # 3. Enclosure
        elif hasattr(entry, 'enclosures') and entry.enclosures:
            for enc in entry.enclosures:
                if enc.get('type', '').startswith('image'):
                    image_url = enc.get('href')
                    break

        # 4. Content HTML parsing
        if not image_url:
            content = entry.get('content', [{}])[0].get('value', '') if hasattr(entry, 'content') else ''
            if not content:
                content = entry.get('summary', '')

            if content:
                soup = BeautifulSoup(content, 'html.parser')
                img = soup.find('img')
                if img:
                    image_url = img.get('src')

        return image_url

    def clean_text(self, text, max_length=500):
        """Vyƒçist√≠ text od HTML a zkr√°t√≠"""
        if not text:
            return ""

        # Odstranit HTML
        soup = BeautifulSoup(text, 'html.parser')
        clean = soup.get_text().strip()

        # Odstranit extra mezery
        clean = ' '.join(clean.split())

        # Zkr√°tit na max d√©lku
        if len(clean) > max_length:
            clean = clean[:max_length] + "..."

        return clean

    def transliterate_to_ascii(self, text):
        """P≈ôevede text s diakritikou na ASCII-only verzi pro URL"""
        if not text:
            return ""

        # Normalize unicode characters
        nfkd_form = unicodedata.normalize('NFKD', text)
        # Filter out non-ASCII characters
        ascii_text = ''.join([c for c in nfkd_form if not unicodedata.combining(c)])

        # Replace remaining non-ASCII chars with safe alternatives
        replacements = {
            'ƒç': 'c', 'ƒè': 'd', 'ƒõ': 'e', '≈à': 'n', '≈ô': 'r', '≈°': 's', '≈•': 't', '≈Ø': 'u', '≈æ': 'z',
            'ƒå': 'C', 'ƒé': 'D', 'ƒö': 'E', '≈á': 'N', '≈ò': 'R', '≈†': 'S', '≈§': 'T', '≈Æ': 'U', '≈Ω': 'Z',
            '√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u', '√Ω': 'y',
            '√Å': 'A', '√â': 'E', '√ç': 'I', '√ì': 'O', '√ö': 'U', '√ù': 'Y'
        }

        for czech_char, ascii_char in replacements.items():
            ascii_text = ascii_text.replace(czech_char, ascii_char)

        return ascii_text

    def fetch_articles_from_all_sources(self) -> List[Dict]:
        """St√°hne ƒçl√°nky ze v≈°ech RSS zdroj≈Ø"""
        all_articles = []

        for source_id, source_config in self.config['sources'].items():
            if not source_config.get('enabled', True):
                logger.debug(f"‚è≠Ô∏è P≈ôeskakuji vypnut√Ω zdroj: {source_config['name']}")
                continue

            try:
                logger.info(f"üì° Stahuji RSS: {source_config['name']}...")

                feed = feedparser.parse(source_config['url'])

                if feed.bozo and not feed.entries:
                    logger.warning(f"‚ö†Ô∏è Probl√©m s {source_config['name']}: {feed.bozo_exception}")
                    continue

                max_articles = source_config.get('max_articles', 5)
                max_age_days = self.config['settings']['filters'].get('max_age_days', 7)
                cutoff_date = datetime.now(timezone.utc) - timedelta(days=max_age_days)

                for entry in feed.entries[:max_articles * 2]:
                    try:
                        title = entry.get('title', '').strip()
                        link = entry.get('link', '')

                        if len(title) < self.config['settings']['filters'].get('min_title_length', 10):
                            continue

                        exclude = self.config['settings']['filters'].get('exclude_keywords', [])
                        if any(kw.lower() in title.lower() for kw in exclude):
                            continue

                        # Datum
                        published = entry.get('published_parsed') or entry.get('updated_parsed')
                        if published:
                            pub_date = datetime.fromtimestamp(time.mktime(published), tz=timezone.utc)
                        else:
                            pub_date = datetime.now(timezone.utc)

                        if pub_date < cutoff_date:
                            continue

                        # Popis
                        description = entry.get('description') or entry.get('summary') or ''
                        description = self.clean_text(description)

                        # Obr√°zek
                        image_url = self.extract_image_from_content(entry)

                        article = {
                            'title': title,
                            'description': description,
                            'url': link,
                            'source': {
                                'name': source_config['name'],
                                'id': source_id,
                                'emoji': source_config.get('emoji', 'üì∞')
                            },
                            'publishedAt': pub_date.isoformat(),
                            'urlToImage': image_url,
                            'priority': source_config.get('priority', 3),
                            'language': source_config.get('language', 'en'),
                            'categories': source_config.get('category_focus', [])
                        }

                        all_articles.append(article)

                        if len(all_articles) >= max_articles:
                            break

                    except Exception as e:
                        logger.error(f"Chyba p≈ôi zpracov√°n√≠ ƒçl√°nku z {source_config['name']}: {e}")
                        continue

                logger.info(f"‚úÖ {source_config['name']}: {len([a for a in all_articles if a['source']['id'] == source_id])} ƒçl√°nk≈Ø")

            except Exception as e:
                logger.error(f"‚ùå Selhalo stahov√°n√≠ {source_config['name']}: {e}")

        return all_articles

    def batch_translate_and_evaluate(self, articles: List[Dict]) -> List[Dict]:
        """Batch p≈ôeklad√° ƒçl√°nky a vyhodnocuje jejich d≈Øle≈æitost pomoc√≠ LLM"""
        if not self.openrouter_key:
            logger.warning("‚ö†Ô∏è P≈ôeskakuji p≈ôeklad - API kl√≠ƒç nen√≠ nastaven")
            return articles

        # P≈ôipravit batch data pro LLM
        batch_data = []
        for i, article in enumerate(articles):
            batch_data.append({
                "id": i,
                "title": article['title'],
                "description": article['description']
            })

        # Strukturovan√Ω prompt pro batch zpracov√°n√≠
        system_prompt = """Jsi expert na technologick√© zpr√°vy. Tv√Ωm √∫kolem je:
1. P≈ôelo≈æit ka≈æd√Ω ƒçl√°nek do ƒçe≈°tiny (zachovat faktickou p≈ôesnost)
2. Vyhodnotit d≈Øle≈æitost ƒçl√°nku na ≈°k√°le 1-5:
   - 1: Bƒõ≈æn√© novinky, drobn√© aktualizace
   - 2: Zaj√≠mav√©, ale ne kritick√© informace
   - 3: V√Ωznamn√© novinky v oboru
   - 4: Velmi d≈Øle≈æit√© ud√°losti (velk√© akvizice, pr≈Ølomy)
   - 5: P≈ôevratn√© ud√°losti (z√°sadn√≠ pr≈Ølomy, mega-akvizice, bezpeƒçnostn√≠ kriz√≠)

3. Identifikovat prim√°rn√≠ kategorii ƒçl√°nku:
   - ai: Umƒõl√° inteligence a machine learning
   - startups: Startupy, investice, podnik√°n√≠
   - hardware: Hardware, ƒçipy, za≈ô√≠zen√≠
   - mobile: Mobiln√≠ technologie, aplikace
   - security: Kybernetick√° bezpeƒçnost, soukrom√≠
   - science: V√Ωzkum, vƒõda, technologie
   - programming: V√Ωvoj software, programov√°n√≠
   - gaming: Hern√≠ pr≈Ømysl
   - social: Soci√°ln√≠ s√≠tƒõ, platformy
   - business: Obchodn√≠ dohody, firemn√≠ strategie

Vra≈• odpovƒõƒè jako JSON array, kde ka≈æd√Ω objekt obsahuje:
{
  "id": ƒç√≠slo_ƒçl√°nku,
  "title_cs": "ƒçesk√Ω_p≈ôeklad_titulku",
  "description_cs": "ƒçesk√Ω_p≈ôeklad_popisu",
  "importance": ƒç√≠slo_1_a≈æ_5,
  "category": "kategorie"
}"""

        user_prompt = f"""Zpracuj tyto technologick√© ƒçl√°nky:

{json.dumps(batch_data, ensure_ascii=False, indent=2)}

Vra≈• odpovƒõƒè jako JSON array s p≈ôeklady a vyhodnocen√≠m."""

        try:
            logger.info(f"ü§ñ Pos√≠l√°m {len(articles)} ƒçl√°nk≈Ø na batch zpracov√°n√≠...")

            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.openrouter_key}",
                    "HTTP-Referer": "https://marigold.cz",
                    "X-Title": "Marigold Tech News",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "deepseek/deepseek-chat",
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.3,
                    "max_tokens": 8000
                },
                timeout=60
            )

            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content'].strip()

                # Extrahovat JSON z odpovƒõdi
                if content.startswith('```json'):
                    content = content[7:-3].strip()
                elif content.startswith('```'):
                    content = content[3:-3].strip()

                processed_data = json.loads(content)

                logger.info(f"‚úÖ √öspƒõ≈°nƒõ zpracov√°no {len(processed_data)} ƒçl√°nk≈Ø")

                # Aplikovat v√Ωsledky na ƒçl√°nky
                for item in processed_data:
                    article_id = item['id']
                    if article_id < len(articles):
                        articles[article_id].update({
                            'title_cs': item.get('title_cs', articles[article_id]['title']),
                            'description_cs': item.get('description_cs', articles[article_id]['description']),
                            'importance': item.get('importance', 3),
                            'category': item.get('category', 'tech')
                        })

                return articles

            elif response.status_code == 429:
                logger.error("‚ùå Rate limit p≈ôekroƒçen - zkus pozdƒõji")
                wait_time = int(response.headers.get('retry-after', 300))
                logger.info(f"‚è∞ ƒåek√°m {wait_time} sekund...")
                time.sleep(wait_time)
                return articles
            else:
                logger.error(f"‚ùå API chyba: {response.status_code} - {response.text}")
                return articles

        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Chyba p≈ôi parsov√°n√≠ JSON odpovƒõdi: {e}")
            return articles
        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi batch zpracov√°n√≠: {e}")
            return articles

    def save_articles_to_jekyll(self, articles: List[Dict]):
        """Ulo≈æ√≠ ƒçl√°nky do Jekyll collections"""
        output_dir = Path('_tech_news')
        output_dir.mkdir(exist_ok=True)

        # Vyƒçistit star√© ƒçl√°nky
        for old_file in output_dir.glob('*.md'):
            old_file.unlink()

        for i, article in enumerate(articles, 1):
            try:
                # Pou≈æ√≠t ƒçesk√Ω p≈ôeklad pokud existuje
                title = article.get('title_cs', article['title'])
                description = article.get('description_cs', article['description'])

                # Vytvo≈ôit filename safe n√°zev bez diakritiky
                ascii_title = self.transliterate_to_ascii(title)
                safe_title = re.sub(r'[^\w\s-]', '', ascii_title).strip()
                safe_title = re.sub(r'[-\s]+', '-', safe_title)[:50]

                date_str = datetime.now().strftime('%Y-%m-%d')
                filename = f"{date_str}-{i:02d}-{safe_title}.md"

                # Front matter
                front_matter = {
                    'layout': 'tech_news_article',
                    'title': title,
                    'description': description,
                    'original_title': article['title'],
                    'url': article['url'],
                    'source': article['source'],
                    'publishedAt': article['publishedAt'],
                    'importance': article.get('importance', 3),
                    'category': article.get('category', 'tech'),
                    'urlToImage': article.get('urlToImage'),
                    'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }

                # Generovat markdown soubor
                content = f"""---
{yaml.dump(front_matter, default_flow_style=False, allow_unicode=True)}---

{description}

[ƒå√≠st p≈Øvodn√≠ ƒçl√°nek]({article['url']})

**Zdroj:** {article['source']['emoji']} {article['source']['name']}
"""

                file_path = output_dir / filename
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)

                logger.debug(f"üíæ Ulo≈æen ƒçl√°nek: {filename}")

            except Exception as e:
                logger.error(f"‚ùå Chyba p≈ôi ukl√°d√°n√≠ ƒçl√°nku {i}: {e}")

    def run_full_pipeline(self):
        """Spust√≠ cel√Ω pipeline: sta≈æen√≠ -> batch zpracov√°n√≠ -> ulo≈æen√≠"""
        logger.info("üöÄ Spou≈°t√≠m kompletn√≠ tech news pipeline...")

        # 1. St√°hnout ƒçl√°nky
        articles = self.fetch_articles_from_all_sources()
        if not articles:
            logger.warning("‚ö†Ô∏è ≈Ω√°dn√© ƒçl√°nky k zpracov√°n√≠")
            return

        logger.info(f"üìö Celkem naƒçteno {len(articles)} ƒçl√°nk≈Ø")

        # 2. Batch zpracov√°n√≠ (p≈ôeklad + vyhodnocen√≠)
        processed_articles = self.batch_translate_and_evaluate(articles)

        # 3. Se≈ôadit podle d≈Øle≈æitosti a data
        processed_articles.sort(key=lambda x: (-x.get('importance', 3), x['publishedAt']), reverse=True)

        # 4. Omezit poƒçet ƒçl√°nk≈Ø
        max_total = self.config['settings'].get('total_max_articles', 30)
        processed_articles = processed_articles[:max_total]

        # 5. Ulo≈æit do Jekyll
        self.save_articles_to_jekyll(processed_articles)

        logger.info(f"‚úÖ Pipeline dokonƒçen: {len(processed_articles)} ƒçl√°nk≈Ø ulo≈æeno")

        # Statistiky
        importance_stats = {}
        category_stats = {}
        for article in processed_articles:
            imp = article.get('importance', 3)
            cat = article.get('category', 'tech')
            importance_stats[imp] = importance_stats.get(imp, 0) + 1
            category_stats[cat] = category_stats.get(cat, 0) + 1

        logger.info(f"üìä Statistiky d≈Øle≈æitosti: {importance_stats}")
        logger.info(f"üìä Statistiky kategori√≠: {category_stats}")

def main():
    """Hlavn√≠ funkce"""
    manager = BatchTechNewsManager()
    manager.run_full_pipeline()

if __name__ == "__main__":
    main()