#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import yaml
import json
import os
import re
import requests
from datetime import datetime, timezone
from pathlib import Path
import logging
from dotenv import load_dotenv
from bs4 import BeautifulSoup

# Naƒç√≠st .env soubor
load_dotenv()

# Nastaven√≠ logov√°n√≠
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NewsAPITechNewsGenerator:
    """Gener√°tor tech-news z NewsAPI s p≈ôeklady a detekc√≠"""

    def __init__(self):
        self.output_dir = Path('_tech_news')
        self.output_dir.mkdir(exist_ok=True)
        self.news_api_key = os.getenv('NEWS_API_KEY', '')
        self.openrouter_api_key = os.getenv('OPENROUTER_API_KEY', '')
        self.translation_enabled = self.openrouter_api_key and self.openrouter_api_key != 'skip'

    def fetch_article_content(self, url, max_length=2000):
        """St√°hne a parsuje pln√Ω text ƒçl√°nku z URL"""
        try:
            logger.debug(f"üìÑ Stahuji obsah ƒçl√°nku: {url[:50]}...")

            headers = {
                'User-Agent': 'Mozilla/5.0 (Marigold.cz Tech News Bot; +https://marigold.cz)'
            }

            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code != 200:
                logger.warning(f"‚ö†Ô∏è HTTP {response.status_code} pro {url}")
                return None

            soup = BeautifulSoup(response.text, 'html.parser')

            # Odstranit skripty, styly a navigaci
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()

            # Pokusit se naj√≠t hlavn√≠ obsah ƒçl√°nku
            article_content = None

            # Zkusit bƒõ≈æn√© selektory pro obsah ƒçl√°nku
            for selector in ['article', 'main', '.article-content', '.post-content', '.entry-content']:
                content = soup.select_one(selector)
                if content:
                    article_content = content
                    break

            # Pokud nenalezeno, pou≈æ√≠t cel√Ω body
            if not article_content:
                article_content = soup.body

            if not article_content:
                logger.warning(f"‚ö†Ô∏è Nepoda≈ôilo se extrahovat obsah z {url}")
                return None

            # Z√≠skat text a vyƒçistit
            text = article_content.get_text(separator=' ', strip=True)

            # Vyƒçistit whitespace
            text = re.sub(r'\s+', ' ', text)

            # Omezit d√©lku
            if len(text) > max_length:
                text = text[:max_length] + '...'

            logger.debug(f"‚úÖ Extrahov√°no {len(text)} znak≈Ø")
            return text

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Chyba p≈ôi stahov√°n√≠ ƒçl√°nku {url}: {e}")
            return None

    def analyze_and_enhance_article(self, url, title, description, category):
        """Kombinovan√° LLM anal√Ωza: detekce d≈Øle≈æitosti + generov√°n√≠ roz≈°√≠≈ôen√©ho obsahu"""
        if not self.translation_enabled:
            # Fallback bez LLM
            return {
                'importance': 3,
                'czech_title': title,
                'czech_description': description,
                'enhanced_content': description
            }

        try:
            # St√°hnout pln√Ω ƒçl√°nek
            full_text = self.fetch_article_content(url)

            # P≈ôipravit kontext pro LLM
            article_context = f"""
NADPIS: {title}
POPIS: {description}
KATEGORIE: {category}
"""

            if full_text:
                article_context += f"\nPLN√ù TEXT (zkr√°ceno): {full_text}\n"

            # Kombinovan√Ω prompt pro d≈Øle≈æitost + obsah
            prompt = f"""Analyzuj tento technologick√Ω ƒçl√°nek a vytvo≈ô ƒçesk√Ω obsah.

{article_context}

√öKOL 1 - D≈ÆLE≈ΩITOST (1-5):
5 = Pr≈Ølomov√© (AGI, kvantov√© poƒç√≠taƒçe, akvizice $1B+, bezpeƒçnostn√≠ krize, shutdown velk√Ωch slu≈æeb)
4 = Velmi d≈Øle≈æit√© (nov√© produkty Apple/Google/Microsoft/Meta/OpenAI, v√Ωznamn√° partnerstv√≠, IPO, funding $100M+)
3 = Zaj√≠mav√© (bƒõ≈æn√© novinky, updaty, zaj√≠mav√© technologie, novinky od zn√°m√Ωch firem)
2 = Spekulace (rumors, leaky, "mo≈æn√°", "√∫dajnƒõ", "sources say")
1 = Ned≈Øle≈æit√© (trivi√°ln√≠ novinky, clickbait)

√öKOL 2 - ƒåESK√ù OBSAH:
- Pokud d≈Øle≈æitost ‚â• 3: Vytvo≈ô strukturovan√Ω ƒçl√°nek (400-600 slov)
- Pokud d≈Øle≈æitost < 3: Vytvo≈ô pouze kr√°tk√© shrnut√≠ (100-150 slov)

Pro d≈Øle≈æit√© ƒçl√°nky (‚â•3) pou≈æij strukturu:
## Souhrn
[2-3 vƒõty s podstatou novinky]

## Kl√≠ƒçov√© body
- [3-5 nejd≈Øle≈æitƒõj≈°√≠ch bod≈Ø]

## Podrobnosti
[200-300 slov s detaily, kontextem a souvislostmi. Vysvƒõtli, co to znamen√° pro u≈æivatele/pr≈Ømysl.]

## Proƒç je to d≈Øle≈æit√©
[Dopady, kontext v ≈°ir≈°√≠m technologick√©m ekosyst√©mu]

FORM√ÅT ODPOVƒöDI (JSON):
{{
  "importance": 3,
  "czech_title": "P≈ôelo≈æen√Ω titulek",
  "czech_description": "P≈ôelo≈æen√Ω kr√°tk√Ω popis (1-2 vƒõty)",
  "enhanced_content": "Roz≈°√≠≈ôen√Ω obsah v markdown form√°tu"
}}

D≈ÆLE≈ΩIT√â:
- Technick√© term√≠ny zachovej v angliƒçtinƒõ (AI, API, GPU, atd.)
- Buƒè konkr√©tn√≠, ne obecn√Ω
- Odpovƒõz POUZE validn√≠m JSON, bez jak√©hokoli dal≈°√≠ho textu
"""

            headers = {
                'Authorization': f'Bearer {self.openrouter_api_key}',
                'Content-Type': 'application/json'
            }

            data = {
                'model': 'anthropic/claude-sonnet-4.5',
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 2000,  # Dostatek pro dlouh√Ω obsah
                'temperature': 0.3
            }

            logger.debug(f"ü§ñ Vol√°m LLM pro anal√Ωzu ƒçl√°nku: {title[:50]}...")

            response = requests.post(
                'https://openrouter.ai/api/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('choices') and len(result['choices']) > 0:
                    content = result['choices'][0]['message']['content'].strip()

                    # Odstranit p≈ô√≠padn√© markdown code bloky
                    content = re.sub(r'^```json\s*', '', content)
                    content = re.sub(r'\s*```$', '', content)

                    # Parsovat JSON
                    try:
                        analysis = json.loads(content)

                        importance = analysis.get('importance', 3)
                        logger.info(f"‚úÖ LLM anal√Ωza: d≈Øle≈æitost={importance}, d√©lka obsahu={len(analysis.get('enhanced_content', ''))} znak≈Ø")

                        return {
                            'importance': importance,
                            'czech_title': analysis.get('czech_title', title),
                            'czech_description': analysis.get('czech_description', description),
                            'enhanced_content': analysis.get('enhanced_content', description)
                        }
                    except json.JSONDecodeError as e:
                        logger.warning(f"‚ö†Ô∏è Chyba parsov√°n√≠ JSON z LLM: {e}")
                        logger.warning(f"Odpovƒõƒè LLM: {content[:200]}...")
            else:
                logger.warning(f"‚ö†Ô∏è LLM API selhalo (HTTP {response.status_code})")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Chyba p≈ôi LLM anal√Ωze: {e}")

        # Fallback - pou≈æ√≠t star√© metody
        return {
            'importance': self.detect_importance(title, description, category),
            'czech_title': self.translate_title(title),
            'czech_description': self.translate_description(description),
            'enhanced_content': self.translate_description(description)
        }

    def fetch_newsapi_articles(self):
        """St√°hne ƒçl√°nky z NewsAPI"""
        if not self.news_api_key:
            logger.error("‚ùå NEWS_API_KEY nen√≠ nastaven!")
            return []

        url = "https://newsapi.org/v2/top-headlines"
        params = {
            'category': 'technology',
            'apiKey': self.news_api_key,
            'pageSize': 40,  # Max 40 ƒçl√°nk≈Ø
            'language': 'en'
        }

        try:
            logger.info("üì° Stahuji ƒçl√°nky z NewsAPI...")
            response = requests.get(url, params=params, timeout=30)

            if response.status_code == 200:
                data = response.json()
                if data['status'] == 'ok':
                    articles = data.get('articles', [])
                    logger.info(f"‚úÖ Sta≈æeno {len(articles)} ƒçl√°nk≈Ø z NewsAPI")

                    # Vyƒçistit a p≈ôipravit ƒçl√°nky
                    processed_articles = []
                    for article in articles:
                        # P≈ôeskoƒçit ƒçl√°nky bez podstatn√Ωch dat
                        if not article.get('title') or article['title'] == '[Removed]':
                            continue
                        if not article.get('url'):
                            continue

                        # P≈ôipravit zdroj
                        source_info = {
                            'id': article['source'].get('id', 'unknown'),
                            'name': article['source'].get('name', 'Unknown'),
                            'emoji': self.get_source_emoji(article['source'].get('name', ''))
                        }

                        processed_article = {
                            'title': article['title'],
                            'description': article.get('description', ''),
                            'url': article['url'],
                            'urlToImage': article.get('urlToImage'),
                            'publishedAt': article['publishedAt'],
                            'source': source_info,
                            'content': article.get('content', '')
                        }

                        processed_articles.append(processed_article)

                    return processed_articles
                else:
                    logger.error(f"‚ùå NewsAPI error: {data.get('message', 'Unknown')}")
            else:
                logger.error(f"‚ùå HTTP {response.status_code}: {response.text}")

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi stahov√°n√≠ z NewsAPI: {e}")

        return []

    def get_source_emoji(self, source_name):
        """Vr√°t√≠ emoji pro zdroj"""
        emoji_map = {
            'TechCrunch': 'üöÄ',
            'The Verge': '‚ö°',
            'Wired': 'üîß',
            'Ars Technica': 'üî¨',
            'MIT Technology Review': 'üéì',
            'OpenAI': 'ü§ñ',
            'Associated Press': 'üì∞',
            'Bloomberg': 'üíπ',
            'Forbes': 'üíº',
            'Axios': 'üì°',
            'CBS News': 'üì∫',
            'The Wall Street Journal': 'üìà',
        }

        for name, emoji in emoji_map.items():
            if name.lower() in source_name.lower():
                return emoji

        return 'üì∞'  # Default

    def create_jekyll_article(self, article, article_index):
        """Vytvo≈ô√≠ Jekyll ƒçl√°nek s optimalizovan√Ωm front matter"""

        # Vytvo≈ôit slug z titulku
        slug = self.create_slug(article['title'])

        # Datum pro filename - p≈ôev√©st na UTC
        pub_date_str = article['publishedAt']
        if pub_date_str.endswith('Z'):
            pub_date = datetime.fromisoformat(pub_date_str.replace('Z', '+00:00'))
        elif '+' in pub_date_str or pub_date_str.endswith('00:00'):
            pub_date = datetime.fromisoformat(pub_date_str)
        else:
            pub_date = datetime.fromisoformat(pub_date_str).replace(tzinfo=timezone.utc)

        # Zajistit UTC
        if pub_date.tzinfo is None:
            pub_date = pub_date.replace(tzinfo=timezone.utc)
        else:
            pub_date = pub_date.astimezone(timezone.utc)

        date_str = pub_date.strftime('%Y-%m-%d')

        filename = f"{date_str}-{slug}.md"
        filepath = self.output_dir / filename

        # Detekce kategorie nejd≈ô√≠v (pot≈ôebujeme pro LLM anal√Ωzu)
        category = self.detect_category(article['title'], article.get('description', ''))

        # Kombinovan√° LLM anal√Ωza: d≈Øle≈æitost + ƒçesk√Ω obsah
        analysis = self.analyze_and_enhance_article(
            article['url'],
            article['title'],
            article.get('description', ''),
            category
        )

        czech_title = analysis['czech_title']
        czech_description = analysis['czech_description']
        importance = analysis['importance']
        enhanced_content = analysis['enhanced_content']

        # Detekce firem a osobnost√≠ (zachov√°me pro metadata)
        companies = self.detect_companies(article['title'], article.get('description', ''))
        people = self.detect_people(article['title'], article.get('description', ''))

        # Front matter podle tech_news_article layoutu
        front_matter = {
            'layout': 'tech_news_article',
            'title': czech_title,
            'original_title': article['title'],
            'slug': slug,
            'description': czech_description,
            'publishedAt': pub_date.isoformat(),
            'date': pub_date.strftime('%Y-%m-%d %H:%M:%S'),
            'url': article['url'],
            'category': category,
            'importance': importance,
            'source': article['source']
        }

        # P≈ôidat firmy a osobnosti pouze pokud existuj√≠
        if companies:
            front_matter['companies'] = companies
        if people:
            front_matter['people'] = people

        # P≈ôidat urlToImage a urlToImageBackup pro fallback mechanismus
        if article.get('urlToImage'):
            front_matter['urlToImage'] = article['urlToImage']
            front_matter['urlToImageBackup'] = article['urlToImage']  # Backup URL pro p≈ô√≠pad lok√°ln√≠ho selh√°n√≠

        # Vytvo≈ôit obsah
        content = f"""---
{yaml.dump(front_matter, default_flow_style=False, allow_unicode=True)}---

{enhanced_content}

---

[ƒå√≠st p≈Øvodn√≠ ƒçl√°nek]({article['url']})

**Zdroj:** {article['source']['emoji']} {article['source']['name']}
"""

        # Zapsat soubor
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        logger.info(f"‚úÖ Vytvo≈ôen: {filename}")
        return filepath

    def create_slug(self, title):
        """Vytvo≈ô√≠ URL-friendly slug z titulku"""
        slug = re.sub(r'[^\w\s-]', '', title.lower())
        slug = re.sub(r'[-\s]+', '-', slug).strip('-')

        # Pokud slug zaƒç√≠n√° ƒç√≠slic√≠, p≈ôidat prefix "A-"
        if slug and slug[0].isdigit():
            slug = 'A-' + slug

        # Pokud je slug pr√°zdn√Ω, pou≈æ√≠t fallback
        if not slug:
            slug = 'article'

        return slug[:50]  # Omezit d√©lku

    def translate_text(self, text, text_type="text"):
        """P≈ôelo≈æ√≠ text pomoc√≠ OpenRouter API"""
        if not self.translation_enabled or not text or text.strip() == '':
            return text

        try:
            # R≈Øzn√© syst√©mov√© prompty podle typu textu
            system_prompts = {
                "title": "P≈ôekl√°dej technologick√© nadpisy ƒçl√°nk≈Ø z angliƒçtiny do ƒçe≈°tiny. Zachovej technick√© term√≠ny v angliƒçtinƒõ, pokud jsou bƒõ≈ænƒõ pou≈æ√≠van√©. Odpovƒõz POUZE p≈ôelo≈æen√Ωm nadpisem, bez jak√Ωchkoli dodateƒçn√Ωch text≈Ø jako 'Nadpis ƒçl√°nku v ƒçe≈°tinƒõ:' nebo uvozovek.",
                "description": "P≈ôekl√°dej perex/popis technologick√Ωch ƒçl√°nk≈Ø z angliƒçtiny do ƒçe≈°tiny. Zachovej technick√© term√≠ny v angliƒçtinƒõ, pokud jsou bƒõ≈ænƒõ pou≈æ√≠van√©. Odpovƒõz POUZE p≈ôelo≈æen√Ωm textem, bez jak√Ωchkoli dodateƒçn√Ωch koment√°≈ô≈Ø nebo uvozovek."
            }

            system_prompt = system_prompts.get(text_type, system_prompts["description"])

            headers = {
                'Authorization': f'Bearer {self.openrouter_api_key}',
                'Content-Type': 'application/json'
            }

            data = {
                'model': 'anthropic/claude-sonnet-4.5',
                'messages': [
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': text}
                ],
                'max_tokens': 200 if text_type == "title" else 500,
                'temperature': 0.3
            }

            response = requests.post(
                'https://openrouter.ai/api/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=10
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('choices') and len(result['choices']) > 0:
                    translated = result['choices'][0]['message']['content'].strip()
                    translated = translated.strip('"\'')
                    logger.debug(f"P≈ôeklad {text_type}: {text[:30]}... ‚Üí {translated[:30]}...")
                    return translated

            logger.warning(f"P≈ôeklad selhal (HTTP {response.status_code}), pou≈æ√≠v√°m origin√°l")

        except Exception as e:
            logger.warning(f"Chyba p≈ôekladu: {e}, pou≈æ√≠v√°m origin√°l")

        return text

    def translate_title(self, title):
        """P≈ôelo≈æ√≠ titulek"""
        return self.translate_text(title, "title")

    def translate_description(self, description):
        """P≈ôelo≈æ√≠ popis"""
        return self.translate_text(description or '', "description")

    def detect_category(self, title, description):
        """Detekuje kategorii ƒçl√°nku pomoc√≠ LLM"""
        if not self.translation_enabled:
            return 'tech'

        try:
            prompt = f"""P≈ôi≈ôaƒè tomuto technologick√©mu ƒçl√°nku jednu p≈ôesnou kategorii (1-2 slova v ƒçe≈°tinƒõ).

Nadpis: {title}
Popis: {description or 'Nen√≠ k dispozici'}

Kategorie by mƒõla b√Ωt:
- Struƒçn√° (1-2 slova)
- V ƒçe≈°tinƒõ
- Specifick√° pro obsah ƒçl√°nku
- Bez emoji

P≈ô√≠klady kategori√≠: AI, hardware, startupy, programov√°n√≠, mobiln√≠ aplikace, kybernetika, kosmonautika, elektromobilita, hern√≠ pr≈Ømysl, kryptomƒõny, j√≠dlo, zdrav√≠ atd.

Odpovƒõz POUZE n√°zvem kategorie, nic jin√©ho."""

            headers = {
                'Authorization': f'Bearer {self.openrouter_api_key}',
                'Content-Type': 'application/json'
            }

            data = {
                'model': 'anthropic/claude-sonnet-4.5',
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 20,
                'temperature': 0.1
            }

            response = requests.post(
                'https://openrouter.ai/api/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=10
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('choices') and len(result['choices']) > 0:
                    category = result['choices'][0]['message']['content'].strip()
                    category = category.lower().strip('"\'.,!?')
                    if len(category) > 20:
                        category = category[:20]

                    logger.debug(f"LLM kategorie: {title[:30]}... ‚Üí {category}")
                    return category

            logger.warning(f"LLM kategorie selhala, pou≈æ√≠v√°m fallback")

        except Exception as e:
            logger.warning(f"Chyba LLM kategorie: {e}")

        return 'tech'

    def detect_companies(self, title, description):
        """Detekuje v√Ωznamn√© firmy zm√≠nƒõn√© v ƒçl√°nku pomoc√≠ LLM"""
        if not self.translation_enabled:
            return []

        try:
            prompt = f"""Identifikuj v≈°echny v√Ωznamn√© technologick√© firmy zm√≠nƒõn√© v tomto ƒçl√°nku.

Nadpis: {title}
Popis: {description or 'Nen√≠ k dispozici'}

Zamƒõ≈ô se na:
- Technologick√© firmy (Apple, Google, Microsoft, Tesla, SpaceX, OpenAI, atd.)
- Startupy a scale-upy
- V√Ωznamn√© instituce (NASA, MIT, atd.)

Ignoruj:
- Obecn√© term√≠ny
- Produkty nebo slu≈æby (m√≠sto firem)
- Nez√°va≈æn√© zm√≠nky

Odpovƒõz POUZE seznam n√°zv≈Ø firem oddƒõlen√Ωch ƒç√°rkami, nic jin√©ho.
Pokud nejsou ≈æ√°dn√© v√Ωznamn√© firmy, odpovƒõz "≈æ√°dn√©"."""

            headers = {
                'Authorization': f'Bearer {self.openrouter_api_key}',
                'Content-Type': 'application/json'
            }

            data = {
                'model': 'anthropic/claude-sonnet-4.5',
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 100,
                'temperature': 0.1
            }

            response = requests.post(
                'https://openrouter.ai/api/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=10
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('choices') and len(result['choices']) > 0:
                    companies_text = result['choices'][0]['message']['content'].strip()

                    if companies_text.lower() in ['≈æ√°dn√©', 'zadne', 'none', '']:
                        return []

                    companies = [
                        company.strip().strip('"\'.,!?')
                        for company in companies_text.split(',')
                        if company.strip() and len(company.strip()) > 1
                    ]

                    logger.debug(f"LLM firmy: {title[:30]}... ‚Üí {companies}")
                    return companies[:5]

        except Exception as e:
            logger.warning(f"Chyba LLM firem: {e}")

        return []

    def detect_people(self, title, description):
        """Detekuje v√Ωznamn√© osobnosti zm√≠nƒõn√© v ƒçl√°nku pomoc√≠ LLM"""
        if not self.translation_enabled:
            return []

        try:
            prompt = f"""Identifikuj v≈°echny v√Ωznamn√© osobnosti zm√≠nƒõn√© v tomto ƒçl√°nku.

Nadpis: {title}
Popis: {description or 'Nen√≠ k dispozici'}

Zamƒõ≈ô se na:
- CEO a zakladatele tech firem (Elon Musk, Tim Cook, Satya Nadella, atd.)
- V√Ωznamn√© investory a podnikatele
- Vƒõdce a v√Ωzkumn√≠ky
- Politiky souvisej√≠c√≠ s technologiemi

Ignoruj:
- Obecn√© role bez jmen
- Fiktivn√≠ postavy
- Nez√°va≈æn√© zm√≠nky

Odpovƒõz POUZE seznam jmen oddƒõlen√Ωch ƒç√°rkami, nic jin√©ho.
Pokud nejsou ≈æ√°dn√© v√Ωznamn√© osobnosti, odpovƒõz "≈æ√°dn√©"."""

            headers = {
                'Authorization': f'Bearer {self.openrouter_api_key}',
                'Content-Type': 'application/json'
            }

            data = {
                'model': 'anthropic/claude-sonnet-4.5',
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 100,
                'temperature': 0.1
            }

            response = requests.post(
                'https://openrouter.ai/api/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=10
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('choices') and len(result['choices']) > 0:
                    people_text = result['choices'][0]['message']['content'].strip()

                    if people_text.lower() in ['≈æ√°dn√©', 'zadne', 'none', '']:
                        return []

                    people = [
                        person.strip().strip('"\'.,!?')
                        for person in people_text.split(',')
                        if person.strip() and len(person.strip()) > 2
                    ]

                    logger.debug(f"LLM osobnosti: {title[:30]}... ‚Üí {people}")
                    return people[:3]

        except Exception as e:
            logger.warning(f"Chyba LLM osobnost√≠: {e}")

        return []

    def is_gaming_article(self, title, description):
        """Detekuje ƒçl√°nky o poƒç√≠taƒçov√Ωch hr√°ch a hern√≠m pr≈Ømyslu"""
        text = f"{title} {description}".lower()

        # Kl√≠ƒçov√° slova souvisej√≠c√≠ s hrami a hern√≠m pr≈Ømyslem
        gaming_keywords = [
            # Obecn√© hern√≠ term√≠ny
            'game', 'games', 'gaming', 'gamer', 'esports', 'e-sports',
            # Hern√≠ platformy a engine
            'steam', 'playstation', 'xbox', 'nintendo', 'unreal engine', 'unity engine',
            # Hern√≠ spoleƒçnosti
            'activision', 'ubisoft', 'ea sports', 'electronic arts', 'rockstar', 'take-two',
            'square enix', 'bandai namco', 'konami', 'capcom', 'sega', 'nintendo', 'sony',
            'microsoft gaming', 'blizzard', 'valve',
            # Hern√≠ ≈æ√°nry
            'call of duty', 'fortnite', 'valorant', 'counter-strike', 'dota 2',
            'world of warcraft', 'elden ring', 'dark souls', 'zelda', 'minecraft',
            # VR/AR hry
            'virtual reality game', 'vr game', 'metaverse game', 'roblox',
            # Hern√≠ konference
            'gamescom', 'e3', 'pax', 'game developers conference', 'gdc',
            # Streamy a obsah
            'twitch', 'youtube gaming', 'streaming game',
            # Zneva≈æov√°n√≠ her
            'loot box', 'battle pass', 'microtransaction', 'dlc',
            # eSports a streamov√°n√≠
            'esports tournament', 'esports team', 'gaming tournament',
            'esports player', 'pro gamer', 'speedrun',
        ]

        # Poƒçet nalezen√Ωch hern√≠ch kl√≠ƒçov√Ωch slov
        gaming_matches = sum(1 for keyword in gaming_keywords if keyword in text)

        # Pokud se najde v√≠ce ne≈æ 1 hern√≠ kl√≠ƒçov√© slovo, je to pravdƒõpodobnƒõ artikel o hr√°ch
        if gaming_matches > 1:
            logger.debug(f"üéÆ Detekov√°n hern√≠ ƒçl√°nek (nalezeno {gaming_matches} kl√≠ƒçov√Ωch slov): {title[:50]}...")
            return True

        # Pokud se najde kl√≠ƒçov√© slovo "game" s dal≈°√≠mi indik√°tory
        if 'game' in text or 'gaming' in text:
            # Pod√≠vat se na dal≈°√≠ indik√°tory, kter√© by potvrdily, ≈æe je to o hr√°ch
            gaming_indicators = [
                'game release', 'game update', 'new game', 'game trailer',
                'game review', 'game patch', 'gaming news', 'game developer',
                'game engine', 'gaming studio', 'gaming hardware',
            ]

            if any(indicator in text for indicator in gaming_indicators):
                logger.debug(f"üéÆ Detekov√°n hern√≠ ƒçl√°nek (hern√≠ indik√°tor): {title[:50]}...")
                return True

        return False

    def detect_importance(self, title, description, category):
        """Detekuje d≈Øle≈æitost ƒçl√°nku"""
        text = f"{title} {description}".lower()

        # Vysok√° d≈Øle≈æitost
        if any(word in text for word in ['breakthrough', 'major', 'billion', 'acquisition', 'merge']):
            return 5

        # St≈ôedn√≠-vysok√° d≈Øle≈æitost
        if any(word in text for word in ['new', 'launches', 'announces', 'first', 'partnership']):
            return 4

        # N√≠zk√° d≈Øle≈æitost
        if any(word in text for word in ['rumors', 'might', 'reportedly', 'could']):
            return 2

        return 3  # Default

    def clean_duplicates(self, new_articles):
        """Sma≈æe pouze ƒçl√°nky s duplicitn√≠m slug, zachov√° archiv"""
        if not new_articles:
            logger.info("üßπ ≈Ω√°dn√© nov√© ƒçl√°nky - p≈ôeskakuji ƒçi≈°tƒõn√≠ duplicit≈Ø")
            return

        # Z√≠skat slugy nov√Ωch ƒçl√°nk≈Ø
        new_slugs = set()
        for article in new_articles:
            slug = self.create_slug(article.get('title', ''))
            new_slugs.add(slug)

        logger.info(f"üßπ Kontroluji duplicity pro {len(new_slugs)} nov√Ωch ƒçl√°nk≈Ø...")

        removed_count = 0
        for old_file in self.output_dir.glob('*.md'):
            if old_file.name == 'index.md':
                continue

            try:
                # Extrahovat slug ze jm√©na souboru
                # Form√°t: YYYY-MM-DD-slug.md
                file_parts = old_file.stem.split('-', 3)
                if len(file_parts) >= 4:
                    file_slug = file_parts[3]

                    if file_slug in new_slugs:
                        logger.debug(f"üóëÔ∏è Ma≈æu duplicitn√≠ ƒçl√°nek: {old_file.name}")
                        old_file.unlink()
                        removed_count += 1
                    else:
                        logger.debug(f"‚úÖ Zachov√°v√°m archivn√≠ ƒçl√°nek: {old_file.name}")

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Probl√©m p≈ôi kontrole souboru {old_file.name}: {e}")

        if removed_count > 0:
            logger.info(f"üßπ Smaz√°no {removed_count} duplicitn√≠ch ƒçl√°nk≈Ø, archiv zachov√°n")
        else:
            logger.info("üßπ ≈Ω√°dn√© duplicity nenalezeny")

    def generate_tech_news(self):
        """Hlavn√≠ funkce pro generov√°n√≠ tech-news z NewsAPI"""
        logger.info("üöÄ Spou≈°t√≠ se generov√°n√≠ tech-news z NewsAPI")

        # Z√≠skat ƒçl√°nky z NewsAPI
        articles = self.fetch_newsapi_articles()

        if not articles:
            logger.error("‚ùå ≈Ω√°dn√© ƒçl√°nky nenalezeny z NewsAPI")
            return False

        logger.info(f"üì∞ Zpracov√°v√°m {len(articles)} ƒçl√°nk≈Ø...")

        # Chytr√© smaz√°n√≠ duplicit≈Ø
        self.clean_duplicates(articles)

        processed_count = 0
        skipped_gaming_count = 0

        for i, article in enumerate(articles, 1):
            try:
                # P≈ôeskoƒçit ƒçl√°nky bez obsahu
                if not article.get('title'):
                    logger.warning(f"‚è≠Ô∏è P≈ôeskakuji ƒçl√°nek {i} - chyb√≠ titulek")
                    continue

                # P≈ôeskoƒçit ƒçl√°nky o hr√°ch a hern√≠m pr≈Ømyslu
                if self.is_gaming_article(article['title'], article.get('description', '')):
                    logger.info(f"üéÆ P≈ôeskakuji hern√≠ ƒçl√°nek {i}: {article['title'][:50]}...")
                    skipped_gaming_count += 1
                    continue

                logger.info(f"üìù Zpracov√°v√°m ƒçl√°nek {i}: {article['title'][:50]}...")

                # Kontrola obr√°zku
                if article.get('urlToImage'):
                    logger.info(f"üñºÔ∏è Obr√°zek nalezen: {article['urlToImage'][:50]}...")
                else:
                    logger.warning("üñºÔ∏è Obr√°zek chyb√≠")

                # Vytvo≈ôen√≠ Jekyll souboru
                self.create_jekyll_article(article, processed_count + 1)
                processed_count += 1

            except Exception as e:
                logger.error(f"‚ùå Chyba p≈ôi zpracov√°n√≠ ƒçl√°nku {i}: {e}")
                continue

        logger.info(f"‚úÖ √öspƒõ≈°nƒõ zpracov√°no {processed_count} ƒçl√°nk≈Ø")
        if skipped_gaming_count > 0:
            logger.info(f"üéÆ P≈ôeskoƒçeno {skipped_gaming_count} hern√≠ch ƒçl√°nk≈Ø")

        # Vytvo≈ôen√≠ index str√°nky
        self.create_index_page(processed_count)

        return True

    def create_index_page(self, article_count):
        """Vytvo≈ô√≠ index str√°nku tech-news"""
        index_content = f"""---
layout: tech_news_index
title: Technologick√© zpr√°vy
permalink: /tech-news/
description: Nejnovƒõj≈°√≠ zpr√°vy ze svƒõta technologi√≠ z NewsAPI s p≈ôeklady do ƒçe≈°tiny
---

# Technologick√© zpr√°vy

Automaticky aktualizovan√© zpr√°vy ze svƒõta technologi√≠ z NewsAPI, p≈ôelo≈æen√© do ƒçe≈°tiny.

**Celkem ƒçl√°nk≈Ø:** {article_count}
**Posledn√≠ aktualizace:** {datetime.now(timezone.utc).strftime('%d.%m.%Y %H:%M UTC')}

## Zdroje

ƒål√°nky jsou z√≠sk√°v√°ny z NewsAPI Technology kategorie, vƒçetnƒõ zdroj≈Ø jako:
- üì∞ **Associated Press** - zpravodajstv√≠
- üöÄ **TechCrunch** - startupy a technologie
- üíπ **Bloomberg** - business a finance
- üíº **Forbes** - podnik√°n√≠ a investice
- üì° **Axios** - technologie a politika
- ü§ñ **OpenAI** - AI pr≈Ølomy
"""

        index_path = self.output_dir / 'index.md'
        with open(index_path, 'w', encoding='utf-8') as f:
            f.write(index_content)

        logger.info("‚úÖ Index str√°nka vytvo≈ôena")

    def generate_daily_pages(self):
        """Generuje str√°nky pro ka≈æd√Ω den s ƒçl√°nky"""
        from collections import defaultdict

        # Cesty
        pages_dir = Path('tech-news')

        # Naƒç√≠st v≈°echny ƒçl√°nky a seskupit podle data
        articles_by_date = defaultdict(list)

        for article_file in self.output_dir.glob('*.md'):
            with open(article_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # Extrahovat front matter
            if content.startswith('---'):
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    try:
                        front_matter = yaml.safe_load(parts[1])

                        # Z√≠skat datum
                        date_str = None
                        if 'publishedAt' in front_matter:
                            date_obj = datetime.fromisoformat(front_matter['publishedAt'].replace('Z', '+00:00'))
                            date_str = date_obj.strftime('%Y-%m-%d')
                        elif 'date' in front_matter:
                            if isinstance(front_matter['date'], str):
                                date_obj = datetime.fromisoformat(front_matter['date'].split(' ')[0])
                            else:
                                date_obj = front_matter['date']
                            date_str = date_obj.strftime('%Y-%m-%d')

                        if date_str:
                            articles_by_date[date_str].append(article_file.name)

                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Chyba p≈ôi zpracov√°n√≠ {article_file.name}: {e}")

        # Vytvo≈ôit str√°nku pro ka≈æd√Ω den
        for date_str, articles in articles_by_date.items():
            # Vytvo≈ôit adres√°≈ô pro tento den
            day_dir = pages_dir / date_str
            day_dir.mkdir(parents=True, exist_ok=True)

            # Vytvo≈ôit index.md pro tento den
            index_file = day_dir / 'index.md'

            # Front matter pro denn√≠ str√°nku
            front_matter = {
                'layout': 'tech_news_day',
                'title': f'Technologick√© zpr√°vy - {date_str}',
                'date': date_str,
                'permalink': f'/tech-news/{date_str}/'
            }

            # Vytvo≈ôit obsah str√°nky
            content = f"""---
{yaml.dump(front_matter, default_flow_style=False, allow_unicode=True)}---

<!-- Tato str√°nka automaticky zobrazuje ƒçl√°nky z kolekce _tech_news pro datum {date_str} -->
"""

            # Zapsat soubor
            with open(index_file, 'w', encoding='utf-8') as f:
                f.write(content)

            logger.debug(f"üìÖ Vytvo≈ôena denn√≠ str√°nka pro {date_str}")

def main():
    """Hlavn√≠ funkce"""
    generator = NewsAPITechNewsGenerator()

    # Kontrola API kl√≠ƒç≈Ø
    if not generator.news_api_key:
        logger.error("‚ùå NEWS_API_KEY nen√≠ nastaven v .env souboru!")
        return 1

    logger.info(f"üìä NewsAPI generator p≈ôipraven")
    if generator.translation_enabled:
        logger.info("‚úÖ P≈ôeklady povoleny (OpenRouter API)")
    else:
        logger.info("‚ö†Ô∏è P≈ôeklady zak√°z√°ny (chyb√≠ OPENROUTER_API_KEY)")

    # Generovat tech-news
    success = generator.generate_tech_news()

    if success:
        # Generovat denn√≠ archivn√≠ str√°nky
        generator.generate_daily_pages()
        logger.info("üéâ Generov√°n√≠ tech-news z NewsAPI dokonƒçeno")
    else:
        logger.error("üí• Generov√°n√≠ tech-news z NewsAPI selhalo")
        return 1

    return 0

if __name__ == "__main__":
    exit(main())