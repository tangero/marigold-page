#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import yaml
import json
import os
import re
import requests
from datetime import datetime, timezone
from pathlib import Path
import logging
from dotenv import load_dotenv
from bs4 import BeautifulSoup

# Naƒç√≠st .env soubor
load_dotenv()

# Nastaven√≠ logov√°n√≠
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NewsAPITechNewsGenerator:
    """Gener√°tor tech-news z NewsAPI s p≈ôeklady a detekc√≠"""

    def __init__(self):
        self.output_dir = Path('_tech_news')
        self.output_dir.mkdir(exist_ok=True)
        self.news_api_key = os.getenv('NEWS_API_KEY', '')
        self.openrouter_api_key = os.getenv('OPENROUTER_API_KEY', '')
        self.translation_enabled = self.openrouter_api_key and self.openrouter_api_key != 'skip'

    def fetch_article_content(self, url, max_length=2000):
        """St√°hne a parsuje pln√Ω text ƒçl√°nku z URL"""
        try:
            logger.debug(f"üìÑ Stahuji obsah ƒçl√°nku: {url[:50]}...")

            headers = {
                'User-Agent': 'Mozilla/5.0 (Marigold.cz Tech News Bot; +https://marigold.cz)'
            }

            response = requests.get(url, headers=headers, timeout=10)

            if response.status_code != 200:
                logger.warning(f"‚ö†Ô∏è HTTP {response.status_code} pro {url}")
                return None

            soup = BeautifulSoup(response.text, 'html.parser')

            # Odstranit skripty, styly a navigaci
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()

            # Pokusit se naj√≠t hlavn√≠ obsah ƒçl√°nku
            article_content = None

            # Zkusit bƒõ≈æn√© selektory pro obsah ƒçl√°nku
            for selector in ['article', 'main', '.article-content', '.post-content', '.entry-content']:
                content = soup.select_one(selector)
                if content:
                    article_content = content
                    break

            # Pokud nenalezeno, pou≈æ√≠t cel√Ω body
            if not article_content:
                article_content = soup.body

            if not article_content:
                logger.warning(f"‚ö†Ô∏è Nepoda≈ôilo se extrahovat obsah z {url}")
                return None

            # Z√≠skat text a vyƒçistit
            text = article_content.get_text(separator=' ', strip=True)

            # Vyƒçistit whitespace
            text = re.sub(r'\s+', ' ', text)

            # Omezit d√©lku
            if len(text) > max_length:
                text = text[:max_length] + '...'

            logger.debug(f"‚úÖ Extrahov√°no {len(text)} znak≈Ø")
            return text

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Chyba p≈ôi stahov√°n√≠ ƒçl√°nku {url}: {e}")
            return None

    def analyze_and_enhance_article(self, url, title, description, category):
        """Kombinovan√° LLM anal√Ωza: detekce d≈Øle≈æitosti + generov√°n√≠ roz≈°√≠≈ôen√©ho obsahu"""
        if not self.translation_enabled:
            # Fallback bez LLM
            return {
                'importance': 3,
                'czech_title': title,
                'czech_description': description,
                'enhanced_content': description
            }

        try:
            # St√°hnout pln√Ω ƒçl√°nek
            full_text = self.fetch_article_content(url)

            # P≈ôipravit kontext pro LLM
            article_context = f"""
NADPIS: {title}
POPIS: {description}
KATEGORIE: {category}
"""

            if full_text:
                article_context += f"\nPLN√ù TEXT (zkr√°ceno): {full_text}\n"

            # Kombinovan√Ω prompt pro d≈Øle≈æitost + obsah
            prompt = f"""Analyzuj tento technologick√Ω ƒçl√°nek a vytvo≈ô ƒçesk√Ω obsah.

{article_context}

√öKOL 1 - D≈ÆLE≈ΩITOST (1-5):
5 = Pr≈Ølomov√© (AGI, kvantov√© poƒç√≠taƒçe, akvizice $1B+, bezpeƒçnostn√≠ krize, shutdown velk√Ωch slu≈æeb)
4 = Velmi d≈Øle≈æit√© (nov√© produkty Apple/Google/Microsoft/Meta/OpenAI, v√Ωznamn√° partnerstv√≠, IPO, funding $100M+)
3 = Zaj√≠mav√© (bƒõ≈æn√© novinky, updaty, zaj√≠mav√© technologie, novinky od zn√°m√Ωch firem)
2 = Spekulace (rumors, leaky, "mo≈æn√°", "√∫dajnƒõ", "sources say")
1 = Ned≈Øle≈æit√© (trivi√°ln√≠ novinky, clickbait)

√öKOL 2 - ƒåESK√ù OBSAH:
- Pokud d≈Øle≈æitost ‚â• 3: Vytvo≈ô strukturovan√Ω ƒçl√°nek (400-600 slov)
- Pokud d≈Øle≈æitost < 3: Vytvo≈ô pouze kr√°tk√© shrnut√≠ (100-150 slov)

Pro d≈Øle≈æit√© ƒçl√°nky (‚â•3) pou≈æij strukturu:
## Souhrn
[2-3 vƒõty s podstatou novinky]

## Kl√≠ƒçov√© body
- [3-5 nejd≈Øle≈æitƒõj≈°√≠ch bod≈Ø]

## Podrobnosti
[200-300 slov s detaily, kontextem a souvislostmi. Vysvƒõtli, co to znamen√° pro u≈æivatele/pr≈Ømysl.]

## Proƒç je to d≈Øle≈æit√©
[Dopady, kontext v ≈°ir≈°√≠m technologick√©m ekosyst√©mu]

STYLISTICK√â Z√ÅSADY:
- Pi≈° ƒçesky, ƒçitelnƒõ, jasnƒõ, konkr√©tnƒõ a st≈ô√≠zlivƒõ
- Vyvaruj se superlativ≈Ø a nad≈°en√Ωch v√Ωraz≈Ø
- Vynechej preambuli, jdi p≈ô√≠mo k vƒõci
- Buƒè kritick√Ω expert na umƒõlou inteligenci, IT a robotiku
- Pou≈æ√≠vej co nejv√≠ce detail≈Ø a konkr√©tn√≠ch informac√≠
- Preferuj del≈°√≠ souvisl√Ω text p≈ôed bodov√Ωmi odr√°≈ækami
- Vyvaruj se anglicism≈Ø, pou≈æ√≠vej ƒçesk√© v√Ωrazy
- U ka≈æd√©ho nov√©ho software ƒçi funkce zmi≈à, k ƒçemu slou≈æ√≠ a k ƒçemu je mo≈æn√© je pou≈æ√≠t
- U m√©nƒõ zn√°m√Ωch firem uveƒè, ƒç√≠m se zab√Ωvaj√≠
- √övodn√≠ odstavec by mƒõl obsahovat shrnut√≠ toho, o ƒçem ƒçl√°nek je

FORM√ÅT ODPOVƒöDI (JSON):
{{
  "importance": 3,
  "czech_title": "P≈ôelo≈æen√Ω titulek",
  "czech_description": "P≈ôelo≈æen√Ω kr√°tk√Ω popis (1-2 vƒõty)",
  "enhanced_content": "Roz≈°√≠≈ôen√Ω obsah v markdown form√°tu"
}}

D≈ÆLE≈ΩIT√â:
- Technick√© term√≠ny zachovej v angliƒçtinƒõ (AI, API, GPU, atd.)
- Buƒè konkr√©tn√≠, ne obecn√Ω
- Odpovƒõz POUZE validn√≠m JSON, bez jak√©hokoli dal≈°√≠ho textu
"""

            headers = {
                'Authorization': f'Bearer {self.openrouter_api_key}',
                'Content-Type': 'application/json'
            }

            data = {
                'model': 'anthropic/claude-sonnet-4.5',
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 2000,  # Dostatek pro dlouh√Ω obsah
                'temperature': 0.3
            }

            logger.debug(f"ü§ñ Vol√°m LLM pro anal√Ωzu ƒçl√°nku: {title[:50]}...")

            response = requests.post(
                'https://openrouter.ai/api/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('choices') and len(result['choices']) > 0:
                    content = result['choices'][0]['message']['content'].strip()

                    # Odstranit p≈ô√≠padn√© markdown code bloky
                    content = re.sub(r'^```json\s*', '', content)
                    content = re.sub(r'\s*```$', '', content)

                    # Parsovat JSON
                    try:
                        analysis = json.loads(content)

                        importance = analysis.get('importance', 3)
                        logger.info(f"‚úÖ LLM anal√Ωza: d≈Øle≈æitost={importance}, d√©lka obsahu={len(analysis.get('enhanced_content', ''))} znak≈Ø")

                        return {
                            'importance': importance,
                            'czech_title': analysis.get('czech_title', title),
                            'czech_description': analysis.get('czech_description', description),
                            'enhanced_content': analysis.get('enhanced_content', description)
                        }
                    except json.JSONDecodeError as e:
                        logger.warning(f"‚ö†Ô∏è Chyba parsov√°n√≠ JSON z LLM: {e}")
                        logger.warning(f"Odpovƒõƒè LLM: {content[:200]}...")
                        # Fallback p≈ôi chybƒõ parsov√°n√≠
                        return {
                            'importance': self.detect_importance(title, description, category),
                            'czech_title': self.translate_title(title),
                            'czech_description': self.translate_description(description),
                            'enhanced_content': self.translate_description(description)
                        }
            else:
                logger.warning(f"‚ö†Ô∏è LLM API selhalo (HTTP {response.status_code})")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Chyba p≈ôi LLM anal√Ωze: {e}")

        # Fallback - pou≈æ√≠t star√© metody
        return {
            'importance': self.detect_importance(title, description, category),
            'czech_title': self.translate_title(title),
            'czech_description': self.translate_description(description),
            'enhanced_content': self.translate_description(description)
        }

    def fetch_newsapi_articles(self):
        """St√°hne ƒçl√°nky z NewsAPI"""
        if not self.news_api_key:
            logger.error("‚ùå NEWS_API_KEY nen√≠ nastaven!")
            return []

        url = "https://newsapi.org/v2/top-headlines"
        params = {
            'category': 'technology',
            'apiKey': self.news_api_key,
            'pageSize': 40,  # Max 40 ƒçl√°nk≈Ø
            'language': 'en'
        }

        try:
            logger.info("üì° Stahuji ƒçl√°nky z NewsAPI...")
            response = requests.get(url, params=params, timeout=30)

            if response.status_code == 200:
                data = response.json()
                if data['status'] == 'ok':
                    articles = data.get('articles', [])
                    logger.info(f"‚úÖ Sta≈æeno {len(articles)} ƒçl√°nk≈Ø z NewsAPI")

                    # Vyƒçistit a p≈ôipravit ƒçl√°nky
                    processed_articles = []
                    for article in articles:
                        # P≈ôeskoƒçit ƒçl√°nky bez podstatn√Ωch dat
                        if not article.get('title') or article['title'] == '[Removed]':
                            continue
                        if not article.get('url'):
                            continue

                        # P≈ôipravit zdroj
                        source_info = {
                            'id': article['source'].get('id', 'unknown'),
                            'name': article['source'].get('name', 'Unknown'),
                            'emoji': self.get_source_emoji(article['source'].get('name', ''))
                        }

                        processed_article = {
                            'title': article['title'],
                            'description': article.get('description', ''),
                            'url': article['url'],
                            'urlToImage': article.get('urlToImage'),
                            'publishedAt': article['publishedAt'],
                            'source': source_info,
                            'content': article.get('content', '')
                        }

                        processed_articles.append(processed_article)

                    return processed_articles
                else:
                    logger.error(f"‚ùå NewsAPI error: {data.get('message', 'Unknown')}")
            else:
                logger.error(f"‚ùå HTTP {response.status_code}: {response.text}")

        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi stahov√°n√≠ z NewsAPI: {e}")

        return []

    def get_source_emoji(self, source_name):
        """Vr√°t√≠ emoji pro zdroj"""
        emoji_map = {
            'TechCrunch': 'üöÄ',
            'The Verge': '‚ö°',
            'Wired': 'üîß',
            'Ars Technica': 'üî¨',
            'MIT Technology Review': 'üéì',
            'OpenAI': 'ü§ñ',
            'Associated Press': 'üì∞',
            'Bloomberg': 'üíπ',
            'Forbes': 'üíº',
            'Axios': 'üì°',
            'CBS News': 'üì∫',
            'The Wall Street Journal': 'üìà',
        }

        for name, emoji in emoji_map.items():
            if name.lower() in source_name.lower():
                return emoji

        return 'üì∞'  # Default

    def create_jekyll_article(self, article, article_index):
        """Vytvo≈ô√≠ Jekyll ƒçl√°nek s optimalizovan√Ωm front matter"""

        # Vytvo≈ôit slug z titulku
        slug = self.create_slug(article['title'])

        # Datum pro filename - p≈ôev√©st na UTC
        pub_date_str = article['publishedAt']
        if pub_date_str.endswith('Z'):
            pub_date = datetime.fromisoformat(pub_date_str.replace('Z', '+00:00'))
        elif '+' in pub_date_str or pub_date_str.endswith('00:00'):
            pub_date = datetime.fromisoformat(pub_date_str)
        else:
            pub_date = datetime.fromisoformat(pub_date_str).replace(tzinfo=timezone.utc)

        # Zajistit UTC
        if pub_date.tzinfo is None:
            pub_date = pub_date.replace(tzinfo=timezone.utc)
        else:
            pub_date = pub_date.astimezone(timezone.utc)

        date_str = pub_date.strftime('%Y-%m-%d')

        filename = f"{date_str}-{slug}.md"
        filepath = self.output_dir / filename

        # Detekce kategorie nejd≈ô√≠v (pot≈ôebujeme pro LLM anal√Ωzu)
        category = self.detect_category(article['title'], article.get('description', ''))

        # Kombinovan√° LLM anal√Ωza: d≈Øle≈æitost + ƒçesk√Ω obsah
        analysis = self.analyze_and_enhance_article(
            article['url'],
            article['title'],
            article.get('description', ''),
            category
        )

        czech_title = analysis['czech_title']
        czech_description = analysis['czech_description']
        importance = analysis['importance']
        enhanced_content = analysis['enhanced_content']

        # Detekce firem a osobnost√≠ (zachov√°me pro metadata)
        companies = self.detect_companies(article['title'], article.get('description', ''))
        people = self.detect_people(article['title'], article.get('description', ''))

        # Front matter podle tech_news_article layoutu
        front_matter = {
            'layout': 'tech_news_article',
            'title': czech_title,
            'original_title': article['title'],
            'slug': slug,
            'description': czech_description,
            'publishedAt': pub_date.isoformat(),
            'date': pub_date.strftime('%Y-%m-%d %H:%M:%S'),
            'url': article['url'],
            'category': category,
            'importance': importance,
            'source': article['source']
        }

        # P≈ôidat firmy a osobnosti pouze pokud existuj√≠
        if companies:
            front_matter['companies'] = companies
        if people:
            front_matter['people'] = people

        # P≈ôidat urlToImage a urlToImageBackup pro fallback mechanismus
        if article.get('urlToImage'):
            front_matter['urlToImage'] = article['urlToImage']
            front_matter['urlToImageBackup'] = article['urlToImage']  # Backup URL pro p≈ô√≠pad lok√°ln√≠ho selh√°n√≠

        # Vytvo≈ôit obsah
        content = f"""---
{yaml.dump(front_matter, default_flow_style=False, allow_unicode=True)}---

{enhanced_content}

---

[ƒå√≠st p≈Øvodn√≠ ƒçl√°nek]({article['url']})

**Zdroj:** {article['source']['emoji']} {article['source']['name']}
"""

        # Zapsat soubor
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        logger.info(f"‚úÖ Vytvo≈ôen: {filename}")
        return filepath

    def create_slug(self, title):
        """Vytvo≈ô√≠ URL-friendly slug z titulku"""
        slug = re.sub(r'[^\w\s-]', '', title.lower())
        slug = re.sub(r'[-\s]+', '-', slug).strip('-')

        # Pokud slug zaƒç√≠n√° ƒç√≠slic√≠, p≈ôidat prefix "A-"
        if slug and slug[0].isdigit():
            slug = 'A-' + slug

        # Pokud je slug pr√°zdn√Ω, pou≈æ√≠t fallback
        if not slug:
            slug = 'article'

        return slug[:50]  # Omezit d√©lku

    def translate_text(self, text, text_type="text"):
        """P≈ôelo≈æ√≠ text pomoc√≠ OpenRouter API"""
        if not self.translation_enabled or not text or text.strip() == '':
            return text

        try:
            # R≈Øzn√© syst√©mov√© prompty podle typu textu
            system_prompts = {
                "title": "P≈ôekl√°dej technologick√© nadpisy ƒçl√°nk≈Ø z angliƒçtiny do ƒçe≈°tiny. Zachovej technick√© term√≠ny v angliƒçtinƒõ, pokud jsou bƒõ≈ænƒõ pou≈æ√≠van√©. Odpovƒõz POUZE p≈ôelo≈æen√Ωm nadpisem, bez jak√Ωchkoli dodateƒçn√Ωch text≈Ø jako 'Nadpis ƒçl√°nku v ƒçe≈°tinƒõ:' nebo uvozovek.",
                "description": "P≈ôekl√°dej perex/popis technologick√Ωch ƒçl√°nk≈Ø z angliƒçtiny do ƒçe≈°tiny. Zachovej technick√© term√≠ny v angliƒçtinƒõ, pokud jsou bƒõ≈ænƒõ pou≈æ√≠van√©. Odpovƒõz POUZE p≈ôelo≈æen√Ωm textem, bez jak√Ωchkoli dodateƒçn√Ωch koment√°≈ô≈Ø nebo uvozovek."
            }

            system_prompt = system_prompts.get(text_type, system_prompts["description"])

            headers = {
                'Authorization': f'Bearer {self.openrouter_api_key}',
                'Content-Type': 'application/json'
            }

            data = {
                'model': 'anthropic/claude-sonnet-4.5',
                'messages': [
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': text}
                ],
                'max_tokens': 200 if text_type == "title" else 500,
                'temperature': 0.3
            }

            response = requests.post(
                'https://openrouter.ai/api/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=10
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('choices') and len(result['choices']) > 0:
                    translated = result['choices'][0]['message']['content'].strip()
                    translated = translated.strip('"\'')
                    logger.debug(f"P≈ôeklad {text_type}: {text[:30]}... ‚Üí {translated[:30]}...")
                    return translated

            logger.warning(f"P≈ôeklad selhal (HTTP {response.status_code}), pou≈æ√≠v√°m origin√°l")

        except Exception as e:
            logger.warning(f"Chyba p≈ôekladu: {e}, pou≈æ√≠v√°m origin√°l")

        return text

    def translate_title(self, title):
        """P≈ôelo≈æ√≠ titulek"""
        return self.translate_text(title, "title")

    def translate_description(self, description):
        """P≈ôelo≈æ√≠ popis"""
        return self.translate_text(description or '', "description")

    def detect_category(self, title, description):
        """Detekuje kategorii ƒçl√°nku pomoc√≠ LLM"""
        if not self.translation_enabled:
            return 'tech'

        try:
            prompt = f"""P≈ôi≈ôaƒè tomuto technologick√©mu ƒçl√°nku jednu p≈ôesnou kategorii (1-2 slova v ƒçe≈°tinƒõ).

Nadpis: {title}
Popis: {description or 'Nen√≠ k dispozici'}

Kategorie by mƒõla b√Ωt:
- Struƒçn√° (1-2 slova)
- V ƒçe≈°tinƒõ
- Specifick√° pro obsah ƒçl√°nku
- Bez emoji

P≈ô√≠klady kategori√≠: AI, hardware, startupy, programov√°n√≠, mobiln√≠ aplikace, kybernetika, kosmonautika, elektromobilita, hern√≠ pr≈Ømysl, kryptomƒõny, j√≠dlo, zdrav√≠ atd.

Odpovƒõz POUZE n√°zvem kategorie, nic jin√©ho."""

            headers = {
                'Authorization': f'Bearer {self.openrouter_api_key}',
                'Content-Type': 'application/json'
            }

            data = {
                'model': 'anthropic/claude-sonnet-4.5',
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 20,
                'temperature': 0.1
            }

            response = requests.post(
                'https://openrouter.ai/api/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=10
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('choices') and len(result['choices']) > 0:
                    category = result['choices'][0]['message']['content'].strip()
                    category = category.lower().strip('"\'.,!?')
                    if len(category) > 20:
                        category = category[:20]

                    logger.debug(f"LLM kategorie: {title[:30]}... ‚Üí {category}")
                    return category

            logger.warning(f"LLM kategorie selhala, pou≈æ√≠v√°m fallback")

        except Exception as e:
            logger.warning(f"Chyba LLM kategorie: {e}")

        return 'tech'

    def detect_companies(self, title, description):
        """Detekuje v√Ωznamn√© firmy zm√≠nƒõn√© v ƒçl√°nku pomoc√≠ LLM"""
        if not self.translation_enabled:
            return []

        try:
            prompt = f"""Identifikuj v≈°echny v√Ωznamn√© technologick√© firmy zm√≠nƒõn√© v tomto ƒçl√°nku.

Nadpis: {title}
Popis: {description or 'Nen√≠ k dispozici'}

Zamƒõ≈ô se na:
- Technologick√© firmy (Apple, Google, Microsoft, Tesla, SpaceX, OpenAI, atd.)
- Startupy a scale-upy
- V√Ωznamn√© instituce (NASA, MIT, atd.)

Ignoruj:
- Obecn√© term√≠ny
- Produkty nebo slu≈æby (m√≠sto firem)
- Nez√°va≈æn√© zm√≠nky

Odpovƒõz POUZE seznam n√°zv≈Ø firem oddƒõlen√Ωch ƒç√°rkami, nic jin√©ho.
Pokud nejsou ≈æ√°dn√© v√Ωznamn√© firmy, odpovƒõz "≈æ√°dn√©"."""

            headers = {
                'Authorization': f'Bearer {self.openrouter_api_key}',
                'Content-Type': 'application/json'
            }

            data = {
                'model': 'anthropic/claude-sonnet-4.5',
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 100,
                'temperature': 0.1
            }

            response = requests.post(
                'https://openrouter.ai/api/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=10
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('choices') and len(result['choices']) > 0:
                    companies_text = result['choices'][0]['message']['content'].strip()

                    if companies_text.lower() in ['≈æ√°dn√©', 'zadne', 'none', '']:
                        return []

                    companies = [
                        company.strip().strip('"\'.,!?')
                        for company in companies_text.split(',')
                        if company.strip() and len(company.strip()) > 1
                    ]

                    logger.debug(f"LLM firmy: {title[:30]}... ‚Üí {companies}")
                    return companies[:5]

        except Exception as e:
            logger.warning(f"Chyba LLM firem: {e}")

        return []

    def detect_people(self, title, description):
        """Detekuje v√Ωznamn√© osobnosti zm√≠nƒõn√© v ƒçl√°nku pomoc√≠ LLM"""
        if not self.translation_enabled:
            return []

        try:
            prompt = f"""Identifikuj v≈°echny v√Ωznamn√© osobnosti zm√≠nƒõn√© v tomto ƒçl√°nku.

Nadpis: {title}
Popis: {description or 'Nen√≠ k dispozici'}

Zamƒõ≈ô se na:
- CEO a zakladatele tech firem (Elon Musk, Tim Cook, Satya Nadella, atd.)
- V√Ωznamn√© investory a podnikatele
- Vƒõdce a v√Ωzkumn√≠ky
- Politiky souvisej√≠c√≠ s technologiemi

Ignoruj:
- Obecn√© role bez jmen
- Fiktivn√≠ postavy
- Nez√°va≈æn√© zm√≠nky

Odpovƒõz POUZE seznam jmen oddƒõlen√Ωch ƒç√°rkami, nic jin√©ho.
Pokud nejsou ≈æ√°dn√© v√Ωznamn√© osobnosti, odpovƒõz "≈æ√°dn√©"."""

            headers = {
                'Authorization': f'Bearer {self.openrouter_api_key}',
                'Content-Type': 'application/json'
            }

            data = {
                'model': 'anthropic/claude-sonnet-4.5',
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': 100,
                'temperature': 0.1
            }

            response = requests.post(
                'https://openrouter.ai/api/v1/chat/completions',
                headers=headers,
                json=data,
                timeout=10
            )

            if response.status_code == 200:
                result = response.json()
                if result.get('choices') and len(result['choices']) > 0:
                    people_text = result['choices'][0]['message']['content'].strip()

                    if people_text.lower() in ['≈æ√°dn√©', 'zadne', 'none', '']:
                        return []

                    people = [
                        person.strip().strip('"\'.,!?')
                        for person in people_text.split(',')
                        if person.strip() and len(person.strip()) > 2
                    ]

                    logger.debug(f"LLM osobnosti: {title[:30]}... ‚Üí {people}")
                    return people[:3]

        except Exception as e:
            logger.warning(f"Chyba LLM osobnost√≠: {e}")

        return []

    def should_skip_article(self, title, description):
        """Detekuje ƒçl√°nky, kter√© by mƒõly b√Ωt p≈ôeskoƒçeny (hry, sport, z√°bava)"""
        text = f"{title} {description}".lower()

        # === HERN√ç KL√çƒåOV√Å SLOVA ===
        gaming_keywords = [
            # Obecn√© hern√≠ term√≠ny
            'game', 'games', 'gaming', 'gamer', 'gameplay', 'playthrough',
            'esports', 'e-sports', 'speedrun', 'streamer', 'streaming game',
            # Hern√≠ platformy
            'steam', 'playstation', 'ps4', 'ps5', 'xbox', 'nintendo', 'switch',
            'game pass', 'epic games store',
            # Hern√≠ engine a n√°stroje
            'unreal engine', 'unity engine', 'game engine',
            # Hern√≠ spoleƒçnosti
            'activision', 'ubisoft', 'ea sports', 'electronic arts', 'rockstar',
            'take-two', 'square enix', 'bandai namco', 'konami', 'capcom', 'sega',
            'blizzard', 'valve', 'bungie', 'bethesda', 'bioware',
            # Popul√°rn√≠ hern√≠ s√©rie a hry
            'call of duty', 'fortnite', 'valorant', 'counter-strike', 'cs:go', 'cs2',
            'dota', 'league of legends', 'lol', 'world of warcraft', 'wow',
            'elden ring', 'dark souls', 'zelda', 'minecraft', 'roblox',
            'gta', 'grand theft auto', 'red dead', 'assassin\'s creed',
            'final fantasy', 'borderlands', 'destiny', 'halo', 'spider-man',
            'god of war', 'horizon', 'the last of us', 'uncharted',
            'pokemon', 'mario', 'sonic', 'yakuza', 'resident evil',
            'street fighter', 'mortal kombat', 'overwatch', 'apex legends',
            'pubg', 'battlefield', 'fifa', 'nba 2k', 'madden',
            'diablo', 'starcraft', 'hearthstone', 'witcher', 'cyberpunk',
            'fallout', 'skyrim', 'elder scrolls', 'doom', 'wolfenstein',
            'marathon', 'ananta',
            # Hern√≠ ≈æ√°nry a term√≠ny
            'rpg', 'mmorpg', 'fps', 'battle royale', 'roguelike', 'metroidvania',
            'souls-like', 'open world', 'sandbox', 'moba', 'rts',
            'loot box', 'battle pass', 'microtransaction', 'dlc', 'season pass',
            'gacha game', 'mobile game',
            # Hern√≠ ud√°losti a m√©dia
            'gamescom', 'e3', 'pax', 'game developers conference', 'gdc',
            'state of play', 'nintendo direct', 'xbox showcase',
            'game awards', 'ign', 'gamespot', 'polygon', 'kotaku',
            'rock paper shotgun',
            # VR/AR hry
            'vr game', 'virtual reality game', 'oculus', 'quest',
            # Streamov√°n√≠
            'twitch', 'youtube gaming', 'mixer',
        ]

        # === SPORTOVN√ç KL√çƒåOV√Å SLOVA ===
        sports_keywords = [
            # Obecn√© sportovn√≠ term√≠ny
            'sport', 'sports', 'athlete', 'championship', 'tournament',
            'league', 'season', 'playoffs', 'finals', 'match', 'game score',
            # Konkr√©tn√≠ sporty
            'football', 'soccer', 'basketball', 'baseball', 'hockey',
            'tennis', 'golf', 'cricket', 'rugby', 'boxing', 'ufc', 'mma',
            'formula 1', 'f1', 'nascar', 'racing', 'motorsport',
            # Sportovn√≠ organizace
            'nfl', 'nba', 'mlb', 'nhl', 'fifa world cup', 'premier league',
            'champions league', 'olympics', 'super bowl',
            # Wrestling a z√°bavn√≠ sport
            'wwe', 'wrestling', 'wrestler', 'smackdown', 'raw', 'wrestlemania',
            'aew', 'impact wrestling',
        ]

        # === Z√ÅBAVN√ç KL√çƒåOV√Å SLOVA ===
        entertainment_keywords = [
            # Filmy a seri√°ly (pokud nejsou o technologii)
            'movie review', 'film review', 'tv series', 'netflix show',
            'season finale', 'episode', 'actor', 'actress', 'director',
            'box office', 'trailer review',
            # Reality show a celebritn√≠ zpr√°vy
            'reality show', 'celebrity news', 'gossip',
        ]

        # Zkombinovat v≈°echny kl√≠ƒçov√© slova
        all_skip_keywords = gaming_keywords + sports_keywords + entertainment_keywords

        # Spoƒç√≠tat shody
        matches = sum(1 for keyword in all_skip_keywords if keyword in text)

        # Agresivnƒõj≈°√≠ detekce - staƒç√≠ 1 shoda
        if matches > 0:
            # Extra kontrola - nƒõkter√° slova mohou b√Ωt fale≈°nƒõ pozitivn√≠
            # Pokud je to o technologii (ne o samotn√© h≈ôe), nep≈ôeskakovat
            tech_indicators = [
                'ai in gaming', 'machine learning', 'artificial intelligence',
                'cloud gaming technology', 'game streaming technology',
                'graphics card', 'gpu', 'processor', 'chip',
                'nvidia', 'amd', 'intel' # pokud nen√≠ p≈ô√≠mo o hern√≠m hardware
            ]

            has_tech_context = any(indicator in text for indicator in tech_indicators)

            if not has_tech_context:
                logger.debug(f"üö´ P≈ôeskakuji ƒçl√°nek (nalezeno {matches} kl√≠ƒçov√Ωch slov pro p≈ôeskoƒçen√≠): {title[:50]}...")
                return True

        return False

    def detect_importance(self, title, description, category):
        """Detekuje d≈Øle≈æitost ƒçl√°nku"""
        text = f"{title} {description}".lower()

        # Vysok√° d≈Øle≈æitost
        if any(word in text for word in ['breakthrough', 'major', 'billion', 'acquisition', 'merge']):
            return 5

        # St≈ôedn√≠-vysok√° d≈Øle≈æitost
        if any(word in text for word in ['new', 'launches', 'announces', 'first', 'partnership']):
            return 4

        # N√≠zk√° d≈Øle≈æitost
        if any(word in text for word in ['rumors', 'might', 'reportedly', 'could']):
            return 2

        return 3  # Default

    def clean_duplicates(self, new_articles):
        """Sma≈æe pouze ƒçl√°nky s duplicitn√≠m slug, zachov√° archiv"""
        if not new_articles:
            logger.info("üßπ ≈Ω√°dn√© nov√© ƒçl√°nky - p≈ôeskakuji ƒçi≈°tƒõn√≠ duplicit≈Ø")
            return

        # Z√≠skat slugy nov√Ωch ƒçl√°nk≈Ø
        new_slugs = set()
        for article in new_articles:
            slug = self.create_slug(article.get('title', ''))
            new_slugs.add(slug)

        logger.info(f"üßπ Kontroluji duplicity pro {len(new_slugs)} nov√Ωch ƒçl√°nk≈Ø...")

        removed_count = 0
        for old_file in self.output_dir.glob('*.md'):
            if old_file.name == 'index.md':
                continue

            try:
                # Extrahovat slug ze jm√©na souboru
                # Form√°t: YYYY-MM-DD-slug.md
                file_parts = old_file.stem.split('-', 3)
                if len(file_parts) >= 4:
                    file_slug = file_parts[3]

                    if file_slug in new_slugs:
                        logger.debug(f"üóëÔ∏è Ma≈æu duplicitn√≠ ƒçl√°nek: {old_file.name}")
                        old_file.unlink()
                        removed_count += 1
                    else:
                        logger.debug(f"‚úÖ Zachov√°v√°m archivn√≠ ƒçl√°nek: {old_file.name}")

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Probl√©m p≈ôi kontrole souboru {old_file.name}: {e}")

        if removed_count > 0:
            logger.info(f"üßπ Smaz√°no {removed_count} duplicitn√≠ch ƒçl√°nk≈Ø, archiv zachov√°n")
        else:
            logger.info("üßπ ≈Ω√°dn√© duplicity nenalezeny")

    def generate_tech_news(self):
        """Hlavn√≠ funkce pro generov√°n√≠ tech-news z NewsAPI"""
        logger.info("üöÄ Spou≈°t√≠ se generov√°n√≠ tech-news z NewsAPI")

        # Z√≠skat ƒçl√°nky z NewsAPI
        articles = self.fetch_newsapi_articles()

        if not articles:
            logger.error("‚ùå ≈Ω√°dn√© ƒçl√°nky nenalezeny z NewsAPI")
            return False

        logger.info(f"üì∞ Zpracov√°v√°m {len(articles)} ƒçl√°nk≈Ø...")

        # Chytr√© smaz√°n√≠ duplicit≈Ø
        self.clean_duplicates(articles)

        processed_count = 0
        skipped_count = 0

        for i, article in enumerate(articles, 1):
            try:
                # P≈ôeskoƒçit ƒçl√°nky bez obsahu
                if not article.get('title'):
                    logger.warning(f"‚è≠Ô∏è P≈ôeskakuji ƒçl√°nek {i} - chyb√≠ titulek")
                    continue

                # P≈ôeskoƒçit ƒçl√°nky o hr√°ch, sportu a z√°bavƒõ
                if self.should_skip_article(article['title'], article.get('description', '')):
                    logger.info(f"üö´ P≈ôeskakuji nerelevantn√≠ ƒçl√°nek {i}: {article['title'][:50]}...")
                    skipped_count += 1
                    continue

                logger.info(f"üìù Zpracov√°v√°m ƒçl√°nek {i}: {article['title'][:50]}...")

                # Kontrola obr√°zku
                if article.get('urlToImage'):
                    logger.info(f"üñºÔ∏è Obr√°zek nalezen: {article['urlToImage'][:50]}...")
                else:
                    logger.warning("üñºÔ∏è Obr√°zek chyb√≠")

                # Vytvo≈ôen√≠ Jekyll souboru
                self.create_jekyll_article(article, processed_count + 1)
                processed_count += 1

            except Exception as e:
                logger.error(f"‚ùå Chyba p≈ôi zpracov√°n√≠ ƒçl√°nku {i}: {e}")
                continue

        logger.info(f"‚úÖ √öspƒõ≈°nƒõ zpracov√°no {processed_count} ƒçl√°nk≈Ø")
        if skipped_count > 0:
            logger.info(f"üö´ P≈ôeskoƒçeno {skipped_count} nerelevantn√≠ch ƒçl√°nk≈Ø (hry, sport, z√°bava)")

        # Vytvo≈ôen√≠ index str√°nky
        self.create_index_page(processed_count)

        return True

    def create_index_page(self, article_count):
        """Vytvo≈ô√≠ index str√°nku tech-news"""
        index_content = f"""---
layout: tech_news_index
title: Technologick√© zpr√°vy
permalink: /tech-news/
description: Nejnovƒõj≈°√≠ zpr√°vy ze svƒõta technologi√≠ z NewsAPI s p≈ôeklady do ƒçe≈°tiny
---

# Technologick√© zpr√°vy

Automaticky aktualizovan√© zpr√°vy ze svƒõta technologi√≠ z NewsAPI, p≈ôelo≈æen√© do ƒçe≈°tiny.

**Celkem ƒçl√°nk≈Ø:** {article_count}
**Posledn√≠ aktualizace:** {datetime.now(timezone.utc).strftime('%d.%m.%Y %H:%M UTC')}

## Zdroje

ƒål√°nky jsou z√≠sk√°v√°ny z NewsAPI Technology kategorie, vƒçetnƒõ zdroj≈Ø jako:
- üì∞ **Associated Press** - zpravodajstv√≠
- üöÄ **TechCrunch** - startupy a technologie
- üíπ **Bloomberg** - business a finance
- üíº **Forbes** - podnik√°n√≠ a investice
- üì° **Axios** - technologie a politika
- ü§ñ **OpenAI** - AI pr≈Ølomy
"""

        index_path = self.output_dir / 'index.md'
        with open(index_path, 'w', encoding='utf-8') as f:
            f.write(index_content)

        logger.info("‚úÖ Index str√°nka vytvo≈ôena")

    def generate_daily_pages(self):
        """Generuje str√°nky pro ka≈æd√Ω den s ƒçl√°nky"""
        from collections import defaultdict

        # Cesty
        pages_dir = Path('tech-news')

        # Naƒç√≠st v≈°echny ƒçl√°nky a seskupit podle data
        articles_by_date = defaultdict(list)

        for article_file in self.output_dir.glob('*.md'):
            with open(article_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # Extrahovat front matter
            if content.startswith('---'):
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    try:
                        front_matter = yaml.safe_load(parts[1])

                        # Z√≠skat datum
                        date_str = None
                        if 'publishedAt' in front_matter:
                            date_obj = datetime.fromisoformat(front_matter['publishedAt'].replace('Z', '+00:00'))
                            date_str = date_obj.strftime('%Y-%m-%d')
                        elif 'date' in front_matter:
                            if isinstance(front_matter['date'], str):
                                date_obj = datetime.fromisoformat(front_matter['date'].split(' ')[0])
                            else:
                                date_obj = front_matter['date']
                            date_str = date_obj.strftime('%Y-%m-%d')

                        if date_str:
                            articles_by_date[date_str].append(article_file.name)

                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Chyba p≈ôi zpracov√°n√≠ {article_file.name}: {e}")

        # Vytvo≈ôit str√°nku pro ka≈æd√Ω den
        for date_str, articles in articles_by_date.items():
            # Vytvo≈ôit adres√°≈ô pro tento den
            day_dir = pages_dir / date_str
            day_dir.mkdir(parents=True, exist_ok=True)

            # Vytvo≈ôit index.md pro tento den
            index_file = day_dir / 'index.md'

            # Front matter pro denn√≠ str√°nku
            front_matter = {
                'layout': 'tech_news_day',
                'title': f'Technologick√© zpr√°vy - {date_str}',
                'date': date_str,
                'permalink': f'/tech-news/{date_str}/'
            }

            # Vytvo≈ôit obsah str√°nky
            content = f"""---
{yaml.dump(front_matter, default_flow_style=False, allow_unicode=True)}---

<!-- Tato str√°nka automaticky zobrazuje ƒçl√°nky z kolekce _tech_news pro datum {date_str} -->
"""

            # Zapsat soubor
            with open(index_file, 'w', encoding='utf-8') as f:
                f.write(content)

            logger.debug(f"üìÖ Vytvo≈ôena denn√≠ str√°nka pro {date_str}")

def main():
    """Hlavn√≠ funkce"""
    generator = NewsAPITechNewsGenerator()

    # Kontrola API kl√≠ƒç≈Ø
    if not generator.news_api_key:
        logger.error("‚ùå NEWS_API_KEY nen√≠ nastaven v .env souboru!")
        return 1

    logger.info(f"üìä NewsAPI generator p≈ôipraven")
    if generator.translation_enabled:
        logger.info("‚úÖ P≈ôeklady povoleny (OpenRouter API)")
    else:
        logger.info("‚ö†Ô∏è P≈ôeklady zak√°z√°ny (chyb√≠ OPENROUTER_API_KEY)")

    # Generovat tech-news
    success = generator.generate_tech_news()

    if success:
        # Generovat denn√≠ archivn√≠ str√°nky
        generator.generate_daily_pages()
        logger.info("üéâ Generov√°n√≠ tech-news z NewsAPI dokonƒçeno")
    else:
        logger.error("üí• Generov√°n√≠ tech-news z NewsAPI selhalo")
        return 1

    return 0

if __name__ == "__main__":
    exit(main())