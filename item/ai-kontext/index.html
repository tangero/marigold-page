<!DOCTYPE html>
<html>
  <head>
    <title>ProÄ je velikost kontextu u LLM tak dÅ¯leÅ¾itÃ¡? | Marigold.cz - SÃ­tÄ› a Technologie</title>
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="ProÄ je velikost kontextu u LLM tak dÅ¯leÅ¾itÃ¡?" />
<meta name="author" content="Patrick Zandl" />
<meta property="og:locale" content="cs" />
<meta name="description" content="A pÅ™edevÅ¡Ã­m, proÄ je tak drahÃ© a zdlouhavÃ© zvyÅ¡ovat velikost kontextu? Tento ÄlÃ¡nek se podrobnÄ› zabÃ½vÃ¡ tÃ­m, co kontext znamenÃ¡, proÄ je jeho dÃ©lka kritickÃ¡, jakÃ© technickÃ© pÅ™ekÃ¡Å¾ky brÃ¡nÃ­ jeho neomezenÃ©mu rozÅ¡iÅ™ovÃ¡nÃ­ a jakÃ¡ Å™eÅ¡enÃ­ se v souÄasnosti vyvÃ­jejÃ­." />
<meta property="og:description" content="A pÅ™edevÅ¡Ã­m, proÄ je tak drahÃ© a zdlouhavÃ© zvyÅ¡ovat velikost kontextu? Tento ÄlÃ¡nek se podrobnÄ› zabÃ½vÃ¡ tÃ­m, co kontext znamenÃ¡, proÄ je jeho dÃ©lka kritickÃ¡, jakÃ© technickÃ© pÅ™ekÃ¡Å¾ky brÃ¡nÃ­ jeho neomezenÃ©mu rozÅ¡iÅ™ovÃ¡nÃ­ a jakÃ¡ Å™eÅ¡enÃ­ se v souÄasnosti vyvÃ­jejÃ­." />
<link rel="canonical" href="https://www.marigold.cz/item/ai-kontext/" />
<meta property="og:url" content="https://www.marigold.cz/item/ai-kontext/" />
<meta property="og:site_name" content="Marigold.cz" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-05-05T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ProÄ je velikost kontextu u LLM tak dÅ¯leÅ¾itÃ¡?" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Patrick Zandl"},"dateModified":"2025-05-05T00:00:00+00:00","datePublished":"2025-05-05T00:00:00+00:00","description":"A pÅ™edevÅ¡Ã­m, proÄ je tak drahÃ© a zdlouhavÃ© zvyÅ¡ovat velikost kontextu? Tento ÄlÃ¡nek se podrobnÄ› zabÃ½vÃ¡ tÃ­m, co kontext znamenÃ¡, proÄ je jeho dÃ©lka kritickÃ¡, jakÃ© technickÃ© pÅ™ekÃ¡Å¾ky brÃ¡nÃ­ jeho neomezenÃ©mu rozÅ¡iÅ™ovÃ¡nÃ­ a jakÃ¡ Å™eÅ¡enÃ­ se v souÄasnosti vyvÃ­jejÃ­.","headline":"ProÄ je velikost kontextu u LLM tak dÅ¯leÅ¾itÃ¡?","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.marigold.cz/item/ai-kontext/"},"url":"https://www.marigold.cz/item/ai-kontext/"}</script>
<!-- End Jekyll SEO tag -->

        <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>
    <link href="https://fed.brid.gy/" rel="alternate" type="application/activity+json">

    
    <meta property="og:description" content="A pÅ™edevÅ¡Ã­m, proÄ je tak drahÃ© a zdlouhavÃ© zvyÅ¡ovat velikost kontextu? Tento ÄlÃ¡nek se podrobnÄ› zabÃ½vÃ¡ tÃ­m, co kontext znamenÃ¡, proÄ je jeho dÃ©lka kritickÃ¡, jakÃ© technickÃ© pÅ™ekÃ¡Å¾ky brÃ¡nÃ­ jeho neomezenÃ©mu rozÅ¡iÅ™ovÃ¡nÃ­ a jakÃ¡ Å™eÅ¡enÃ­ se v souÄasnosti vyvÃ­jejÃ­.
" />
    
    <meta name="author" content="Marigold.cz" />

    
    <meta property="og:title" content="ProÄ je velikost kontextu u LLM tak dÅ¯leÅ¾itÃ¡?" />
    <meta property="twitter:title" content="ProÄ je velikost kontextu u LLM tak dÅ¯leÅ¾itÃ¡?" />
    

    
    <!-- page.thumbnail -->
    
        <!-- page.thumbnail https://www.marigold.cz/assets/llm-kontext.jpg |  -->
    <meta property="og:image" content="https://res.cloudinary.com/dvwv5cne3/image/fetch/w_1200,h_630,c_fill,g_auto,f_auto,q_auto/https://www.marigold.cz/assets/llm-kontext.jpg"/>
    <meta property="twitter:image" content="https://res.cloudinary.com/dvwv5cne3/image/fetch/w_1024,h_512,c_fill,g_auto,f_auto,q_auto/https://www.marigold.cz/assets/llm-kontext.jpg"/>
    

    <meta property="og:site_name" content="Marigold.cz | Technologie a SpoleÄnost"/>

    


    <link rel="stylesheet" type="text/css" href="//assets/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Marigold.cz - Technologie a SvÄ›t" href="//feed.xml" />
    <link rel="canonical" href="https://www.marigold.cz/item/ai-kontext/" />

    <meta name="theme-color" content="#000000">

    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="manifest" href="/images/site.webmanifest">
    <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="/images/favicon.ico">
    <meta name="msapplication-TileColor" content="#2d89ef">
    <meta name="msapplication-config" content="/images/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
    <script type="text/javascript">
      window.heapReadyCb=window.heapReadyCb||[],window.heap=window.heap||[],heap.load=function(e,t){window.heap.envId=e,window.heap.clientConfig=t=t||{},window.heap.clientConfig.shouldFetchServerConfig=!1;var a=document.createElement("script");a.type="text/javascript",a.async=!0,a.src="https://cdn.us.heap-api.com/config/"+e+"/heap_config.js";var r=document.getElementsByTagName("script")[0];r.parentNode.insertBefore(a,r);var n=["init","startTracking","stopTracking","track","resetIdentity","identify","getSessionId","getUserId","getIdentity","addUserProperties","addEventProperties","removeEventProperty","clearEventProperties","addAccountProperties","addAdapter","addTransformer","addTransformerFn","onReady","addPageviewProperties","removePageviewProperty","clearPageviewProperties","trackPageview"],i=function(e){return function(){var t=Array.prototype.slice.call(arguments,0);window.heapReadyCb.push({name:e,fn:function(){heap[e]&&heap[e].apply(heap,t)}})}};for(var p=0;p<n.length;p++)heap[n[p]]=i(n[p])};
      heap.load("2219710997");
  </script>
  </head>

  <body>
    <div id="bar"></div>
    <div class="wrapper-container">
      <div class="wrapper-masthead">
        <div class="container">
          <header class="masthead clearfix">
            <a href="//" class="site-avatar"><img src="//images/patrick-avatar.jpg" alt="" /></a>

            <div class="site-info">
              <h1 class="site-name"><a href="//">Marigold.cz</a></h1>
              <p class="site-description">Technologie a SvÄ›t</p>

            </div>

            <nav>
              <a href="/search">ğŸ”</a> | <a href="https://www.prolnuto.cz/">ğŸ§‘â€ğŸ’» Kurzy AI</a> | <a href="/vibecoding">ğŸ‘¨â€ğŸ’» Vibe Coding</a> | <a href="/mobilnisite">ğŸ—¼ 4G/5G</a> | <a href="/ai">ğŸ¤– AI</a> | <a href="/obrazy">ğŸ–¼ï¸ Obrazy</a>
            </nav>
          </header>
        </div>
      </div>

      <div class="wrapper-main">
        <div id="main" role="main" class="container">
          <!-- start Mermaid run code --> 
<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.0.2/+esm'
  mermaid.initialize({startOnLoad:true,theme:'neutral'})
  await mermaid.run({querySelector:'code.language-mermaid'})
</script>
<!-- end fMermaid run code --> 

<!-- start feedwind code --> 
<!-- start feedwind code --> <script type="text/javascript" src="https://feed.mikle.com/js/fw-loader.js" preloader-text="Nahr%C3%A1v%C3%A1m" data-fw-param="168257/"></script> <!-- end feedwind code -->


<style>
  code.language-mermaid {
    display: flex;
    justify-content: center;
  }
  pre:has(code.language-mermaid), code.language-mermaid {
    background-color: transparent;
  }
  .edgeLabel {
    font-size: 92%;
    opacity: .95;
    color: #111;
    padding: 0 3px;
  }
  .node rect {
    stroke: #214f78 !important;
  }
  .nodeLabel {
    color: #214f78 !important;
  }

  /* OdstranÄ›nÃ­ rÃ¡meÄkÅ¯ kolem textu */
  .post.detailed {
    padding: 0;
    margin: 0;
    border: none;
    box-shadow: none;
  }

  .post.detailed .entry {
    padding: 0;
    margin: 0;
    border: none;
    box-shadow: none;
  }

  /* OdstranÄ›nÃ­ okrajÅ¯ kolem sekce ÄŒlÃ¡nky a novinky */
  .posts {
    margin: 0 !important;
    padding: 0 !important;
    border: none !important;
    box-shadow: none !important;
    background: none !important;
  }

  .posts .post {
    margin: 0 !important;
    padding: 0 !important;
    border: none !important;
    box-shadow: none !important;
    background: none !important;
  }

  .posts .post .entry {
    margin: 0 !important;
    padding: 0 !important;
    border: none !important;
    box-shadow: none !important;
    background: none !important;
  }

  /* CSS pro tlaÄÃ­tko kopÃ­rovat */
  .code-block-container {
    position: relative;
    margin: 20px 0;
  }
  .copy-button {
    position: absolute;
    top: 10px;
    right: 10px;
    padding: 8px 15px;
    background-color: #4CAF50;
    color: white;
    border: none;
    border-radius: 5px;
    cursor: pointer;
    font-size: 14px;
    z-index: 1;
  }
  .copy-button:hover {
    background-color: #45a049;
  }
  .copy-button:active {
    background-color: #3e8e41;
  }
  .copy-button.copied {
    background-color: #666;
  }
  .code-block-container pre {
    position: relative;
    padding-top: 40px;
  }
  .toast {
    position: fixed;
    bottom: 20px;
    left: 50%;
    transform: translateX(-50%);
    background-color: #333;
    color: white;
    padding: 12px 24px;
    border-radius: 5px;
    display: none;
    z-index: 1000;
  }
</style>
<!-- end feedwind code -->

<article class="post detailed">
  <h1>ProÄ je velikost kontextu u LLM tak dÅ¯leÅ¾itÃ¡?</h1>

  
  <div class="posts">
    <blockquote>
      <p>ğŸ’¡ FiremnÃ­ <a href="https://www.prolnuto.cz/">konzultace a workshopy o umÄ›lÃ© inteligenci</a>. Jak se vaÅ¡Ã­ firmy dotkne AI a jak se na to pÅ™ipravit?<br/>ğŸ‘‰ PoradÃ­me v <a href="https://www.prolnuto.cz/">Prolnuto.cz</a></p>
    </blockquote>
  </div>
  

  
  <!-- Zde se zobrazÃ­ obsah pro vÅ¡echny ostatnÃ­ kolekce neÅ¾ 'obrazy' -->
  <div>
    <p class="author_title">Patrick Zandl  Â·

5.
kvÄ›ten
  
2025 
    
    </p>

    
    <div class="post-tags">
      
      
        <a href="//rubrika/#AI">AI</a>
        &nbsp;
      
        <a href="//rubrika/#kontext">kontext</a>
        
      
    </div>
   
  </div>

  
  
  <div class="thumbnail-strip">
    <img src="https://res.cloudinary.com/dvwv5cne3/image/fetch/w_1200,h_300,c_fill,g_auto,f_auto,q_auto/https://www.marigold.cz/assets/llm-kontext.jpg" alt="ProÄ je velikost kontextu u LLM tak dÅ¯leÅ¾itÃ¡?">
  </div>
  


 
  <div class="quick-summary">
    <div class="quick-summary-header">
      <svg class="summary-icon" viewBox="0 0 24 24" width="24" height="24">
        <path fill="currentColor" d="M14,17H7V15H14M17,13H7V11H17M17,9H7V7H17M19,3H5C3.89,3 3,3.89 3,5V19A2,2 0 0,0 5,21H19A2,2 0 0,0 21,19V5C21,3.89 20.1,3 19,3Z" />
      </svg>
      <span>RychlÃ© shrnutÃ­ ÄlÃ¡nku</span>
    </div>
    <ul class="summary-points">
      
        <li>Kontext v LLM je pamÄ›Å¥ modelu pro zpracovÃ¡nÃ­ dat.</li>
      
        <li>DÃ©lka kontextu ovlivÅˆuje porozumÄ›nÃ­ a kvalitu vÃ½stupu.</li>
      
        <li>KvadratickÃ¡ sloÅ¾itost pozornosti omezuje dÃ©lku kontextu.</li>
      
        <li>VÃ½zkum hledÃ¡ optimalizace a alternativnÃ­ architektury pro delÅ¡Ã­ kontext.</li>
      
    </ul>
  </div>

  

<style>
.quick-summary {
    background: linear-gradient(145deg, #ffffff, #f5f5f5);
    border-radius: 12px;
    padding: 1.5rem;
    margin: 2rem 0;
    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1),
                0 2px 4px -1px rgba(0, 0, 0, 0.06);
}

.quick-summary-header {
    display: flex;
    align-items: center;
    margin-bottom: 1rem;
    color: #2d3748;
    font-weight: 600;
    font-size: 1.1rem;
}

.summary-icon {
    margin-right: 0.5rem;
    color: #4a5568;
}

.summary-points {
    margin: 0;
    padding: 0;
    list-style: none;
}

.summary-points li {
    position: relative;
    padding-left: 1.5rem;
    margin-bottom: 0.5rem;
    color: #4a5568;
    line-height: 1.5;
}

.summary-points li::before {
    content: "â€¢";
    position: absolute;
    left: 0;
    color: #667eea;
    font-weight: bold;
}

@media (prefers-color-scheme: dark) {
    .quick-summary {
        background: linear-gradient(145deg, #2d3748, #1a202c);
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.2);
    }
    
    .quick-summary-header {
        color: #e2e8f0;
    }
    
    .summary-icon,
    .summary-points li {
        color: #cbd5e0;
    }
    
    .summary-points li::before {
        color: #7f9cf5;
    }
}
</style>




  <div class="entry">
    <p>A pÅ™edevÅ¡Ã­m, proÄ je tak drahÃ© a zdlouhavÃ© zvyÅ¡ovat velikost kontextu? Tento ÄlÃ¡nek se podrobnÄ› zabÃ½vÃ¡ tÃ­m, co kontext znamenÃ¡, proÄ je jeho dÃ©lka kritickÃ¡, jakÃ© technickÃ© pÅ™ekÃ¡Å¾ky brÃ¡nÃ­ jeho neomezenÃ©mu rozÅ¡iÅ™ovÃ¡nÃ­ a jakÃ¡ Å™eÅ¡enÃ­ se v souÄasnosti vyvÃ­jejÃ­.</p>

<p><strong>Co konkrÃ©tnÄ› se v tomto ÄlÃ¡nku dozvÃ­te?</strong></p>
<ul id="markdown-toc">
  <li><a href="#co-je-kontext-a-proÄ-je-jeho-dÃ©lka-klÃ­ÄovÃ¡" id="markdown-toc-co-je-kontext-a-proÄ-je-jeho-dÃ©lka-klÃ­ÄovÃ¡">Co je kontext a proÄ je jeho dÃ©lka klÃ­ÄovÃ¡?</a>    <ul>
      <li><a href="#vÃ½znam-dÃ©lky-kontextu-pro-kvalitu-vÃ½stupu" id="markdown-toc-vÃ½znam-dÃ©lky-kontextu-pro-kvalitu-vÃ½stupu">VÃ½znam dÃ©lky kontextu pro kvalitu vÃ½stupu:</a></li>
      <li><a href="#aktuÃ¡lnÃ­-velikosti-kontextovÃ½ch-oken-a-ceny-kvÄ›ten-2025" id="markdown-toc-aktuÃ¡lnÃ­-velikosti-kontextovÃ½ch-oken-a-ceny-kvÄ›ten-2025">AktuÃ¡lnÃ­ velikosti kontextovÃ½ch oken a ceny (kvÄ›ten 2025)</a></li>
    </ul>
  </li>
  <li><a href="#jÃ¡dro-problÃ©mu-kvadratickÃ¡-sloÅ¾itost-mechanismu-pozornosti" id="markdown-toc-jÃ¡dro-problÃ©mu-kvadratickÃ¡-sloÅ¾itost-mechanismu-pozornosti">JÃ¡dro problÃ©mu: KvadratickÃ¡ sloÅ¾itost mechanismu pozornosti</a></li>
  <li><a href="#praktickÃ©-dÅ¯sledky-kvadratickÃ©-sloÅ¾itosti" id="markdown-toc-praktickÃ©-dÅ¯sledky-kvadratickÃ©-sloÅ¾itosti">PraktickÃ© dÅ¯sledky kvadratickÃ© sloÅ¾itosti</a></li>
  <li><a href="#souÄasnÃ©-pÅ™Ã­stupy-a-Å™eÅ¡enÃ­" id="markdown-toc-souÄasnÃ©-pÅ™Ã­stupy-a-Å™eÅ¡enÃ­">SouÄasnÃ© pÅ™Ã­stupy a Å™eÅ¡enÃ­</a>    <ul>
      <li><a href="#1-optimalizace-standardnÃ­-pozornosti" id="markdown-toc-1-optimalizace-standardnÃ­-pozornosti">1. Optimalizace standardnÃ­ pozornosti</a></li>
      <li><a href="#2-aproximace-pozornosti-Å™Ã­dkÃ¡-pozornost---sparse-attention" id="markdown-toc-2-aproximace-pozornosti-Å™Ã­dkÃ¡-pozornost---sparse-attention">2. Aproximace pozornosti (Å˜Ã­dkÃ¡ pozornost - Sparse Attention)</a></li>
      <li><a href="#3-alternativnÃ­-architektury-mimo-transformÃ¡tory" id="markdown-toc-3-alternativnÃ­-architektury-mimo-transformÃ¡tory">3. AlternativnÃ­ architektury (mimo transformÃ¡tory)</a></li>
      <li><a href="#4-retrieval-augmented-generation-rag" id="markdown-toc-4-retrieval-augmented-generation-rag">4. Retrieval-Augmented Generation (RAG)</a></li>
      <li><a href="#5-dalÅ¡Ã­-techniky" id="markdown-toc-5-dalÅ¡Ã­-techniky">5. DalÅ¡Ã­ techniky</a></li>
    </ul>
  </li>
  <li><a href="#vÃ½zvy-a-budoucÃ­-smÄ›Å™ovÃ¡nÃ­" id="markdown-toc-vÃ½zvy-a-budoucÃ­-smÄ›Å™ovÃ¡nÃ­">VÃ½zvy a budoucÃ­ smÄ›Å™ovÃ¡nÃ­</a></li>
  <li><a href="#zÃ¡vÄ›r" id="markdown-toc-zÃ¡vÄ›r">ZÃ¡vÄ›r</a></li>
</ul>

<p>VelkÃ© jazykovÃ© modely (LLM) jako GPT-4, Claude 3 nebo Gemini 2.5 se staly vÃ½konnÃ½mi nÃ¡stroji pro zpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka. Jejich schopnost generovat text, pÅ™eklÃ¡dat, odpovÃ­dat na otÃ¡zky a psÃ¡t kÃ³d je vyuÅ¾Ã­vÃ¡na v mnoha oblastech. Navzdory jejich pokroÄilÃ½m schopnostem vÅ¡ak narÃ¡Å¾ejÃ­ na vÃ½znamnÃ© omezenÃ­: efektivnÃ­ zpracovÃ¡nÃ­ velmi dlouhÃ½ch sekvencÃ­ dat, znÃ¡mÃ© jako â€œproblÃ©m dlouhÃ©ho kontextuâ€.</p>

<h2 id="co-je-kontext-a-proÄ-je-jeho-dÃ©lka-klÃ­ÄovÃ¡">Co je kontext a proÄ je jeho dÃ©lka klÃ­ÄovÃ¡?</h2>

<p>V pÅ™Ã­padÄ› LLM pÅ™edstavuje kontext (context window) veÅ¡kerÃ¡ data, kterÃ¡ mÃ¡ model k dispozici v danÃ©m okamÅ¾iku pro zpracovÃ¡nÃ­ a generovÃ¡nÃ­ odpovÄ›di. Funguje jako operaÄnÃ­ pamÄ›Å¥ modelu. Pokud si LLM chce nÄ›co pamatovat v rÃ¡mci rozhovoru, pÅ™edÃ¡vÃ¡ si to jako kontext, aÄkoliv to tÅ™eba nevidÃ­te. Pokud mÃ¡ LLM pracovat s vaÅ¡imi pÅ™edchozÃ­mi zprÃ¡vami v rÃ¡mci chatu, prostÄ› je pÅ™ibalÃ­ do posÃ­lanÃ½ch dat. Obsah kontextu typicky zahrnuje:</p>

<ol>
  <li>
    <p>VstupnÃ­ text (prompt): ZadÃ¡nÃ­ nebo otÃ¡zka od uÅ¾ivatele.</p>
  </li>
  <li>
    <p>Historie konverzace: PÅ™edchozÃ­ vÃ½mÄ›ny v rÃ¡mci aktuÃ¡lnÃ­ interakce. U nÄ›kterÃ½ch systÃ©mÅ¯ mÅ¯Å¾e zahrnovat i relevantnÃ­ informace z minulÃ½ch interakcÃ­ (napÅ™. pomocÃ­ explicitnÃ­ch pamÄ›Å¥ovÃ½ch mechanismÅ¯).</p>
  </li>
  <li>
    <p>PoskytnutÃ© dokumenty: ExternÃ­ texty, kterÃ© mÃ¡ model analyzovat, shrnout nebo z nich Äerpat informace (napÅ™. nahranÃ© PDF, webovÃ© strÃ¡nky).</p>
  </li>
  <li>
    <p>InternÃ­ instrukce: SystÃ©movÃ© prompty definujÃ­cÃ­ chovÃ¡nÃ­ modelu, jeho personu nebo specifickÃ© Ãºkoly.</p>
  </li>
  <li>
    <p>VygenerovanÃ½ text: ÄŒÃ¡st textu, kterou model sÃ¡m postupnÄ› generuje jako odpovÄ›Ä.</p>
  </li>
</ol>

<p>DÃ©lka kontextu, obvykle mÄ›Å™enÃ¡ v tokenech, definuje maximÃ¡lnÃ­ mnoÅ¾stvÃ­ informacÃ­, kterÃ© model mÅ¯Å¾e souÄasnÄ› zpracovat. Token je zÃ¡kladnÃ­ jednotka textu pro LLM, kterÃ¡ mÅ¯Å¾e odpovÃ­dat slovu, ÄÃ¡sti slova nebo interpunkÄnÃ­mu znamÃ©nku (pro hlubÅ¡Ã­ vysvÄ›tlenÃ­ viz ÄlÃ¡nek <a href="/ai/[tokeny](/ai/tokeny-versus-slova/)-versus-slova">Tokeny versus Slova</a>).</p>

<h3 id="vÃ½znam-dÃ©lky-kontextu-pro-kvalitu-vÃ½stupu">VÃ½znam dÃ©lky kontextu pro kvalitu vÃ½stupu:</h3>

<ul>
  <li>
    <p>PorozumÄ›nÃ­ souvislostem: DelÅ¡Ã­ kontext umoÅ¾Åˆuje modelu lÃ©pe zachytit sloÅ¾itÃ© vztahy, zÃ¡vislosti a nuance v rozsÃ¡hlÃ½ch textech.</p>
  </li>
  <li>
    <p>Konzistence: Schopnost udrÅ¾et jednotnÃ½ styl, tÃ©ma a faktickou sprÃ¡vnost napÅ™Ã­Ä dlouhÃ½mi konverzacemi nebo dokumenty.</p>
  </li>
  <li>
    <p>PÅ™esnost a relevance: PÅ™Ã­stup k vÄ›tÅ¡Ã­mu mnoÅ¾stvÃ­ relevantnÃ­ch informacÃ­ vede k pÅ™esnÄ›jÅ¡Ã­m a lÃ©pe zacÃ­lenÃ½m odpovÄ›dÃ­m.</p>
  </li>
  <li>
    <p>ZpracovÃ¡nÃ­ komplexnÃ­ch Ãºloh: Ãšlohy jako detailnÃ­ analÃ½za rozsÃ¡hlÃ½ch reportÅ¯, knih nebo kÃ³dovÃ½ch bÃ¡zÃ­ vyÅ¾adujÃ­ schopnost pojmout velkÃ© mnoÅ¾stvÃ­ dat najednou.</p>
  </li>
  <li>
    <p>OmezenÃ­ â€œ<a href="/ai/halucinace-ai/">halucinacÃ­</a>â€: PoskytnutÃ­ dostateÄnÃ©ho kontextu mÅ¯Å¾e snÃ­Å¾it tendenci modelu vymÃ½Å¡let si informace, kterÃ© nejsou ve vstupnÃ­ch datech.</p>
  </li>
</ul>

<h3 id="aktuÃ¡lnÃ­-velikosti-kontextovÃ½ch-oken-a-ceny-kvÄ›ten-2025">AktuÃ¡lnÃ­ velikosti kontextovÃ½ch oken a ceny (kvÄ›ten 2025)</h3>

<p>Velikost kontextovÃ©ho okna a cena jsou klÃ­ÄovÃ© parametry pÅ™i vÃ½bÄ›ru modelu. NÃ­Å¾e je uveden pÅ™ehled nÄ›kterÃ½ch populÃ¡rnÃ­ch modelÅ¯ s daty pÅ™evÃ¡Å¾nÄ› z OpenRouter (duben 2025):</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>KontextovÃ© okno (Max vstup)</strong></th>
      <th><strong>Max. vÃ½stup</strong></th>
      <th><strong>Cena vstupu ($/1M <a href="/ai/tokeny-versus-slova/">tokenÅ¯</a>)</strong></th>
      <th><strong>Cena vÃ½stupu ($/1M tokenÅ¯)</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>o3 (OpenAI)</td>
      <td>200 000</td>
      <td>100 000</td>
      <td>$10.00</td>
      <td>$40.00</td>
    </tr>
    <tr>
      <td>o4-Mini (OpenAI)</td>
      <td>200 000</td>
      <td>100 000</td>
      <td>$1.10</td>
      <td>$4.40</td>
    </tr>
    <tr>
      <td>o4-Mini High (OpenAI)</td>
      <td>200 000</td>
      <td>100 000</td>
      <td>$1.10</td>
      <td>$4.40</td>
    </tr>
    <tr>
      <td>GPT-4.1 (OpenAI)</td>
      <td>1 050 000</td>
      <td>33 000</td>
      <td>$2.00</td>
      <td>$8.00</td>
    </tr>
    <tr>
      <td>Claude 3.7 Sonnet</td>
      <td>200 000</td>
      <td>64 000</td>
      <td>$3.00</td>
      <td>$15.00</td>
    </tr>
    <tr>
      <td>Claude 3.7 Sonnet Think</td>
      <td>200 000</td>
      <td>64 000</td>
      <td>$3.00</td>
      <td>$15.00</td>
    </tr>
    <tr>
      <td>Gemini 2.5 Pro (Google)</td>
      <td>1 050 000</td>
      <td>66 000</td>
      <td>$1.25 - $2.50</td>
      <td>$10.00 - $15.00</td>
    </tr>
    <tr>
      <td>Grok 3 beta (xAI)</td>
      <td>131 000</td>
      <td>131 000</td>
      <td>$3.00</td>
      <td>$15.00</td>
    </tr>
    <tr>
      <td>Llama 4</td>
      <td>10 milionÅ¯</td>
      <td>-</td>
      <td>(Open Source)</td>
      <td>(Open Source)</td>
    </tr>
    <tr>
      <td>Jamba-1.5 (AI21, OS)</td>
      <td>256 000</td>
      <td>-</td>
      <td>(Open Source)</td>
      <td>(Open Source)</td>
    </tr>
  </tbody>
</table>

<p>PoznÃ¡mka: Ceny se mohou liÅ¡it v zÃ¡vislosti na poskytovateli API (zde OpenRouter) a aktuÃ¡lnÃ­m vytÃ­Å¾enÃ­. U Gemini 2.5 Pro jsou ceny uvedeny v rozsahu. Open-source modely nemajÃ­ pÅ™Ã­mÃ© ceny za <a href="/ai/tokeny-versus-slova/">token</a>, ale nÃ¡klady na jejich provoz. Hodnota u LLAMA 4 je velmi optimistickÃ¡, model na to nebyl Å™Ã¡dnÄ› testovÃ¡n a vÃ½sledky nejsou pÅ™Ã­liÅ¡ kvalitnÃ­.</p>

<p>Je dÅ¯leÅ¾itÃ© poznamenat, Å¾e nominÃ¡lnÃ­ dÃ©lka kontextovÃ©ho okna nemusÃ­ vÅ¾dy odpovÃ­dat efektivnÃ­ schopnosti modelu vyuÅ¾Ã­vat informace z celÃ©ho kontextu. Testy jako <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">Needle In A Haystack</a> (NIAH) ukazujÃ­, Å¾e nÄ›kterÃ© modely majÃ­ problÃ©my s vyhledÃ¡vÃ¡nÃ­m informacÃ­ umÃ­stÄ›nÃ½ch uprostÅ™ed velmi dlouhÃ©ho kontextu (tzv. â€œlost in the middleâ€ problÃ©m), i kdyÅ¾ se tento problÃ©m postupnÄ› daÅ™Ã­ zmÃ­rÅˆovat.</p>

<p>UÅ¾ teÄ je tedy zÅ™ejmÃ©, Å¾e na rozsahu kontextu zÃ¡leÅ¾Ã­, pÅ™iÄemÅ¾ â€œkontextâ€ nenÃ­ jen to, co zadÃ¡te do Prompt okna v ChatGPT, ale takÃ© spousta dodateÄnÃ½ch dat, kterÃ½mi ChatGPT vÃ¡Å¡ dotaz â€œobalÃ­â€, aby vyuÅ¾il toho, co vÃ­ o vÃ¡s, o tom, co vyÅ¾adujete atd. NabÃ­zÃ­ se tedy otÃ¡zka, proÄ se jednoduÅ¡e velikost kontextovÃ©ho okna nerozÅ¡Ã­Å™Ã­ na maximum! OdpovÄ›Ä? ProtoÅ¾e to nenÃ­ vÅ¯bec jednoduchÃ© a pÅ™edevÅ¡Ã­m to stojÃ­ hromadu penÄ›z pÅ™i pouÅ¾Ã­vÃ¡nÃ­! Jak to?</p>

<h2 id="jÃ¡dro-problÃ©mu-kvadratickÃ¡-sloÅ¾itost-mechanismu-pozornosti">JÃ¡dro problÃ©mu: KvadratickÃ¡ sloÅ¾itost mechanismu pozornosti</h2>

<p>ZÃ¡kladem vÄ›tÅ¡iny modernÃ­ch LLM je architektura transformÃ¡toru, pÅ™edstavenÃ¡ v roce 2017 v ÄlÃ¡nku â€œAttention Is All You Needâ€. KlÃ­Äovou inovacÃ­ tÃ©to architektury je mechanismus sebe-pozornosti (self-attention). Ten umoÅ¾Åˆuje modelu vÃ¡Å¾it dÅ¯leÅ¾itost vÅ¡ech ostatnÃ­ch tokenÅ¯ v kontextu pÅ™i zpracovÃ¡nÃ­ kaÅ¾dÃ©ho jednotlivÃ©ho tokenu.</p>

<p>Jak to funguje (velmi zjednoduÅ¡enÄ›): model se pÅ™i ÄtenÃ­ kaÅ¾dÃ©ho slova â€œdÃ­vÃ¡â€ na vÅ¡echna ostatnÃ­ slova v textu, aby pochopil jeho vÃ½znam v danÃ© vÄ›tÄ›. Tedy poÄÃ­tÃ¡ jej vÅ¯Äi vÅ¡em pÅ™edchozÃ­m slovÅ¯m. TÃ­mto zpÅ¯sobem zjiÅ¡Å¥uje, kterÃ¡ slova jsou pro aktuÃ¡lnÃ­ slovo nejdÅ¯leÅ¾itÄ›jÅ¡Ã­ a jak spolu souvisÃ­. Proto prodluÅ¾ovÃ¡nÃ­ textu zvyÅ¡uje nÃ¡roÄnost vÃ½poÄtÅ¯ exponenciÃ¡lnÄ›.</p>

<p>Jak funguje (mÃ©nÄ› zjednoduÅ¡enÄ›): Pro kaÅ¾dÃ½ token model vypoÄÃ­tÃ¡ tÅ™i vektory: Query (Q), Key (K) a Value (V). PotÃ© pro kaÅ¾dÃ½ token (reprezentovanÃ½ jeho Q vektorem) vypoÄÃ­tÃ¡ skÃ³re pozornosti vÅ¯Äi vÅ¡em ostatnÃ­m tokenÅ¯m (porovnÃ¡nÃ­m Q s K vektory vÅ¡ech tokenÅ¯). Tato skÃ³re se normalizujÃ­ (typicky pomocÃ­ funkce softmax) a pouÅ¾ijÃ­ se k vytvoÅ™enÃ­ vÃ¡Å¾enÃ©ho souÄtu V vektorÅ¯ vÅ¡ech tokenÅ¯. VÃ½sledkem je novÃ¡ reprezentace tokenu, kterÃ¡ zohledÅˆuje jeho vztah ke vÅ¡em ostatnÃ­m tokenÅ¯m v kontextu.</p>

<p>ProblÃ©m Å¡kÃ¡lovÃ¡nÃ­: Tento mechanismus je extrÃ©mnÄ› efektivnÃ­ pro zachycenÃ­ zÃ¡vislostÃ­ v textu, ale mÃ¡ zÃ¡sadnÃ­ nevÃ½hodu: jeho vÃ½poÄetnÃ­ a pamÄ›Å¥ovÃ¡ sloÅ¾itost roste kvadraticky s dÃ©lkou sekvence (N, poÄet tokenÅ¯).</p>

<ul>
  <li>
    <p>VÃ½poÄetnÃ­ sloÅ¾itost: PoÄet operacÃ­ potÅ™ebnÃ½ch pro vÃ½poÄet matice pozornosti je ÃºmÄ›rnÃ½ O(N2). Pro kaÅ¾dÃ½ z N tokenÅ¯ musÃ­me vypoÄÃ­tat jeho vztah k N tokenÅ¯m (vÄetnÄ› sebe sama).</p>
  </li>
  <li>
    <p>PamÄ›Å¥ovÃ¡ sloÅ¾itost: Model si musÃ­ bÄ›hem vÃ½poÄtu uchovÃ¡vat matici pozornosti o velikosti NÃ—N, coÅ¾ vede k pamÄ›Å¥ovÃ© nÃ¡roÄnosti O(N2).</p>
  </li>
</ul>

<p>Ilustrace dopadu:</p>

<p>PÅ™esnÃ© Äasy zpracovÃ¡nÃ­ zÃ¡visÃ­ na mnoha faktorech (konkrÃ©tnÃ­ model, hardware - napÅ™. typ GPU, optimalizace - napÅ™. FlashAttention, datovÃ½ typ vÃ½poÄtÅ¯), ale pro ilustraci Å™Ã¡dovÃ©ho nÃ¡rÅ¯stu nÃ¡roÄnosti na vÃ½konnÃ©m GPU (napÅ™. NVIDIA H100/B100):</p>

<ul>
  <li>
    <p>Kontext 1 000 tokenÅ¯: VyÅ¾aduje Å™Ã¡dovÄ› 10002=1000000 operacÃ­/pamÄ›Å¥ovÃ½ch jednotek. ZpracovÃ¡nÃ­ (inference) mÅ¯Å¾e trvat zlomky sekundy aÅ¾ jednotky sekund.</p>
  </li>
  <li>
    <p>Kontext 10 000 tokenÅ¯: VyÅ¾aduje Å™Ã¡dovÄ› 100002=100000000 operacÃ­/pamÄ›Å¥ovÃ½ch jednotek (100x vÃ­ce). Doba zpracovÃ¡nÃ­ se mÅ¯Å¾e pohybovat v jednotkÃ¡ch aÅ¾ desÃ­tkÃ¡ch sekund.</p>
  </li>
  <li>
    <p>Kontext 100 000 tokenÅ¯: VyÅ¾aduje Å™Ã¡dovÄ› 1000002=10000000000 operacÃ­/pamÄ›Å¥ovÃ½ch jednotek (10 000x vÃ­ce neÅ¾ pro 1k tokenÅ¯). Doba zpracovÃ¡nÃ­ mÅ¯Å¾e dosahovat desÃ­tek sekund aÅ¾ nÄ›kolika minut.</p>
  </li>
  <li>
    <p>Kontext 1 000 000 tokenÅ¯ (jako u Gemini Pro, GPT-4.1): VyÅ¾aduje Å™Ã¡dovÄ› 10000002=1000000000000 (bilion) operacÃ­/pamÄ›Å¥ovÃ½ch jednotek. Doba zpracovÃ¡nÃ­ se mÅ¯Å¾e pohybovat v Å™Ã¡du nÄ›kolika minut aÅ¾ desÃ­tek minut, silnÄ› zÃ¡visÃ­ na optimalizacÃ­ch a poÄtu pouÅ¾itÃ½ch akcelerÃ¡torÅ¯.</p>
  </li>
</ul>

<p>Tento kvadratickÃ½ nÃ¡rÅ¯st pÅ™edstavuje obrovskou bariÃ©ru pro neomezenÃ© prodluÅ¾ovÃ¡nÃ­ kontextovÃ©ho okna u standardnÃ­ch <a href="/ai/transformatory/">transformÃ¡torÅ¯</a>, jak z hlediska vÃ½poÄetnÃ­ nÃ¡roÄnosti (Äas), tak pamÄ›Å¥ovÃ½ch poÅ¾adavkÅ¯.</p>

<h2 id="praktickÃ©-dÅ¯sledky-kvadratickÃ©-sloÅ¾itosti">PraktickÃ© dÅ¯sledky kvadratickÃ© sloÅ¾itosti</h2>

<p>KvadratickÃ¡ sloÅ¾itost mechanismu pozornosti mÃ¡ nÄ›kolik zÃ¡sadnÃ­ch praktickÃ½ch dopadÅ¯. PÅ™edevÅ¡Ã­m vede k enormnÃ­ vÃ½poÄetnÃ­ nÃ¡roÄnosti a latenci pÅ™i zpracovÃ¡nÃ­ dlouhÃ½ch kontextÅ¯. VyÅ¾aduje to obrovskÃ© mnoÅ¾stvÃ­ vÃ½poÄetnÃ­ch zdrojÅ¯, jako jsou GPU nebo TPU, coÅ¾ se projevuje delÅ¡Ã­ dobou odezvy pÅ™i generovÃ¡nÃ­ odpovÄ›dÃ­, vysokou spotÅ™ebou energie a nÃ¡slednÄ› i vysokÃ½mi nÃ¡klady na trÃ©nink a inferenci modelÅ¯ kvÅ¯li potÅ™ebÄ› vÃ½konnÃ©ho a drahÃ©ho hardwaru. Proto jsou modely, kterÃ© majÃ­ velkÃ© mnoÅ¾stvÃ­ parametrÅ¯ a umoÅ¾ÅˆujÃ­ zpracovat velkÃ½ kontext, takÃ© zpravidla vÃ½raznÄ› draÅ¾Å¡Ã­.</p>

<p>DalÅ¡Ã­m vÃ½znamnÃ½m dÅ¯sledkem jsou vysokÃ© pamÄ›Å¥ovÃ© nÃ¡roky, zejmÃ©na na VRAM akcelerÃ¡torÅ¯. Model musÃ­ uchovÃ¡vat matice pozornosti a mezivÃ½poÄty (aktivace) pro vÅ¡echny tokeny v kontextu. NapÅ™Ã­klad optimalizace zvanÃ¡ KV cache, kterÃ¡ uklÃ¡dÃ¡ vypoÄtenÃ© vektory pro zrychlenÃ­ inference, vyÅ¾aduje pro model Llama 3 70B s kontextem 128 000 tokenÅ¯ stovky gigabajtÅ¯ VRAM. Pro kontexty v Å™Ã¡du milionÅ¯ tokenÅ¯ tyto nÃ¡roky dÃ¡le dramaticky rostou, coÅ¾ omezuje nasazenÃ­ takovÃ½ch modelÅ¯ pouze na hardware s masivnÃ­ pamÄ›Å¥ovou kapacitou.</p>

<p>Tyto zvÃ½Å¡enÃ© vÃ½poÄetnÃ­ a pamÄ›Å¥ovÃ© nÃ¡roky se promÃ­tajÃ­ do ekonomickÃ½ch dopadÅ¯. PoskytovatelÃ© LLM sluÅ¾eb musÃ­ tyto nÃ¡klady zohlednit, a proto zpravidla ÃºÄtujÃ­ vyÅ¡Å¡Ã­ ceny za pouÅ¾itÃ­ modelÅ¯ s delÅ¡Ã­mi kontextovÃ½mi okny nebo za zpracovÃ¡nÃ­ tokenÅ¯ pÅ™esahujÃ­cÃ­ch urÄitou hranici, jak je vidÄ›t v pÅ™ehledovÃ© tabulce cen.</p>

<p>Nakonec, i kdyÅ¾ model technicky zvlÃ¡dne zpracovat velmi dlouhÃ½ kontext, objevuje se problÃ©m znÃ¡mÃ½ jako â€œLost in the Middleâ€. EmpirickÃ© testy ukazujÃ­, Å¾e schopnost modelu efektivnÄ› vyuÅ¾Ã­vat informace mÅ¯Å¾e klesat, pokud jsou tyto informace umÃ­stÄ›ny uprostÅ™ed velmi dlouhÃ©ho vstupnÃ­ho textu. Modely Äasto vykazujÃ­ tendenci lÃ©pe pracovat s informacemi uvedenÃ½mi na zaÄÃ¡tku nebo na konci kontextovÃ©ho okna.</p>

<h2 id="souÄasnÃ©-pÅ™Ã­stupy-a-Å™eÅ¡enÃ­">SouÄasnÃ© pÅ™Ã­stupy a Å™eÅ¡enÃ­</h2>

<p>VÃ½zkum a vÃ½voj se intenzivnÄ› zamÄ›Å™ujÃ­ na zmÃ­rnÄ›nÃ­ nebo pÅ™ekonÃ¡nÃ­ O(N2) bariÃ©ry, protoÅ¾e pÅ™ekroÄenÃ­ limitÅ¯ pÅ™inÃ¡Å¡enÃ½ch kontextem by umoÅ¾Åˆovalo vÃ½raznÄ› rozÅ¡Ã­Å™it Ãºlohy, v nichÅ¾ AI / LLM excelujÃ­. A takÃ© dosÃ¡hnout lepÅ¡Ã­ ekonomiky. HlavnÃ­ smÄ›ry vÃ½zkumu jsou zhruba nÃ¡sledujÃ­cÃ­:</p>

<h3 id="1-optimalizace-standardnÃ­-pozornosti">1. Optimalizace standardnÃ­ pozornosti</h3>

<ul>
  <li>
    <p>FlashAttention (a jeho nÃ¡sledovnÃ­ci FlashAttention-2, FlashAttention-3): Algoritmus, kterÃ½ restrukturalizuje vÃ½poÄet pozornosti tak, aby lÃ©pe vyuÅ¾Ã­val hierarchii pamÄ›ti GPU. Minimalizuje pomalÃ© pÅ™esuny dat mezi HBM (High Bandwidth Memory) a SRAM (on-chip pamÄ›Å¥) pomocÃ­ technik jako tiling a recomputation. VÃ½raznÄ› zrychluje vÃ½poÄet a sniÅ¾uje pamÄ›Å¥ovÃ© nÃ¡roky bez zmÄ›ny matematiky pozornosti, takÅ¾e vÃ½sledky jsou (tÃ©mÄ›Å™) identickÃ© se standardnÃ­ pozornostÃ­. Stal se de facto standardem pro trÃ©nink a inferenci modernÃ­ch LLM.</p>
  </li>
  <li>
    <p>KV Cache (Key-Value Cache): Optimalizace pro inferenci (generovÃ¡nÃ­ textu). MÃ­sto pÅ™epoÄÃ­tÃ¡vÃ¡nÃ­ K a V vektorÅ¯ pro vÅ¡echny pÅ™edchozÃ­ tokeny pÅ™i generovÃ¡nÃ­ kaÅ¾dÃ©ho novÃ©ho tokenu se tyto vektory uklÃ¡dajÃ­ do pamÄ›ti (cache). To sniÅ¾uje vÃ½poÄetnÃ­ nÃ¡roÄnost generovÃ¡nÃ­ z O(N2) na O(N) pro kaÅ¾dÃ½ novÃ½ token, ale pamÄ›Å¥ovÃ¡ nÃ¡roÄnost pro uloÅ¾enÃ­ cache zÅ¯stÃ¡vÃ¡ O(N).</p>
  </li>
</ul>

<h3 id="2-aproximace-pozornosti-Å™Ã­dkÃ¡-pozornost---sparse-attention">2. Aproximace pozornosti (Å˜Ã­dkÃ¡ pozornost - Sparse Attention)</h3>

<p>CÃ­lem tohoto pÅ™Ã­stupu je snÃ­Å¾it poÄet pÃ¡rÅ¯ tokenÅ¯, mezi kterÃ½mi se poÄÃ­tÃ¡ pozornost, a tÃ­m prolomit kvadratickou sloÅ¾itost vÃ½poÄtu plnÃ© matice pozornosti. MÃ­sto aby kaÅ¾dÃ½ token interagoval se vÅ¡emi ostatnÃ­mi, interakce se omezÃ­ na â€œÅ™Ã­dkÃ½â€ vzor, kterÃ½ se snaÅ¾Ã­ zachovat nejdÅ¯leÅ¾itÄ›jÅ¡Ã­ informace. Longformer napÅ™Ã­klad kombinuje lokÃ¡lnÃ­ pozornost, kde kaÅ¾dÃ½ token interaguje pouze se svÃ½mi nejbliÅ¾Å¡Ã­mi sousedy v rÃ¡mci â€œklouzavÃ©ho oknaâ€, s globÃ¡lnÃ­ pozornostÃ­ pro nÄ›kolik pÅ™edem urÄenÃ½ch tokenÅ¯ (napÅ™. speciÃ¡lnÃ­ tokeny jako [CLS]). Tyto globÃ¡lnÃ­ tokeny mohou interagovat se vÅ¡emi ostatnÃ­mi tokeny a vÅ¡echny ostatnÃ­ tokeny mohou interagovat s nimi, coÅ¾ umoÅ¾Åˆuje pÅ™enos informacÃ­ napÅ™Ã­Ä celou sekvencÃ­ pÅ™i zachovÃ¡nÃ­ pÅ™evÃ¡Å¾nÄ› lokÃ¡lnÃ­ch vÃ½poÄtÅ¯. PodobnÄ› BigBird pouÅ¾Ã­vÃ¡ kombinaci tÅ™Ã­ typÅ¯ Å™Ã­dkÃ© pozornosti: nÃ¡hodnou pozornost (kaÅ¾dÃ½ token interaguje s malÃ½m nÃ¡hodnÃ½m vzorkem ostatnÃ­ch tokenÅ¯), okÃ©nkovou pozornost (podobnÄ› jako Longformer) a globÃ¡lnÃ­ pozornost. Tato kombinace mÃ¡ teoretickÃ© zÃ¡klady a snaÅ¾Ã­ se efektivnÄ› aproximovat vlastnosti plnÃ© matice pozornosti. JinÃ© metody, jako Routing Transformer nebo Sinkhorn Transformer, jdou jeÅ¡tÄ› dÃ¡l a snaÅ¾Ã­ se dynamicky â€œnauÄitâ€ nebo optimalizovat, kterÃ© pÃ¡ry tokenÅ¯ jsou nejdÅ¯leÅ¾itÄ›jÅ¡Ã­ pro vÃ½poÄet pozornosti, napÅ™Ã­klad pomocÃ­ technik smÄ›rovÃ¡nÃ­ informacÃ­ nebo metod inspirovanÃ½ch optimÃ¡lnÃ­m transportem (Sinkhorn), ÄÃ­mÅ¾ se vÃ½poÄty soustÅ™edÃ­ pouze na nejrelevantnÄ›jÅ¡Ã­ ÄÃ¡sti matice pozornosti.</p>

<p>AÄkoliv tyto metody mohou dosÃ¡hnout lineÃ¡rnÃ­ (O(N)) nebo tÃ©mÄ›Å™ lineÃ¡rnÃ­ (O(NlogN)) vÃ½poÄetnÃ­ sloÅ¾itosti, kompromisem mÅ¯Å¾e bÃ½t mÃ­rnÃ© snÃ­Å¾enÃ­ kvality modelu oproti plnÃ© pozornosti. DÅ¯vodem je, Å¾e pÅ™eddefinovanÃ© nebo aproximovanÃ© vzory Å™Ã­dkÃ© pozornosti nemusÃ­ vÅ¾dy dokonale zachytit vÅ¡echny relevantnÃ­ dlouhodobÃ© zÃ¡vislosti v textu, kterÃ© by plnÃ¡ pozornost identifikovala.</p>

<h3 id="3-alternativnÃ­-architektury-mimo-transformÃ¡tory">3. AlternativnÃ­ architektury (mimo <a href="/ai/transformatory/">transformÃ¡tory</a>)</h3>

<p>HledÃ¡nÃ­ architektur, kterÃ© nejsou zaloÅ¾eny na standardnÃ­ O(N2) pozornosti:</p>

<ul>
  <li>
    <p>RekurentnÃ­ <a href="/ai/neuronove-site/">[neuronovÃ© sÃ­tÄ›](/ai/neuronove-site/)</a> (RNN) / LSTM / GRU: Tyto sÃ­tÄ› pÅ™edstavujÃ­ starÅ¡Ã­ pÅ™Ã­stup ke zpracovÃ¡nÃ­ sekvencÃ­, jehoÅ¾ koÅ™eny sahajÃ­ aÅ¾ do 80. a 90. let 20. stoletÃ­. ZÃ¡kladnÃ­ myÅ¡lenka RNN spoÄÃ­vÃ¡ ve zpracovÃ¡nÃ­ sekvence krok za krokem (token po tokenu), pÅ™iÄemÅ¾ si sÃ­Å¥ udrÅ¾uje vnitÅ™nÃ­ â€œstavâ€ nebo â€œpamÄ›Å¥â€, kterÃ¡ shrnuje informace z pÅ™edchozÃ­ch krokÅ¯. Tento stav se aktualizuje pÅ™i zpracovÃ¡nÃ­ kaÅ¾dÃ©ho novÃ©ho tokenu. DÃ­ky tomu mÃ¡ zpracovÃ¡nÃ­ inherentnÄ› lineÃ¡rnÃ­ vÃ½poÄetnÃ­ sloÅ¾itost (O(N)), protoÅ¾e vÃ½poÄet pro kaÅ¾dÃ½ token zÃ¡visÃ­ pouze na aktuÃ¡lnÃ­m vstupu a pÅ™edchozÃ­m stavu, nikoli na vÅ¡ech pÅ™edchozÃ­ch tokenech souÄasnÄ›. Varianty jako LSTM (Long Short-Term Memory, Hochreiter &amp; Schmidhuber, 1997) a GRU (Gated Recurrent Unit) byly vyvinuty pozdÄ›ji, aby Å™eÅ¡ily klÃ­ÄovÃ½ problÃ©m zÃ¡kladnÃ­ch RNN: tzv. mizenÃ­ nebo explozi gradientÅ¯ (vanishing/exploding gradients), kterÃ© brÃ¡nily uÄenÃ­ zÃ¡vislostÃ­ na dlouhÃ© vzdÃ¡lenosti v sekvenci. PÅ™estoÅ¾e LSTM a GRU tento problÃ©m zmÃ­rnily pomocÃ­ speciÃ¡lnÃ­ch â€œbranâ€ (gates), kterÃ© Å™Ã­dÃ­ tok informacÃ­ a gradientÅ¯, stÃ¡le mÄ›ly svÃ© limity. HlavnÃ­ nevÃ½hodou oproti transformÃ¡torÅ¯m se ukÃ¡zala bÃ½t jejich sekvenÄnÃ­ povaha, kterÃ¡ znesnadÅˆuje paralelizaci vÃ½poÄtÅ¯ bÄ›hem trÃ©ninku na modernÃ­m hardwaru (GPU/TPU). TransformÃ¡tory, kterÃ© mohou zpracovÃ¡vat vÅ¡echny tokeny v sekvenci vÃ­cemÃ©nÄ› paralelnÄ› dÃ­ky mechanismu pozornosti, se tak staly efektivnÄ›jÅ¡Ã­ pro trÃ©nink na velkÃ½ch datech a dosÃ¡hly lepÅ¡Ã­ch vÃ½sledkÅ¯ v mnoha ÃºlohÃ¡ch. ModernÃ­ vÃ½zkum se vÅ¡ak k RNN a jejich vylepÅ¡enÃ­m ÄÃ¡steÄnÄ› vracÃ­, snaÅ¾Ã­ se kombinovat jejich vÃ½hody (lineÃ¡rnÃ­ sloÅ¾itost) s novÃ½mi technikami pro zlepÅ¡enÃ­ vÃ½konu a paralelizace.</p>
  </li>
  <li>
    <p>State Space Models (SSM): TÅ™Ã­da modelÅ¯ inspirovanÃ¡ teoriÃ­ Å™Ã­zenÃ­.</p>
  </li>
  <li>
    <p>Mamba: PopulÃ¡rnÃ­ SSM architektura, kterÃ¡ dosahuje lineÃ¡rnÃ­ sloÅ¾itosti Å¡kÃ¡lovÃ¡nÃ­ s dÃ©lkou sekvence a zÃ¡roveÅˆ si zachovÃ¡vÃ¡ schopnost modelovat dlouhÃ© zÃ¡vislosti dÃ­ky selektivnÃ­mu mechanismu stavu. Ukazuje slibnÃ© vÃ½sledky, zejmÃ©na v ÃºlohÃ¡ch vyÅ¾adujÃ­cÃ­ch dlouhÃ½ kontext. ExistujÃ­ i novÄ›jÅ¡Ã­ varianty a vylepÅ¡enÃ­ (Mamba-2, etc.).</p>
  </li>
  <li>
    <p>HybridnÃ­ modely: KombinujÃ­ rÅ¯znÃ© pÅ™Ã­stupy.</p>
  </li>
  <li>
    <p>Jamba (AI21 Labs): Architektura, kterÃ¡ stÅ™Ã­dÃ¡ vrstvy standardnÃ­ pozornosti (Transformer bloky) s Mamba bloky. CÃ­lem je zkombinovat sÃ­lu pozornosti pro lokÃ¡lnÃ­ a komplexnÃ­ vztahy s efektivitou Mamby pro dlouhÃ© sekvence. VÃ½sledkem je model, kterÃ½ zvlÃ¡dÃ¡ dlouhÃ½ kontext (256k tokenÅ¯) s vÃ½raznÄ› niÅ¾Å¡Ã­mi pamÄ›Å¥ovÃ½mi nÃ¡roky neÅ¾ ÄistÃ½ transformÃ¡tor podobnÃ© velikosti. OÄekÃ¡vajÃ­ se nÃ¡stupci.</p>
  </li>
</ul>

<h3 id="4-retrieval-augmented-generation-rag">4. Retrieval-Augmented Generation (RAG)</h3>

<p>AlternativnÃ­ pÅ™Ã­stup, kterÃ½ se nesnaÅ¾Ã­ vtÄ›snat veÅ¡kerÃ© informace do kontextovÃ©ho okna modelu. MÃ­sto toho postupuje zhruba nÃ¡sledovnÄ›:</p>

<ol>
  <li>
    <p>RozsÃ¡hlÃ¡ databÃ¡ze znalostÃ­ (napÅ™. dokumenty, webovÃ© strÃ¡nky) je indexovÃ¡na a uloÅ¾ena ve vektorovÃ© databÃ¡zi.</p>
  </li>
  <li>
    <p>KdyÅ¾ pÅ™ijde dotaz uÅ¾ivatele, systÃ©m nejprve vyhledÃ¡ nejrelevantnÄ›jÅ¡Ã­ ÄÃ¡sti informacÃ­ z databÃ¡ze (retrieval).</p>
  </li>
  <li>
    <p>Tyto relevantnÃ­ ÄÃ¡sti (snippets) jsou pak spolu s pÅ¯vodnÃ­m dotazem vloÅ¾eny do kontextovÃ©ho okna LLM.</p>
  </li>
  <li>
    <p>LLM pouÅ¾ije tyto poskytnutÃ© informace k vygenerovÃ¡nÃ­ odpovÄ›di.</p>
  </li>
</ol>

<p>VÃ½hody RAG: MÅ¯Å¾e pracovat s prakticky neomezenÃ½m mnoÅ¾stvÃ­m externÃ­ch dat bez nutnosti extrÃ©mnÄ› dlouhÃ©ho kontextovÃ©ho okna. Je snadnÄ›jÅ¡Ã­ aktualizovat znalosti (staÄÃ­ aktualizovat databÃ¡zi).</p>

<p>NevÃ½hody RAG: Kvalita zÃ¡visÃ­ na ÃºspÄ›Å¡nosti vyhledÃ¡vacÃ­ho kroku. Model nemÃ¡ â€œholistickÃ½â€ pohled na celÃ½ dokument, jen na vybranÃ© ÄÃ¡sti. NemusÃ­ bÃ½t vhodnÃ½ pro Ãºlohy vyÅ¾adujÃ­cÃ­ syntÃ©zu informacÃ­ napÅ™Ã­Ä celÃ½m rozsÃ¡hlÃ½m textem.</p>

<h3 id="5-dalÅ¡Ã­-techniky">5. DalÅ¡Ã­ techniky</h3>

<ul>
  <li>
    <p>Context Compression: Metody, kterÃ© se snaÅ¾Ã­ zkrÃ¡tit prompt nebo odstranit mÃ©nÄ› relevantnÃ­ ÄÃ¡sti kontextu pÅ™ed jeho pÅ™edÃ¡nÃ­m modelu.</p>
  </li>
  <li>
    <p>Ring Attention: Technika pro distribuovanÃ½ trÃ©nink/inferenci, kterÃ¡ umoÅ¾Åˆuje rozdÄ›lit zpracovÃ¡nÃ­ dlouhÃ©ho kontextu mezi vÃ­ce akcelerÃ¡torÅ¯ (GPU) tak, Å¾e kaÅ¾dÃ½ zpracovÃ¡vÃ¡ ÄÃ¡st sekvence, ale mohou si efektivnÄ› vymÄ›Åˆovat informace potÅ™ebnÃ© pro vÃ½poÄet pozornosti napÅ™Ã­Ä celou sekvencÃ­.</p>
  </li>
</ul>

<h2 id="vÃ½zvy-a-budoucÃ­-smÄ›Å™ovÃ¡nÃ­">VÃ½zvy a budoucÃ­ smÄ›Å™ovÃ¡nÃ­</h2>

<p>Navzdory pokrokÅ¯m zÅ¯stÃ¡vÃ¡ efektivnÃ­ a kvalitnÃ­ zpracovÃ¡nÃ­ dlouhÃ©ho kontextu klÃ­Äovou vÃ½zvou. BudoucÃ­ vÃ½voj se pravdÄ›podobnÄ› zamÄ›Å™Ã­ na nÄ›kolik oblastÃ­. Bude pokraÄovat zlepÅ¡ovÃ¡nÃ­ efektivity prostÅ™ednictvÃ­m dalÅ¡Ã­ch optimalizacÃ­ algoritmÅ¯ jako FlashAttention, vÃ½voje novÃ½ch aproximacÃ­ pozornosti a zdokonalovÃ¡nÃ­ alternativnÃ­ch architektur typu SSM a hybridnÃ­ch modelÅ¯. SouÄasnÄ› bude kladen dÅ¯raz na zlepÅ¡ovÃ¡nÃ­ kvality, zejmÃ©na na Å™eÅ¡enÃ­ problÃ©mu â€œlost in the middleâ€ a zajiÅ¡tÄ›nÃ­ spolehlivÃ©ho vyuÅ¾itÃ­ informacÃ­ z celÃ©ho kontextu, coÅ¾ podpoÅ™Ã­ i vÃ½voj lepÅ¡Ã­ch evaluaÄnÃ­ch metrik. OÄekÃ¡vÃ¡ se takÃ© hardwarovÃ¡ ko-evoluce s vÃ½vojem specializovanÃ½ch akcelerÃ¡torÅ¯ s vÄ›tÅ¡Ã­ pamÄ›tÃ­ a propustnostÃ­, optimalizovanÃ½ch pro LLM. DÃ¡le se bude prohlubovat kombinace pÅ™Ã­stupÅ¯, napÅ™Ã­klad hledÃ¡nÃ­ synergiÃ­ mezi modely s dlouhÃ½m kontextem a technikami RAG pro lepÅ¡Ã­ syntÃ©zu informacÃ­. V neposlednÃ­ Å™adÄ› bude pokraÄovat hledÃ¡nÃ­ fundamentÃ¡lnÃ­ch prÅ¯lomÅ¯ a zcela novÃ½ch paradigmat pro zpracovÃ¡nÃ­ sekvenÄnÃ­ch dat, kterÃ¡ by mohla pÅ™ekonat souÄasnÃ¡ omezenÃ­.</p>

<h2 id="zÃ¡vÄ›r">ZÃ¡vÄ›r</h2>

<p>Schopnost pracovat s dlouhÃ½m kontextem je zÃ¡sadnÃ­ pro posun LLM smÄ›rem k hlubÅ¡Ã­mu porozumÄ›nÃ­ a Å™eÅ¡enÃ­ komplexnÄ›jÅ¡Ã­ch Ãºloh. KvadratickÃ¡ sloÅ¾itost standardnÃ­ho mechanismu pozornosti v architektuÅ™e transformÃ¡toru pÅ™edstavuje dosti podstatnou pÅ™ekÃ¡Å¾ku, kterÃ¡ vede k vysokÃ½m vÃ½poÄetnÃ­m, pamÄ›Å¥ovÃ½m a finanÄnÃ­m nÃ¡kladÅ¯m. SouÄasnÃ½ vÃ½zkum pÅ™inÃ¡Å¡Ã­ Å™adu inovativnÃ­ch Å™eÅ¡enÃ­, od optimalizacÃ­ stÃ¡vajÃ­cÃ­ch metod (FlashAttention) pÅ™es aproximace (Sparse Attention) aÅ¾ po zcela novÃ© architektury (Mamba, Jamba) a doplÅˆkovÃ© techniky (RAG). V kaÅ¾dÃ©m pÅ™Ã­padÄ› je tu jeÅ¡tÄ› mnoho pÅ™Ã­leÅ¾itostÃ­, jak mÅ¯Å¾ete prosadit svÅ¯j nÃ¡pad a nabÃ­dnout novÃ©, neotÅ™elÃ© Å™eÅ¡enÃ­.</p>

<p>NicmÃ©nÄ› soudÃ­m, Å¾e neexistuje jedno univerzÃ¡lnÃ­ Å™eÅ¡enÃ­. Budoucnost pravdÄ›podobnÄ› spoÄÃ­vÃ¡ v kombinaci rÅ¯znÃ½ch pÅ™Ã­stupÅ¯, pÅ™izpÅ¯sobenÃ½ch konkrÃ©tnÃ­m ÃºlohÃ¡m a hardwarovÃ½m moÅ¾nostem. VÃ½voj v tÃ©to oblasti je extrÃ©mnÄ› dynamickÃ½ a lze oÄekÃ¡vat dalÅ¡Ã­ rychlÃ© pokroky.</p>

  </div>

  <div class="ai-rubric-link">
    
    <!-- PÅ™idÃ¡nÃ­ tabulky s nÃ¡hodnÄ› vybranÃ½mi obrazy pouze pro ÄlÃ¡nky z kolekce Obrazy -->
    
  </div>

  <div class="posts">
    <h3>Jak se vÃ¡m lÃ­bÃ­ tento ÄlÃ¡nek?</h3>
    <!-- MÃ­sto pro widget -->
    <div id="feedback-widget"></div>

    <!-- Widget se vloÅ¾Ã­ do #feedback-widget -->
    <script 
        src="https://top.marigold.cz/mg-feedback-clean.js" 
        data-slug="item-ai-kontext" 
        data-title="ProÄ je velikost kontextu u LLM tak dÅ¯leÅ¾itÃ¡?" 
        data-url="https://www.marigold.cz/item/ai-kontext/"
        data-target="#feedback-widget"
    ></script>
</div>

  
  <div class="commentbox"></div>
  <script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script>
  <script>commentBox('5677112761516032-proj')</script>
  

  <!-- Tady zaÄÃ­nÃ¡ odkazovÃ¡nÃ­ na featured ÄlÃ¡nky -->
  
  
    
    <div class="featured-posts">
      <h3>ğŸ’¡ Co je tu dalÅ¡Ã­ho zajÃ­mavÃ©ho ke ÄtenÃ­?</h3>
      <table>
        <tbody>
          
            <tr>
              <td>
                <a href="/item/projektovy-manazer-je-v-cesku-sproste-slovo-ke-skode-projektu/">ğŸ‘‰ProjektovÃ½ manaÅ¾er je v ÄŒesku sprostÃ© slovo â€“ ke Å¡kodÄ› projektÅ¯ â€¦</a>
                <p class="excerpt">
                  
                    Na Makers Faire jsem byl v panelu o novÃ½ch projektech. NechtÄ›lo se mi mluvit reklamnÄ› o Turrisu v rÃ¡mci panelu novÃ½ch projektÅ¯. Å˜ekl jsem si, Å¾e uÅ¾iteÄnÄ›jÅ¡Ã­ ...
                  
                </p>
              </td>
            </tr>
          
            <tr>
              <td>
                <a href="/item/proc-je-bydleni-drahe-protoze-nezlevnilo-ze-ano/">ğŸ‘‰ProÄ je bydlenÃ­ drahÃ©? ProtoÅ¾e nezlevnilo, Å¾e anoâ€¦</a>
                <p class="excerpt">
                  
                    HodnÄ› se v poslednÃ­ dobÄ› mluvÃ­ o drahÃ½ch bytech a vÃ½stavbÄ›. Studiem na vysokÃ© Å¡kole Å¾ivota zjistÃ­te, Å¾e by staÄilo je zlevnit a bude po problÃ©mu. Ale pojÄme ...
                  
                </p>
              </td>
            </tr>
          
        </tbody>
      </table>
    </div>
    



  <div class="posts">
    <h3>Chcete tyto ÄlÃ¡nky emailem?</h3>
    <iframe src="https://zandl.substack.com/embed" width="480" height="150" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no"></iframe>
  </div>

  <div>
    <p><span class="share-box">SdÃ­lejte ÄlÃ¡nek:</span> <a href="http://twitter.com/share?text=ProÄ je velikost kontextu u LLM tak dÅ¯leÅ¾itÃ¡?&url=https://www.marigold.cz/item/ai-kontext/" target="_blank">Twitter</a>, <a href="https://www.facebook.com/sharer.php?u=https://www.marigold.cz/item/ai-kontext/" target="_blank">Facebook</a>, 

    
      <a href="https://github.com/tangero/marigold-page/blob/main/_posts/2025/2025-05-05-ai-kontext.md" target="_blank">
        Opravit ğŸ“ƒ
      </a>
    
</p>
    <p>
    <div class="PageNavigation">
      
        <a class="prev" href="/item/urceni-polohy-fotky-chatgpt-o3/">&laquo; Jak urÄit polohu poÅ™Ã­zenÃ­ fotografie pomocÃ­ ChatGPT o3?</a> |
      
      
      
        <a class="next" href="/item/nehoda-ai-agenta/">PrvnÃ­ ÄeskÃ¡ tragÃ©die prostÅ™ednictvÃ­m autonomnÃ­ho AI agenta &raquo;</a>
      
    </div>
    </p>
  </div>
</article>

<!-- Toast notifikace -->
<div class="toast" id="toast">ZkopÃ­rovÃ¡no do schrÃ¡nky!</div>

<!-- JavaScript pro funkcionalitu kopÃ­rovÃ¡nÃ­ -->
<script>
document.addEventListener('DOMContentLoaded', function() {
    // Najdi vÅ¡echny code bloky (kromÄ› Mermaid)
    const codeBlocks = document.querySelectorAll('pre:not(:has(code.language-mermaid))');
    
    codeBlocks.forEach((codeBlock, index) => {
        // VytvoÅ™ kontejner pro code block
        const container = document.createElement('div');
        container.className = 'code-block-container';
        
        // VytvoÅ™ tlaÄÃ­tko kopÃ­rovat
        const copyButton = document.createElement('button');
        copyButton.className = 'copy-button';
        copyButton.textContent = 'KopÃ­rovat';
        copyButton.setAttribute('data-index', index);
        
        // VloÅ¾ code block a tlaÄÃ­tko do kontejneru
        codeBlock.parentNode.insertBefore(container, codeBlock);
        container.appendChild(copyButton);
        container.appendChild(codeBlock);
        
        // PÅ™idej event listener na tlaÄÃ­tko
        copyButton.addEventListener('click', async () => {
            try {
                // ZÃ­skej text z code bloku
                const code = codeBlock.querySelector('code') || codeBlock;
                const text = code.textContent;
                
                // KopÃ­ruj do schrÃ¡nky
                await navigator.clipboard.writeText(text);
                
                // ZmÄ›Åˆ stav tlaÄÃ­tka
                copyButton.textContent = 'ZkopÃ­rovÃ¡no!';
                copyButton.classList.add('copied');
                
                // Zobraz toast notifikaci
                showToast();
                
                // Po 2 sekundÃ¡ch vraÅ¥ pÅ¯vodnÃ­ stav
                setTimeout(() => {
                    copyButton.textContent = 'KopÃ­rovat';
                    copyButton.classList.remove('copied');
                }, 2000);
                
            } catch (err) {
                console.error('Chyba pÅ™i kopÃ­rovÃ¡nÃ­:', err);
                copyButton.textContent = 'Chyba';
                copyButton.classList.add('copied');
                
                setTimeout(() => {
                    copyButton.textContent = 'KopÃ­rovat';
                    copyButton.classList.remove('copied');
                }, 2000);
            }
        });
    });
});

function showToast() {
    const toast = document.getElementById('toast');
    toast.style.display = 'block';
    setTimeout(() => {
        toast.style.display = 'none';
    }, 2000);
}
</script>
        </div>
      </div>

      <div class="wrapper-footer">
        <div class="container">
          <footer class="footer">
            
<a href="mailto:patrick.zandl@marigold.cz"><i class="svg-icon email"></i></a>
<a href="https://www.facebook.com/patrick.zandl"><i class="svg-icon facebook"></i></a>



<a href="https://www.linkedin.com/in/patrickzandl"><i class="svg-icon linkedin"></i></a>

<a href="//feed.xml"><i class="svg-icon rss"></i></a>
<a href="https://www.twitter.com/tangero"><i class="svg-icon twitter"></i></a>





          </footer>
        </div>
      </div>
    </div>

    <a title="Web Analytics" href="https://clicky.com/101451859"><img alt="Clicky" src="//static.getclicky.com/media/links/badge.gif" border="0" /></a>
<script async data-id="101451859" src="//static.getclicky.com/js"></script>
<noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101451859ns.gif" /></p>
</noscript> | <a href="https://github.com/tangero/marigold-page"><img src="https://img.shields.io/github/last-commit/tangero/marigold-page"></a> | <a href="https://www.kronium.eu">flashlights, headlamps Fenix & outdoor</a> | <a href="https://www.vybavenidoprirody.com/">VybavenÃ­ do pÅ™Ã­rody</a>
<!-- 100% privacy-first analytics -->
<script async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>


    <script>
    function toggleDetails(button) {
      const content = button.nextElementSibling;
      const isCollapsed = button.classList.contains('collapsed');
      
      if (isCollapsed) {
        button.classList.remove('collapsed');
        content.classList.add('show');
      } else {
        button.classList.add('collapsed');
        content.classList.remove('show');
      }
    }
    </script>
  </body>
</html>
