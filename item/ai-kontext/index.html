<!DOCTYPE html>
<html>
  <head>
    <title>Proč je velikost kontextu u LLM tak důležitá? | Marigold.cz - Sítě a Technologie</title>
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Proč je velikost kontextu u LLM tak důležitá?" />
<meta name="author" content="Patrick Zandl" />
<meta property="og:locale" content="cs" />
<meta name="description" content="A především, proč je tak drahé a zdlouhavé zvyšovat velikost kontextu? Tento článek se podrobně zabývá tím, co kontext znamená, proč je jeho délka kritická, jaké technické překážky brání jeho neomezenému rozšiřování a jaká řešení se v současnosti vyvíjejí." />
<meta property="og:description" content="A především, proč je tak drahé a zdlouhavé zvyšovat velikost kontextu? Tento článek se podrobně zabývá tím, co kontext znamená, proč je jeho délka kritická, jaké technické překážky brání jeho neomezenému rozšiřování a jaká řešení se v současnosti vyvíjejí." />
<link rel="canonical" href="https://www.marigold.cz/item/ai-kontext/" />
<meta property="og:url" content="https://www.marigold.cz/item/ai-kontext/" />
<meta property="og:site_name" content="Marigold.cz" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-05-05T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Proč je velikost kontextu u LLM tak důležitá?" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Patrick Zandl"},"dateModified":"2025-05-05T00:00:00+00:00","datePublished":"2025-05-05T00:00:00+00:00","description":"A především, proč je tak drahé a zdlouhavé zvyšovat velikost kontextu? Tento článek se podrobně zabývá tím, co kontext znamená, proč je jeho délka kritická, jaké technické překážky brání jeho neomezenému rozšiřování a jaká řešení se v současnosti vyvíjejí.","headline":"Proč je velikost kontextu u LLM tak důležitá?","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.marigold.cz/item/ai-kontext/"},"url":"https://www.marigold.cz/item/ai-kontext/"}</script>
<!-- End Jekyll SEO tag -->

        <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>
    <link href="https://fed.brid.gy/" rel="alternate" type="application/activity+json">

    
    <meta property="og:description" content="A především, proč je tak drahé a zdlouhavé zvyšovat velikost kontextu? Tento článek se podrobně zabývá tím, co kontext znamená, proč je jeho délka kritická, jaké technické překážky brání jeho neomezenému rozšiřování a jaká řešení se v současnosti vyvíjejí.
" />
    
    <meta name="author" content="Marigold.cz" />

    
    <meta property="og:title" content="Proč je velikost kontextu u LLM tak důležitá?" />
    <meta property="twitter:title" content="Proč je velikost kontextu u LLM tak důležitá?" />
    

    
    <!-- page.thumbnail -->
    
        <!-- page.thumbnail https://www.marigold.cz/assets/llm-kontext.jpg |  -->
    <meta property="og:image" content="https://res.cloudinary.com/dvwv5cne3/image/fetch/w_1200,h_630,c_fill,g_auto,f_auto,q_auto/https://www.marigold.cz/assets/llm-kontext.jpg"/>
    <meta property="twitter:image" content="https://res.cloudinary.com/dvwv5cne3/image/fetch/w_1024,h_512,c_fill,g_auto,f_auto,q_auto/https://www.marigold.cz/assets/llm-kontext.jpg"/>
    

    <meta property="og:site_name" content="Marigold.cz | Technologie a Společnost"/>

    


    <link rel="stylesheet" type="text/css" href="//assets/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Marigold.cz - Technologie a Svět" href="//feed.xml" />
    <link rel="canonical" href="https://www.marigold.cz/item/ai-kontext/" />

    <meta name="theme-color" content="#000000">

    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="manifest" href="/images/site.webmanifest">
    <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="/images/favicon.ico">
    <meta name="msapplication-TileColor" content="#2d89ef">
    <meta name="msapplication-config" content="/images/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
    <script type="text/javascript">
      window.heapReadyCb=window.heapReadyCb||[],window.heap=window.heap||[],heap.load=function(e,t){window.heap.envId=e,window.heap.clientConfig=t=t||{},window.heap.clientConfig.shouldFetchServerConfig=!1;var a=document.createElement("script");a.type="text/javascript",a.async=!0,a.src="https://cdn.us.heap-api.com/config/"+e+"/heap_config.js";var r=document.getElementsByTagName("script")[0];r.parentNode.insertBefore(a,r);var n=["init","startTracking","stopTracking","track","resetIdentity","identify","getSessionId","getUserId","getIdentity","addUserProperties","addEventProperties","removeEventProperty","clearEventProperties","addAccountProperties","addAdapter","addTransformer","addTransformerFn","onReady","addPageviewProperties","removePageviewProperty","clearPageviewProperties","trackPageview"],i=function(e){return function(){var t=Array.prototype.slice.call(arguments,0);window.heapReadyCb.push({name:e,fn:function(){heap[e]&&heap[e].apply(heap,t)}})}};for(var p=0;p<n.length;p++)heap[n[p]]=i(n[p])};
      heap.load("2219710997");
  </script>
  </head>

  <body>
    <div id="bar"></div>
    <div class="wrapper-container">
      <div class="wrapper-masthead">
        <div class="container">
          <header class="masthead clearfix">
            <a href="//" class="site-avatar"><img src="//images/patrick-avatar.jpg" alt="" /></a>

            <div class="site-info">
              <h1 class="site-name"><a href="//">Marigold.cz</a></h1>
              <p class="site-description">Technologie a Svět</p>

            </div>

            <nav>
              <a href="/search">🔍</a> | <a href="https://www.prolnuto.cz/">🧑‍💻 Kurzy AI</a> | <a href="/vibecoding">👨‍💻 Vibe Coding</a> | <a href="/mobilnisite">🗼 4G/5G</a> | <a href="/ai">🤖 AI</a> | <a href="/obrazy">🖼️ Obrazy</a>
            </nav>
          </header>
        </div>
      </div>

      <div class="wrapper-main">
        <div id="main" role="main" class="container">
          <!-- start Mermaid run code --> 
<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.0.2/+esm'
  mermaid.initialize({startOnLoad:true,theme:'neutral'})
  await mermaid.run({querySelector:'code.language-mermaid'})
</script>
<!-- end fMermaid run code --> 

<!-- start feedwind code --> 
<!-- start feedwind code --> <script type="text/javascript" src="https://feed.mikle.com/js/fw-loader.js" preloader-text="Nahr%C3%A1v%C3%A1m" data-fw-param="168257/"></script> <!-- end feedwind code -->


<style>
  code.language-mermaid {
    display: flex;
    justify-content: center;
  }
  pre:has(code.language-mermaid), code.language-mermaid {
    background-color: transparent;
  }
  .edgeLabel {
    font-size: 92%;
    opacity: .95;
    color: #111;
    padding: 0 3px;
  }
  .node rect {
    stroke: #214f78 !important;
  }
  .nodeLabel {
    color: #214f78 !important;
  }

  /* Odstranění rámečků kolem textu */
  .post.detailed {
    padding: 0;
    margin: 0;
    border: none;
    box-shadow: none;
  }

  .post.detailed .entry {
    padding: 0;
    margin: 0;
    border: none;
    box-shadow: none;
  }

  /* Odstranění okrajů kolem sekce Články a novinky */
  .posts {
    margin: 0 !important;
    padding: 0 !important;
    border: none !important;
    box-shadow: none !important;
    background: none !important;
  }

  .posts .post {
    margin: 0 !important;
    padding: 0 !important;
    border: none !important;
    box-shadow: none !important;
    background: none !important;
  }

  .posts .post .entry {
    margin: 0 !important;
    padding: 0 !important;
    border: none !important;
    box-shadow: none !important;
    background: none !important;
  }

  /* CSS pro tlačítko kopírovat */
  .code-block-container {
    position: relative;
    margin: 20px 0;
  }
  .copy-button {
    position: absolute;
    top: 10px;
    right: 10px;
    padding: 8px 15px;
    background-color: #4CAF50;
    color: white;
    border: none;
    border-radius: 5px;
    cursor: pointer;
    font-size: 14px;
    z-index: 1;
  }
  .copy-button:hover {
    background-color: #45a049;
  }
  .copy-button:active {
    background-color: #3e8e41;
  }
  .copy-button.copied {
    background-color: #666;
  }
  .code-block-container pre {
    position: relative;
    padding-top: 40px;
  }
  .toast {
    position: fixed;
    bottom: 20px;
    left: 50%;
    transform: translateX(-50%);
    background-color: #333;
    color: white;
    padding: 12px 24px;
    border-radius: 5px;
    display: none;
    z-index: 1000;
  }
</style>
<!-- end feedwind code -->

<article class="post detailed">
  <h1>Proč je velikost kontextu u LLM tak důležitá?</h1>

  
  <div class="posts">
    <blockquote>
      <p>💡 Firemní <a href="https://www.prolnuto.cz/">konzultace a workshopy o umělé inteligenci</a>. Jak se vaší firmy dotkne AI a jak se na to připravit?<br/>👉 Poradíme v <a href="https://www.prolnuto.cz/">Prolnuto.cz</a></p>
    </blockquote>
  </div>
  

  
  <!-- Zde se zobrazí obsah pro všechny ostatní kolekce než 'obrazy' -->
  <div>
    <p class="author_title">Patrick Zandl  ·

5.
květen
  
2025 
    
    </p>

    
    <div class="post-tags">
      
      
        <a href="//rubrika/#AI">AI</a>
        &nbsp;
      
        <a href="//rubrika/#kontext">kontext</a>
        
      
    </div>
   
  </div>

  
  
  <div class="thumbnail-strip">
    <img src="https://res.cloudinary.com/dvwv5cne3/image/fetch/w_1200,h_300,c_fill,g_auto,f_auto,q_auto/https://www.marigold.cz/assets/llm-kontext.jpg" alt="Proč je velikost kontextu u LLM tak důležitá?">
  </div>
  


 
  <div class="quick-summary">
    <div class="quick-summary-header">
      <svg class="summary-icon" viewBox="0 0 24 24" width="24" height="24">
        <path fill="currentColor" d="M14,17H7V15H14M17,13H7V11H17M17,9H7V7H17M19,3H5C3.89,3 3,3.89 3,5V19A2,2 0 0,0 5,21H19A2,2 0 0,0 21,19V5C21,3.89 20.1,3 19,3Z" />
      </svg>
      <span>Rychlé shrnutí článku</span>
    </div>
    <ul class="summary-points">
      
        <li>Kontext v LLM je paměť modelu pro zpracování dat.</li>
      
        <li>Délka kontextu ovlivňuje porozumění a kvalitu výstupu.</li>
      
        <li>Kvadratická složitost pozornosti omezuje délku kontextu.</li>
      
        <li>Výzkum hledá optimalizace a alternativní architektury pro delší kontext.</li>
      
    </ul>
  </div>

  

<style>
.quick-summary {
    background: linear-gradient(145deg, #ffffff, #f5f5f5);
    border-radius: 12px;
    padding: 1.5rem;
    margin: 2rem 0;
    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1),
                0 2px 4px -1px rgba(0, 0, 0, 0.06);
}

.quick-summary-header {
    display: flex;
    align-items: center;
    margin-bottom: 1rem;
    color: #2d3748;
    font-weight: 600;
    font-size: 1.1rem;
}

.summary-icon {
    margin-right: 0.5rem;
    color: #4a5568;
}

.summary-points {
    margin: 0;
    padding: 0;
    list-style: none;
}

.summary-points li {
    position: relative;
    padding-left: 1.5rem;
    margin-bottom: 0.5rem;
    color: #4a5568;
    line-height: 1.5;
}

.summary-points li::before {
    content: "•";
    position: absolute;
    left: 0;
    color: #667eea;
    font-weight: bold;
}

@media (prefers-color-scheme: dark) {
    .quick-summary {
        background: linear-gradient(145deg, #2d3748, #1a202c);
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.2);
    }
    
    .quick-summary-header {
        color: #e2e8f0;
    }
    
    .summary-icon,
    .summary-points li {
        color: #cbd5e0;
    }
    
    .summary-points li::before {
        color: #7f9cf5;
    }
}
</style>




  <div class="entry">
    <p>A především, proč je tak drahé a zdlouhavé zvyšovat velikost kontextu? Tento článek se podrobně zabývá tím, co kontext znamená, proč je jeho délka kritická, jaké technické překážky brání jeho neomezenému rozšiřování a jaká řešení se v současnosti vyvíjejí.</p>

<p><strong>Co konkrétně se v tomto článku dozvíte?</strong></p>
<ul id="markdown-toc">
  <li><a href="#co-je-kontext-a-proč-je-jeho-délka-klíčová" id="markdown-toc-co-je-kontext-a-proč-je-jeho-délka-klíčová">Co je kontext a proč je jeho délka klíčová?</a>    <ul>
      <li><a href="#význam-délky-kontextu-pro-kvalitu-výstupu" id="markdown-toc-význam-délky-kontextu-pro-kvalitu-výstupu">Význam délky kontextu pro kvalitu výstupu:</a></li>
      <li><a href="#aktuální-velikosti-kontextových-oken-a-ceny-květen-2025" id="markdown-toc-aktuální-velikosti-kontextových-oken-a-ceny-květen-2025">Aktuální velikosti kontextových oken a ceny (květen 2025)</a></li>
    </ul>
  </li>
  <li><a href="#jádro-problému-kvadratická-složitost-mechanismu-pozornosti" id="markdown-toc-jádro-problému-kvadratická-složitost-mechanismu-pozornosti">Jádro problému: Kvadratická složitost mechanismu pozornosti</a></li>
  <li><a href="#praktické-důsledky-kvadratické-složitosti" id="markdown-toc-praktické-důsledky-kvadratické-složitosti">Praktické důsledky kvadratické složitosti</a></li>
  <li><a href="#současné-přístupy-a-řešení" id="markdown-toc-současné-přístupy-a-řešení">Současné přístupy a řešení</a>    <ul>
      <li><a href="#1-optimalizace-standardní-pozornosti" id="markdown-toc-1-optimalizace-standardní-pozornosti">1. Optimalizace standardní pozornosti</a></li>
      <li><a href="#2-aproximace-pozornosti-řídká-pozornost---sparse-attention" id="markdown-toc-2-aproximace-pozornosti-řídká-pozornost---sparse-attention">2. Aproximace pozornosti (Řídká pozornost - Sparse Attention)</a></li>
      <li><a href="#3-alternativní-architektury-mimo-transformátory" id="markdown-toc-3-alternativní-architektury-mimo-transformátory">3. Alternativní architektury (mimo transformátory)</a></li>
      <li><a href="#4-retrieval-augmented-generation-rag" id="markdown-toc-4-retrieval-augmented-generation-rag">4. Retrieval-Augmented Generation (RAG)</a></li>
      <li><a href="#5-další-techniky" id="markdown-toc-5-další-techniky">5. Další techniky</a></li>
    </ul>
  </li>
  <li><a href="#výzvy-a-budoucí-směřování" id="markdown-toc-výzvy-a-budoucí-směřování">Výzvy a budoucí směřování</a></li>
  <li><a href="#závěr" id="markdown-toc-závěr">Závěr</a></li>
</ul>

<p>Velké jazykové modely (LLM) jako GPT-4, Claude 3 nebo Gemini 2.5 se staly výkonnými nástroji pro zpracování přirozeného jazyka. Jejich schopnost generovat text, překládat, odpovídat na otázky a psát kód je využívána v mnoha oblastech. Navzdory jejich pokročilým schopnostem však narážejí na významné omezení: efektivní zpracování velmi dlouhých sekvencí dat, známé jako “problém dlouhého kontextu”.</p>

<h2 id="co-je-kontext-a-proč-je-jeho-délka-klíčová">Co je kontext a proč je jeho délka klíčová?</h2>

<p>V případě LLM představuje kontext (context window) veškerá data, která má model k dispozici v daném okamžiku pro zpracování a generování odpovědi. Funguje jako operační paměť modelu. Pokud si LLM chce něco pamatovat v rámci rozhovoru, předává si to jako kontext, ačkoliv to třeba nevidíte. Pokud má LLM pracovat s vašimi předchozími zprávami v rámci chatu, prostě je přibalí do posílaných dat. Obsah kontextu typicky zahrnuje:</p>

<ol>
  <li>
    <p>Vstupní text (prompt): Zadání nebo otázka od uživatele.</p>
  </li>
  <li>
    <p>Historie konverzace: Předchozí výměny v rámci aktuální interakce. U některých systémů může zahrnovat i relevantní informace z minulých interakcí (např. pomocí explicitních paměťových mechanismů).</p>
  </li>
  <li>
    <p>Poskytnuté dokumenty: Externí texty, které má model analyzovat, shrnout nebo z nich čerpat informace (např. nahrané PDF, webové stránky).</p>
  </li>
  <li>
    <p>Interní instrukce: Systémové prompty definující chování modelu, jeho personu nebo specifické úkoly.</p>
  </li>
  <li>
    <p>Vygenerovaný text: Část textu, kterou model sám postupně generuje jako odpověď.</p>
  </li>
</ol>

<p>Délka kontextu, obvykle měřená v tokenech, definuje maximální množství informací, které model může současně zpracovat. Token je základní jednotka textu pro LLM, která může odpovídat slovu, části slova nebo interpunkčnímu znaménku (pro hlubší vysvětlení viz článek <a href="/ai/[tokeny](/ai/tokeny-versus-slova/)-versus-slova">Tokeny versus Slova</a>).</p>

<h3 id="význam-délky-kontextu-pro-kvalitu-výstupu">Význam délky kontextu pro kvalitu výstupu:</h3>

<ul>
  <li>
    <p>Porozumění souvislostem: Delší kontext umožňuje modelu lépe zachytit složité vztahy, závislosti a nuance v rozsáhlých textech.</p>
  </li>
  <li>
    <p>Konzistence: Schopnost udržet jednotný styl, téma a faktickou správnost napříč dlouhými konverzacemi nebo dokumenty.</p>
  </li>
  <li>
    <p>Přesnost a relevance: Přístup k většímu množství relevantních informací vede k přesnějším a lépe zacíleným odpovědím.</p>
  </li>
  <li>
    <p>Zpracování komplexních úloh: Úlohy jako detailní analýza rozsáhlých reportů, knih nebo kódových bází vyžadují schopnost pojmout velké množství dat najednou.</p>
  </li>
  <li>
    <p>Omezení “<a href="/ai/halucinace-ai/">halucinací</a>”: Poskytnutí dostatečného kontextu může snížit tendenci modelu vymýšlet si informace, které nejsou ve vstupních datech.</p>
  </li>
</ul>

<h3 id="aktuální-velikosti-kontextových-oken-a-ceny-květen-2025">Aktuální velikosti kontextových oken a ceny (květen 2025)</h3>

<p>Velikost kontextového okna a cena jsou klíčové parametry při výběru modelu. Níže je uveden přehled některých populárních modelů s daty převážně z OpenRouter (duben 2025):</p>

<table>
  <thead>
    <tr>
      <th><strong>Model</strong></th>
      <th><strong>Kontextové okno (Max vstup)</strong></th>
      <th><strong>Max. výstup</strong></th>
      <th><strong>Cena vstupu ($/1M <a href="/ai/tokeny-versus-slova/">tokenů</a>)</strong></th>
      <th><strong>Cena výstupu ($/1M tokenů)</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>o3 (OpenAI)</td>
      <td>200 000</td>
      <td>100 000</td>
      <td>$10.00</td>
      <td>$40.00</td>
    </tr>
    <tr>
      <td>o4-Mini (OpenAI)</td>
      <td>200 000</td>
      <td>100 000</td>
      <td>$1.10</td>
      <td>$4.40</td>
    </tr>
    <tr>
      <td>o4-Mini High (OpenAI)</td>
      <td>200 000</td>
      <td>100 000</td>
      <td>$1.10</td>
      <td>$4.40</td>
    </tr>
    <tr>
      <td>GPT-4.1 (OpenAI)</td>
      <td>1 050 000</td>
      <td>33 000</td>
      <td>$2.00</td>
      <td>$8.00</td>
    </tr>
    <tr>
      <td>Claude 3.7 Sonnet</td>
      <td>200 000</td>
      <td>64 000</td>
      <td>$3.00</td>
      <td>$15.00</td>
    </tr>
    <tr>
      <td>Claude 3.7 Sonnet Think</td>
      <td>200 000</td>
      <td>64 000</td>
      <td>$3.00</td>
      <td>$15.00</td>
    </tr>
    <tr>
      <td>Gemini 2.5 Pro (Google)</td>
      <td>1 050 000</td>
      <td>66 000</td>
      <td>$1.25 - $2.50</td>
      <td>$10.00 - $15.00</td>
    </tr>
    <tr>
      <td>Grok 3 beta (xAI)</td>
      <td>131 000</td>
      <td>131 000</td>
      <td>$3.00</td>
      <td>$15.00</td>
    </tr>
    <tr>
      <td>Llama 4</td>
      <td>10 milionů</td>
      <td>-</td>
      <td>(Open Source)</td>
      <td>(Open Source)</td>
    </tr>
    <tr>
      <td>Jamba-1.5 (AI21, OS)</td>
      <td>256 000</td>
      <td>-</td>
      <td>(Open Source)</td>
      <td>(Open Source)</td>
    </tr>
  </tbody>
</table>

<p>Poznámka: Ceny se mohou lišit v závislosti na poskytovateli API (zde OpenRouter) a aktuálním vytížení. U Gemini 2.5 Pro jsou ceny uvedeny v rozsahu. Open-source modely nemají přímé ceny za <a href="/ai/tokeny-versus-slova/">token</a>, ale náklady na jejich provoz. Hodnota u LLAMA 4 je velmi optimistická, model na to nebyl řádně testován a výsledky nejsou příliš kvalitní.</p>

<p>Je důležité poznamenat, že nominální délka kontextového okna nemusí vždy odpovídat efektivní schopnosti modelu využívat informace z celého kontextu. Testy jako <a href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack">Needle In A Haystack</a> (NIAH) ukazují, že některé modely mají problémy s vyhledáváním informací umístěných uprostřed velmi dlouhého kontextu (tzv. “lost in the middle” problém), i když se tento problém postupně daří zmírňovat.</p>

<p>Už teď je tedy zřejmé, že na rozsahu kontextu záleží, přičemž “kontext” není jen to, co zadáte do Prompt okna v ChatGPT, ale také spousta dodatečných dat, kterými ChatGPT váš dotaz “obalí”, aby využil toho, co ví o vás, o tom, co vyžadujete atd. Nabízí se tedy otázka, proč se jednoduše velikost kontextového okna nerozšíří na maximum! Odpověď? Protože to není vůbec jednoduché a především to stojí hromadu peněz při používání! Jak to?</p>

<h2 id="jádro-problému-kvadratická-složitost-mechanismu-pozornosti">Jádro problému: Kvadratická složitost mechanismu pozornosti</h2>

<p>Základem většiny moderních LLM je architektura transformátoru, představená v roce 2017 v článku “Attention Is All You Need”. Klíčovou inovací této architektury je mechanismus sebe-pozornosti (self-attention). Ten umožňuje modelu vážit důležitost všech ostatních tokenů v kontextu při zpracování každého jednotlivého tokenu.</p>

<p>Jak to funguje (velmi zjednodušeně): model se při čtení každého slova “dívá” na všechna ostatní slova v textu, aby pochopil jeho význam v dané větě. Tedy počítá jej vůči všem předchozím slovům. Tímto způsobem zjišťuje, která slova jsou pro aktuální slovo nejdůležitější a jak spolu souvisí. Proto prodlužování textu zvyšuje náročnost výpočtů exponenciálně.</p>

<p>Jak funguje (méně zjednodušeně): Pro každý token model vypočítá tři vektory: Query (Q), Key (K) a Value (V). Poté pro každý token (reprezentovaný jeho Q vektorem) vypočítá skóre pozornosti vůči všem ostatním tokenům (porovnáním Q s K vektory všech tokenů). Tato skóre se normalizují (typicky pomocí funkce softmax) a použijí se k vytvoření váženého součtu V vektorů všech tokenů. Výsledkem je nová reprezentace tokenu, která zohledňuje jeho vztah ke všem ostatním tokenům v kontextu.</p>

<p>Problém škálování: Tento mechanismus je extrémně efektivní pro zachycení závislostí v textu, ale má zásadní nevýhodu: jeho výpočetní a paměťová složitost roste kvadraticky s délkou sekvence (N, počet tokenů).</p>

<ul>
  <li>
    <p>Výpočetní složitost: Počet operací potřebných pro výpočet matice pozornosti je úměrný O(N2). Pro každý z N tokenů musíme vypočítat jeho vztah k N tokenům (včetně sebe sama).</p>
  </li>
  <li>
    <p>Paměťová složitost: Model si musí během výpočtu uchovávat matici pozornosti o velikosti N×N, což vede k paměťové náročnosti O(N2).</p>
  </li>
</ul>

<p>Ilustrace dopadu:</p>

<p>Přesné časy zpracování závisí na mnoha faktorech (konkrétní model, hardware - např. typ GPU, optimalizace - např. FlashAttention, datový typ výpočtů), ale pro ilustraci řádového nárůstu náročnosti na výkonném GPU (např. NVIDIA H100/B100):</p>

<ul>
  <li>
    <p>Kontext 1 000 tokenů: Vyžaduje řádově 10002=1000000 operací/paměťových jednotek. Zpracování (inference) může trvat zlomky sekundy až jednotky sekund.</p>
  </li>
  <li>
    <p>Kontext 10 000 tokenů: Vyžaduje řádově 100002=100000000 operací/paměťových jednotek (100x více). Doba zpracování se může pohybovat v jednotkách až desítkách sekund.</p>
  </li>
  <li>
    <p>Kontext 100 000 tokenů: Vyžaduje řádově 1000002=10000000000 operací/paměťových jednotek (10 000x více než pro 1k tokenů). Doba zpracování může dosahovat desítek sekund až několika minut.</p>
  </li>
  <li>
    <p>Kontext 1 000 000 tokenů (jako u Gemini Pro, GPT-4.1): Vyžaduje řádově 10000002=1000000000000 (bilion) operací/paměťových jednotek. Doba zpracování se může pohybovat v řádu několika minut až desítek minut, silně závisí na optimalizacích a počtu použitých akcelerátorů.</p>
  </li>
</ul>

<p>Tento kvadratický nárůst představuje obrovskou bariéru pro neomezené prodlužování kontextového okna u standardních <a href="/ai/transformatory/">transformátorů</a>, jak z hlediska výpočetní náročnosti (čas), tak paměťových požadavků.</p>

<h2 id="praktické-důsledky-kvadratické-složitosti">Praktické důsledky kvadratické složitosti</h2>

<p>Kvadratická složitost mechanismu pozornosti má několik zásadních praktických dopadů. Především vede k enormní výpočetní náročnosti a latenci při zpracování dlouhých kontextů. Vyžaduje to obrovské množství výpočetních zdrojů, jako jsou GPU nebo TPU, což se projevuje delší dobou odezvy při generování odpovědí, vysokou spotřebou energie a následně i vysokými náklady na trénink a inferenci modelů kvůli potřebě výkonného a drahého hardwaru. Proto jsou modely, které mají velké množství parametrů a umožňují zpracovat velký kontext, také zpravidla výrazně dražší.</p>

<p>Dalším významným důsledkem jsou vysoké paměťové nároky, zejména na VRAM akcelerátorů. Model musí uchovávat matice pozornosti a mezivýpočty (aktivace) pro všechny tokeny v kontextu. Například optimalizace zvaná KV cache, která ukládá vypočtené vektory pro zrychlení inference, vyžaduje pro model Llama 3 70B s kontextem 128 000 tokenů stovky gigabajtů VRAM. Pro kontexty v řádu milionů tokenů tyto nároky dále dramaticky rostou, což omezuje nasazení takových modelů pouze na hardware s masivní paměťovou kapacitou.</p>

<p>Tyto zvýšené výpočetní a paměťové nároky se promítají do ekonomických dopadů. Poskytovatelé LLM služeb musí tyto náklady zohlednit, a proto zpravidla účtují vyšší ceny za použití modelů s delšími kontextovými okny nebo za zpracování tokenů přesahujících určitou hranici, jak je vidět v přehledové tabulce cen.</p>

<p>Nakonec, i když model technicky zvládne zpracovat velmi dlouhý kontext, objevuje se problém známý jako “Lost in the Middle”. Empirické testy ukazují, že schopnost modelu efektivně využívat informace může klesat, pokud jsou tyto informace umístěny uprostřed velmi dlouhého vstupního textu. Modely často vykazují tendenci lépe pracovat s informacemi uvedenými na začátku nebo na konci kontextového okna.</p>

<h2 id="současné-přístupy-a-řešení">Současné přístupy a řešení</h2>

<p>Výzkum a vývoj se intenzivně zaměřují na zmírnění nebo překonání O(N2) bariéry, protože překročení limitů přinášených kontextem by umožňovalo výrazně rozšířit úlohy, v nichž AI / LLM excelují. A také dosáhnout lepší ekonomiky. Hlavní směry výzkumu jsou zhruba následující:</p>

<h3 id="1-optimalizace-standardní-pozornosti">1. Optimalizace standardní pozornosti</h3>

<ul>
  <li>
    <p>FlashAttention (a jeho následovníci FlashAttention-2, FlashAttention-3): Algoritmus, který restrukturalizuje výpočet pozornosti tak, aby lépe využíval hierarchii paměti GPU. Minimalizuje pomalé přesuny dat mezi HBM (High Bandwidth Memory) a SRAM (on-chip paměť) pomocí technik jako tiling a recomputation. Výrazně zrychluje výpočet a snižuje paměťové nároky bez změny matematiky pozornosti, takže výsledky jsou (téměř) identické se standardní pozorností. Stal se de facto standardem pro trénink a inferenci moderních LLM.</p>
  </li>
  <li>
    <p>KV Cache (Key-Value Cache): Optimalizace pro inferenci (generování textu). Místo přepočítávání K a V vektorů pro všechny předchozí tokeny při generování každého nového tokenu se tyto vektory ukládají do paměti (cache). To snižuje výpočetní náročnost generování z O(N2) na O(N) pro každý nový token, ale paměťová náročnost pro uložení cache zůstává O(N).</p>
  </li>
</ul>

<h3 id="2-aproximace-pozornosti-řídká-pozornost---sparse-attention">2. Aproximace pozornosti (Řídká pozornost - Sparse Attention)</h3>

<p>Cílem tohoto přístupu je snížit počet párů tokenů, mezi kterými se počítá pozornost, a tím prolomit kvadratickou složitost výpočtu plné matice pozornosti. Místo aby každý token interagoval se všemi ostatními, interakce se omezí na “řídký” vzor, který se snaží zachovat nejdůležitější informace. Longformer například kombinuje lokální pozornost, kde každý token interaguje pouze se svými nejbližšími sousedy v rámci “klouzavého okna”, s globální pozorností pro několik předem určených tokenů (např. speciální tokeny jako [CLS]). Tyto globální tokeny mohou interagovat se všemi ostatními tokeny a všechny ostatní tokeny mohou interagovat s nimi, což umožňuje přenos informací napříč celou sekvencí při zachování převážně lokálních výpočtů. Podobně BigBird používá kombinaci tří typů řídké pozornosti: náhodnou pozornost (každý token interaguje s malým náhodným vzorkem ostatních tokenů), okénkovou pozornost (podobně jako Longformer) a globální pozornost. Tato kombinace má teoretické základy a snaží se efektivně aproximovat vlastnosti plné matice pozornosti. Jiné metody, jako Routing Transformer nebo Sinkhorn Transformer, jdou ještě dál a snaží se dynamicky “naučit” nebo optimalizovat, které páry tokenů jsou nejdůležitější pro výpočet pozornosti, například pomocí technik směrování informací nebo metod inspirovaných optimálním transportem (Sinkhorn), čímž se výpočty soustředí pouze na nejrelevantnější části matice pozornosti.</p>

<p>Ačkoliv tyto metody mohou dosáhnout lineární (O(N)) nebo téměř lineární (O(NlogN)) výpočetní složitosti, kompromisem může být mírné snížení kvality modelu oproti plné pozornosti. Důvodem je, že předdefinované nebo aproximované vzory řídké pozornosti nemusí vždy dokonale zachytit všechny relevantní dlouhodobé závislosti v textu, které by plná pozornost identifikovala.</p>

<h3 id="3-alternativní-architektury-mimo-transformátory">3. Alternativní architektury (mimo <a href="/ai/transformatory/">transformátory</a>)</h3>

<p>Hledání architektur, které nejsou založeny na standardní O(N2) pozornosti:</p>

<ul>
  <li>
    <p>Rekurentní <a href="/ai/neuronove-site/">[neuronové sítě](/ai/neuronove-site/)</a> (RNN) / LSTM / GRU: Tyto sítě představují starší přístup ke zpracování sekvencí, jehož kořeny sahají až do 80. a 90. let 20. století. Základní myšlenka RNN spočívá ve zpracování sekvence krok za krokem (token po tokenu), přičemž si síť udržuje vnitřní “stav” nebo “paměť”, která shrnuje informace z předchozích kroků. Tento stav se aktualizuje při zpracování každého nového tokenu. Díky tomu má zpracování inherentně lineární výpočetní složitost (O(N)), protože výpočet pro každý token závisí pouze na aktuálním vstupu a předchozím stavu, nikoli na všech předchozích tokenech současně. Varianty jako LSTM (Long Short-Term Memory, Hochreiter &amp; Schmidhuber, 1997) a GRU (Gated Recurrent Unit) byly vyvinuty později, aby řešily klíčový problém základních RNN: tzv. mizení nebo explozi gradientů (vanishing/exploding gradients), které bránily učení závislostí na dlouhé vzdálenosti v sekvenci. Přestože LSTM a GRU tento problém zmírnily pomocí speciálních “bran” (gates), které řídí tok informací a gradientů, stále měly své limity. Hlavní nevýhodou oproti transformátorům se ukázala být jejich sekvenční povaha, která znesnadňuje paralelizaci výpočtů během tréninku na moderním hardwaru (GPU/TPU). Transformátory, které mohou zpracovávat všechny tokeny v sekvenci víceméně paralelně díky mechanismu pozornosti, se tak staly efektivnější pro trénink na velkých datech a dosáhly lepších výsledků v mnoha úlohách. Moderní výzkum se však k RNN a jejich vylepšením částečně vrací, snaží se kombinovat jejich výhody (lineární složitost) s novými technikami pro zlepšení výkonu a paralelizace.</p>
  </li>
  <li>
    <p>State Space Models (SSM): Třída modelů inspirovaná teorií řízení.</p>
  </li>
  <li>
    <p>Mamba: Populární SSM architektura, která dosahuje lineární složitosti škálování s délkou sekvence a zároveň si zachovává schopnost modelovat dlouhé závislosti díky selektivnímu mechanismu stavu. Ukazuje slibné výsledky, zejména v úlohách vyžadujících dlouhý kontext. Existují i novější varianty a vylepšení (Mamba-2, etc.).</p>
  </li>
  <li>
    <p>Hybridní modely: Kombinují různé přístupy.</p>
  </li>
  <li>
    <p>Jamba (AI21 Labs): Architektura, která střídá vrstvy standardní pozornosti (Transformer bloky) s Mamba bloky. Cílem je zkombinovat sílu pozornosti pro lokální a komplexní vztahy s efektivitou Mamby pro dlouhé sekvence. Výsledkem je model, který zvládá dlouhý kontext (256k tokenů) s výrazně nižšími paměťovými nároky než čistý transformátor podobné velikosti. Očekávají se nástupci.</p>
  </li>
</ul>

<h3 id="4-retrieval-augmented-generation-rag">4. Retrieval-Augmented Generation (RAG)</h3>

<p>Alternativní přístup, který se nesnaží vtěsnat veškeré informace do kontextového okna modelu. Místo toho postupuje zhruba následovně:</p>

<ol>
  <li>
    <p>Rozsáhlá databáze znalostí (např. dokumenty, webové stránky) je indexována a uložena ve vektorové databázi.</p>
  </li>
  <li>
    <p>Když přijde dotaz uživatele, systém nejprve vyhledá nejrelevantnější části informací z databáze (retrieval).</p>
  </li>
  <li>
    <p>Tyto relevantní části (snippets) jsou pak spolu s původním dotazem vloženy do kontextového okna LLM.</p>
  </li>
  <li>
    <p>LLM použije tyto poskytnuté informace k vygenerování odpovědi.</p>
  </li>
</ol>

<p>Výhody RAG: Může pracovat s prakticky neomezeným množstvím externích dat bez nutnosti extrémně dlouhého kontextového okna. Je snadnější aktualizovat znalosti (stačí aktualizovat databázi).</p>

<p>Nevýhody RAG: Kvalita závisí na úspěšnosti vyhledávacího kroku. Model nemá “holistický” pohled na celý dokument, jen na vybrané části. Nemusí být vhodný pro úlohy vyžadující syntézu informací napříč celým rozsáhlým textem.</p>

<h3 id="5-další-techniky">5. Další techniky</h3>

<ul>
  <li>
    <p>Context Compression: Metody, které se snaží zkrátit prompt nebo odstranit méně relevantní části kontextu před jeho předáním modelu.</p>
  </li>
  <li>
    <p>Ring Attention: Technika pro distribuovaný trénink/inferenci, která umožňuje rozdělit zpracování dlouhého kontextu mezi více akcelerátorů (GPU) tak, že každý zpracovává část sekvence, ale mohou si efektivně vyměňovat informace potřebné pro výpočet pozornosti napříč celou sekvencí.</p>
  </li>
</ul>

<h2 id="výzvy-a-budoucí-směřování">Výzvy a budoucí směřování</h2>

<p>Navzdory pokrokům zůstává efektivní a kvalitní zpracování dlouhého kontextu klíčovou výzvou. Budoucí vývoj se pravděpodobně zaměří na několik oblastí. Bude pokračovat zlepšování efektivity prostřednictvím dalších optimalizací algoritmů jako FlashAttention, vývoje nových aproximací pozornosti a zdokonalování alternativních architektur typu SSM a hybridních modelů. Současně bude kladen důraz na zlepšování kvality, zejména na řešení problému “lost in the middle” a zajištění spolehlivého využití informací z celého kontextu, což podpoří i vývoj lepších evaluačních metrik. Očekává se také hardwarová ko-evoluce s vývojem specializovaných akcelerátorů s větší pamětí a propustností, optimalizovaných pro LLM. Dále se bude prohlubovat kombinace přístupů, například hledání synergií mezi modely s dlouhým kontextem a technikami RAG pro lepší syntézu informací. V neposlední řadě bude pokračovat hledání fundamentálních průlomů a zcela nových paradigmat pro zpracování sekvenčních dat, která by mohla překonat současná omezení.</p>

<h2 id="závěr">Závěr</h2>

<p>Schopnost pracovat s dlouhým kontextem je zásadní pro posun LLM směrem k hlubšímu porozumění a řešení komplexnějších úloh. Kvadratická složitost standardního mechanismu pozornosti v architektuře transformátoru představuje dosti podstatnou překážku, která vede k vysokým výpočetním, paměťovým a finančním nákladům. Současný výzkum přináší řadu inovativních řešení, od optimalizací stávajících metod (FlashAttention) přes aproximace (Sparse Attention) až po zcela nové architektury (Mamba, Jamba) a doplňkové techniky (RAG). V každém případě je tu ještě mnoho příležitostí, jak můžete prosadit svůj nápad a nabídnout nové, neotřelé řešení.</p>

<p>Nicméně soudím, že neexistuje jedno univerzální řešení. Budoucnost pravděpodobně spočívá v kombinaci různých přístupů, přizpůsobených konkrétním úlohám a hardwarovým možnostem. Vývoj v této oblasti je extrémně dynamický a lze očekávat další rychlé pokroky.</p>

  </div>

  <div class="ai-rubric-link">
    
    <!-- Přidání tabulky s náhodně vybranými obrazy pouze pro články z kolekce Obrazy -->
    
  </div>

  <div class="posts">
    <h3>Jak se vám líbí tento článek?</h3>
    <!-- Místo pro widget -->
    <div id="feedback-widget"></div>

    <!-- Widget se vloží do #feedback-widget -->
    <script 
        src="https://top.marigold.cz/mg-feedback-clean.js" 
        data-slug="item-ai-kontext" 
        data-title="Proč je velikost kontextu u LLM tak důležitá?" 
        data-url="https://www.marigold.cz/item/ai-kontext/"
        data-target="#feedback-widget"
    ></script>
</div>

  
  <div class="commentbox"></div>
  <script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script>
  <script>commentBox('5677112761516032-proj')</script>
  

  <!-- Tady začíná odkazování na featured články -->
  
  
    
    <div class="featured-posts">
      <h3>💡 Co je tu dalšího zajímavého ke čtení?</h3>
      <table>
        <tbody>
          
            <tr>
              <td>
                <a href="/item/projektovy-manazer-je-v-cesku-sproste-slovo-ke-skode-projektu/">👉Projektový manažer je v Česku sprosté slovo – ke škodě projektů …</a>
                <p class="excerpt">
                  
                    Na Makers Faire jsem byl v panelu o nových projektech. Nechtělo se mi mluvit reklamně o Turrisu v rámci panelu nových projektů. Řekl jsem si, že užitečnější ...
                  
                </p>
              </td>
            </tr>
          
            <tr>
              <td>
                <a href="/item/proc-je-bydleni-drahe-protoze-nezlevnilo-ze-ano/">👉Proč je bydlení drahé? Protože nezlevnilo, že ano…</a>
                <p class="excerpt">
                  
                    Hodně se v poslední době mluví o drahých bytech a výstavbě. Studiem na vysoké škole života zjistíte, že by stačilo je zlevnit a bude po problému. Ale pojďme ...
                  
                </p>
              </td>
            </tr>
          
        </tbody>
      </table>
    </div>
    



  <div class="posts">
    <h3>Chcete tyto články emailem?</h3>
    <iframe src="https://zandl.substack.com/embed" width="480" height="150" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no"></iframe>
  </div>

  <div>
    <p><span class="share-box">Sdílejte článek:</span> <a href="http://twitter.com/share?text=Proč je velikost kontextu u LLM tak důležitá?&url=https://www.marigold.cz/item/ai-kontext/" target="_blank">Twitter</a>, <a href="https://www.facebook.com/sharer.php?u=https://www.marigold.cz/item/ai-kontext/" target="_blank">Facebook</a>, 

    
      <a href="https://github.com/tangero/marigold-page/blob/main/_posts/2025/2025-05-05-ai-kontext.md" target="_blank">
        Opravit 📃
      </a>
    
</p>
    <p>
    <div class="PageNavigation">
      
        <a class="prev" href="/item/urceni-polohy-fotky-chatgpt-o3/">&laquo; Jak určit polohu pořízení fotografie pomocí ChatGPT o3?</a> |
      
      
      
        <a class="next" href="/item/nehoda-ai-agenta/">První česká tragédie prostřednictvím autonomního AI agenta &raquo;</a>
      
    </div>
    </p>
  </div>
</article>

<!-- Toast notifikace -->
<div class="toast" id="toast">Zkopírováno do schránky!</div>

<!-- JavaScript pro funkcionalitu kopírování -->
<script>
document.addEventListener('DOMContentLoaded', function() {
    // Najdi všechny code bloky (kromě Mermaid)
    const codeBlocks = document.querySelectorAll('pre:not(:has(code.language-mermaid))');
    
    codeBlocks.forEach((codeBlock, index) => {
        // Vytvoř kontejner pro code block
        const container = document.createElement('div');
        container.className = 'code-block-container';
        
        // Vytvoř tlačítko kopírovat
        const copyButton = document.createElement('button');
        copyButton.className = 'copy-button';
        copyButton.textContent = 'Kopírovat';
        copyButton.setAttribute('data-index', index);
        
        // Vlož code block a tlačítko do kontejneru
        codeBlock.parentNode.insertBefore(container, codeBlock);
        container.appendChild(copyButton);
        container.appendChild(codeBlock);
        
        // Přidej event listener na tlačítko
        copyButton.addEventListener('click', async () => {
            try {
                // Získej text z code bloku
                const code = codeBlock.querySelector('code') || codeBlock;
                const text = code.textContent;
                
                // Kopíruj do schránky
                await navigator.clipboard.writeText(text);
                
                // Změň stav tlačítka
                copyButton.textContent = 'Zkopírováno!';
                copyButton.classList.add('copied');
                
                // Zobraz toast notifikaci
                showToast();
                
                // Po 2 sekundách vrať původní stav
                setTimeout(() => {
                    copyButton.textContent = 'Kopírovat';
                    copyButton.classList.remove('copied');
                }, 2000);
                
            } catch (err) {
                console.error('Chyba při kopírování:', err);
                copyButton.textContent = 'Chyba';
                copyButton.classList.add('copied');
                
                setTimeout(() => {
                    copyButton.textContent = 'Kopírovat';
                    copyButton.classList.remove('copied');
                }, 2000);
            }
        });
    });
});

function showToast() {
    const toast = document.getElementById('toast');
    toast.style.display = 'block';
    setTimeout(() => {
        toast.style.display = 'none';
    }, 2000);
}
</script>
        </div>
      </div>

      <div class="wrapper-footer">
        <div class="container">
          <footer class="footer">
            
<a href="mailto:patrick.zandl@marigold.cz"><i class="svg-icon email"></i></a>
<a href="https://www.facebook.com/patrick.zandl"><i class="svg-icon facebook"></i></a>



<a href="https://www.linkedin.com/in/patrickzandl"><i class="svg-icon linkedin"></i></a>

<a href="//feed.xml"><i class="svg-icon rss"></i></a>
<a href="https://www.twitter.com/tangero"><i class="svg-icon twitter"></i></a>





          </footer>
        </div>
      </div>
    </div>

    <a title="Web Analytics" href="https://clicky.com/101451859"><img alt="Clicky" src="//static.getclicky.com/media/links/badge.gif" border="0" /></a>
<script async data-id="101451859" src="//static.getclicky.com/js"></script>
<noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101451859ns.gif" /></p>
</noscript> | <a href="https://github.com/tangero/marigold-page"><img src="https://img.shields.io/github/last-commit/tangero/marigold-page"></a> | <a href="https://www.kronium.eu">flashlights, headlamps Fenix & outdoor</a> | <a href="https://www.vybavenidoprirody.com/">Vybavení do přírody</a>
<!-- 100% privacy-first analytics -->
<script async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>


    <script>
    function toggleDetails(button) {
      const content = button.nextElementSibling;
      const isCollapsed = button.classList.contains('collapsed');
      
      if (isCollapsed) {
        button.classList.remove('collapsed');
        content.classList.add('show');
      } else {
        button.classList.add('collapsed');
        content.classList.remove('show');
      }
    }
    </script>
  </body>
</html>
