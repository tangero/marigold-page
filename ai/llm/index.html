<!DOCTYPE html>
<html>
  <head>
    <title>Jak fungujÃ­ velkÃ© jazykovÃ© modely LLM - co se dÄ›je po zadÃ¡nÃ­ promptu | Marigold.cz - SÃ­tÄ› a Technologie</title>
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Jak fungujÃ­ velkÃ© jazykovÃ© modely LLM - co se dÄ›je po zadÃ¡nÃ­ promptu" />
<meta name="author" content="Patrick Zandl" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="FungovÃ¡nÃ­ velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) po zadÃ¡nÃ­ promptu je proces, kterÃ½ kombinuje algoritmy, masivnÃ­ neuronovÃ© sÃ­tÄ› a komplexnÃ­ zpracovÃ¡nÃ­ dat. Abyste mÄ›li lepÅ¡Ã­ pÅ™edstavu, jak modernÃ­ AI na bÃ¡zi LLM funguje, je dobrÃ© podÃ­vat se krok za krokem, jak tento proces probÃ­hÃ¡ od okamÅ¾iku, kdy uÅ¾ivatel zadÃ¡ prompt, aÅ¾ po generovÃ¡nÃ­ finÃ¡lnÃ­ odpovÄ›di." />
<meta property="og:description" content="FungovÃ¡nÃ­ velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) po zadÃ¡nÃ­ promptu je proces, kterÃ½ kombinuje algoritmy, masivnÃ­ neuronovÃ© sÃ­tÄ› a komplexnÃ­ zpracovÃ¡nÃ­ dat. Abyste mÄ›li lepÅ¡Ã­ pÅ™edstavu, jak modernÃ­ AI na bÃ¡zi LLM funguje, je dobrÃ© podÃ­vat se krok za krokem, jak tento proces probÃ­hÃ¡ od okamÅ¾iku, kdy uÅ¾ivatel zadÃ¡ prompt, aÅ¾ po generovÃ¡nÃ­ finÃ¡lnÃ­ odpovÄ›di." />
<link rel="canonical" href="https://www.marigold.cz/ai/llm/" />
<meta property="og:url" content="https://www.marigold.cz/ai/llm/" />
<meta property="og:site_name" content="Marigold.cz" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-07-05T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Jak fungujÃ­ velkÃ© jazykovÃ© modely LLM - co se dÄ›je po zadÃ¡nÃ­ promptu" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Patrick Zandl"},"dateModified":"2024-07-05T00:00:00+00:00","datePublished":"2024-07-05T00:00:00+00:00","description":"FungovÃ¡nÃ­ velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) po zadÃ¡nÃ­ promptu je proces, kterÃ½ kombinuje algoritmy, masivnÃ­ neuronovÃ© sÃ­tÄ› a komplexnÃ­ zpracovÃ¡nÃ­ dat. Abyste mÄ›li lepÅ¡Ã­ pÅ™edstavu, jak modernÃ­ AI na bÃ¡zi LLM funguje, je dobrÃ© podÃ­vat se krok za krokem, jak tento proces probÃ­hÃ¡ od okamÅ¾iku, kdy uÅ¾ivatel zadÃ¡ prompt, aÅ¾ po generovÃ¡nÃ­ finÃ¡lnÃ­ odpovÄ›di.","headline":"Jak fungujÃ­ velkÃ© jazykovÃ© modely LLM - co se dÄ›je po zadÃ¡nÃ­ promptu","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.marigold.cz/ai/llm/"},"url":"https://www.marigold.cz/ai/llm/"}</script>
<!-- End Jekyll SEO tag -->

        <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>
    <link href="https://fed.brid.gy/" rel="alternate" type="application/activity+json">

    
    <meta property="og:description" content="FungovÃ¡nÃ­ velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) po zadÃ¡nÃ­ promptu je proces, kterÃ½ kombinuje algoritmy, masivnÃ­ neuronovÃ© sÃ­tÄ› a komplexnÃ­ zpracovÃ¡nÃ­ dat. Abyste mÄ›li lepÅ¡Ã­ pÅ™edstavu, jak modernÃ­ AI na bÃ¡zi LLM funguje, je dobrÃ© podÃ­vat se krok za krokem, jak tento proces probÃ­hÃ¡ od okamÅ¾iku, kdy uÅ¾ivatel zadÃ¡ prompt, aÅ¾ po generovÃ¡nÃ­ finÃ¡lnÃ­ odpovÄ›di.
" />
    
    <meta name="author" content="Marigold.cz" />

    
    <meta property="og:title" content="Jak fungujÃ­ velkÃ© jazykovÃ© modely LLM - co se dÄ›je po zadÃ¡nÃ­ promptu" />
    <meta property="twitter:title" content="Jak fungujÃ­ velkÃ© jazykovÃ© modely LLM - co se dÄ›je po zadÃ¡nÃ­ promptu" />
    

    
    <!-- else -->    
    <meta property="og:image" content="https://www.marigold.cz/images/patrick-avatar.jpg"/>
    <meta property="twitter:image" content="https://www.marigold.cz/images/patrick-avatar.jpg"/>
    

    <meta property="og:site_name" content="Marigold.cz | Technologie a SpoleÄnost"/>

    


    <link rel="stylesheet" type="text/css" href="/assets/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Marigold.cz - Technologie a SvÄ›t" href="/feed.xml" />
    <link rel="canonical" href="https://www.marigold.cz/ai/llm/" />

    <meta name="theme-color" content="#000000">

    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="manifest" href="/images/site.webmanifest">
    <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="/images/favicon.ico">
    <meta name="msapplication-TileColor" content="#2d89ef">
    <meta name="msapplication-config" content="/images/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
    <script type="text/javascript">
      window.heapReadyCb=window.heapReadyCb||[],window.heap=window.heap||[],heap.load=function(e,t){window.heap.envId=e,window.heap.clientConfig=t=t||{},window.heap.clientConfig.shouldFetchServerConfig=!1;var a=document.createElement("script");a.type="text/javascript",a.async=!0,a.src="https://cdn.us.heap-api.com/config/"+e+"/heap_config.js";var r=document.getElementsByTagName("script")[0];r.parentNode.insertBefore(a,r);var n=["init","startTracking","stopTracking","track","resetIdentity","identify","getSessionId","getUserId","getIdentity","addUserProperties","addEventProperties","removeEventProperty","clearEventProperties","addAccountProperties","addAdapter","addTransformer","addTransformerFn","onReady","addPageviewProperties","removePageviewProperty","clearPageviewProperties","trackPageview"],i=function(e){return function(){var t=Array.prototype.slice.call(arguments,0);window.heapReadyCb.push({name:e,fn:function(){heap[e]&&heap[e].apply(heap,t)}})}};for(var p=0;p<n.length;p++)heap[n[p]]=i(n[p])};
      heap.load("2219710997");
  </script>
  </head>

  <body>
    <div id="bar"></div>
    <div class="wrapper-container">
      <div class="wrapper-masthead">
        <div class="container">
          <header class="masthead clearfix">
            <a href="/" class="site-avatar"><img src="/images/patrick-avatar.jpg" alt="" /></a>

            <div class="site-info">
              <h1 class="site-name"><a href="/">Marigold.cz</a></h1>
              <p class="site-description">Technologie a SvÄ›t</p>

            </div>

            <nav>
              <a href="/search">ğŸ”</a> | <a href="https://www.prolnuto.cz/">ğŸ§‘â€ğŸ’» Kurzy AI</a> | <a href="https://www.vibecoding.cz/">ğŸ–¥ï¸ Vibecoding</a> | <a href="/mobilnisite">ğŸ—¼ 4G/5G</a> | <a href="/ai">ğŸ¤– AI</a> | <a href="/obrazy">ğŸ–¼ï¸ Obrazy</a>
            </nav>
          </header>
        </div>
      </div>

      <div class="wrapper-main">
        <div id="main" role="main" class="container">
          <!-- start Mermaid run code --> 
<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.0.2/+esm'
  mermaid.initialize({startOnLoad:true,theme:'neutral'})
  await mermaid.run({querySelector:'code.language-mermaid'})
</script>
<!-- end fMermaid run code --> 

<a href="https://www.arthousemelichar.cz/"><img src="https://www.marigold.cz/assets/arthousemelichar.jpg" alt="Art House Melichar BrandÃ½s nad Labem - popup galerie pro toto lÃ©to" style="width:100%;display:block;margin:0 auto;" /></a>

<!-- mÃ­sto pÅ¯vodnÃ­ho feedwind code -->


<style>
  code.language-mermaid {
    display: flex;
    justify-content: center;
  }
  pre:has(code.language-mermaid), code.language-mermaid {
    background-color: transparent;
  }
  .edgeLabel {
    font-size: 92%;
    opacity: .95;
    color: #111;
    padding: 0 3px;
  }
  .node rect {
    stroke: #214f78 !important;
  }
  .nodeLabel {
    color: #214f78 !important;
  }

  /* OdstranÄ›nÃ­ rÃ¡meÄkÅ¯ kolem textu */
  .post.detailed {
    padding: 0;
    margin: 0;
    border: none;
    box-shadow: none;
  }

  .post.detailed .entry {
    padding: 0;
    margin: 0;
    border: none;
    box-shadow: none;
  }

  /* OdstranÄ›nÃ­ okrajÅ¯ kolem sekce ÄŒlÃ¡nky a novinky */
  .posts {
    margin: 0 !important;
    padding: 0 !important;
    border: none !important;
    box-shadow: none !important;
    background: none !important;
  }

  .posts .post {
    margin: 0 !important;
    padding: 0 !important;
    border: none !important;
    box-shadow: none !important;
    background: none !important;
  }

  .posts .post .entry {
    margin: 0 !important;
    padding: 0 !important;
    border: none !important;
    box-shadow: none !important;
    background: none !important;
  }

  /* CSS pro tlaÄÃ­tko kopÃ­rovat */
  .code-block-container {
    position: relative;
    margin: 20px 0;
  }
  .copy-button {
    position: absolute;
    top: 10px;
    right: 10px;
    padding: 8px 15px;
    background-color: #4CAF50;
    color: white;
    border: none;
    border-radius: 5px;
    cursor: pointer;
    font-size: 14px;
    z-index: 1;
  }
  .copy-button:hover {
    background-color: #45a049;
  }
  .copy-button:active {
    background-color: #3e8e41;
  }
  .copy-button.copied {
    background-color: #666;
  }
  .code-block-container pre {
    position: relative;
    padding-top: 40px;
  }
  .toast {
    position: fixed;
    bottom: 20px;
    left: 50%;
    transform: translateX(-50%);
    background-color: #333;
    color: white;
    padding: 12px 24px;
    border-radius: 5px;
    display: none;
    z-index: 1000;
  }
</style>
<!-- end feedwind code -->

<article class="post detailed">
  <h1>Jak fungujÃ­ velkÃ© jazykovÃ© modely LLM - co se dÄ›je po zadÃ¡nÃ­ promptu</h1>

  

  
  <!-- Zde se zobrazÃ­ obsah pro vÅ¡echny ostatnÃ­ kolekce neÅ¾ 'obrazy' -->
  <div>
    <p class="author_title">Patrick Zandl  Â·

5.
Äervenec
  
2024 
    
    </p>

    
    <div class="post-tags">
      
      
    </div>
   
  </div>

  


 



  <div class="entry">
    <p>FungovÃ¡nÃ­ velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM) po zadÃ¡nÃ­ promptu je proces, kterÃ½ kombinuje algoritmy, masivnÃ­ <a href="/ai/neuronove-site/">neuronovÃ© sÃ­tÄ›</a> a komplexnÃ­ zpracovÃ¡nÃ­ dat. Abyste mÄ›li lepÅ¡Ã­ pÅ™edstavu, jak modernÃ­ AI na bÃ¡zi LLM funguje, je dobrÃ© podÃ­vat se krok za krokem, jak tento proces probÃ­hÃ¡ od okamÅ¾iku, kdy uÅ¾ivatel zadÃ¡ prompt, aÅ¾ po generovÃ¡nÃ­ finÃ¡lnÃ­ odpovÄ›di.</p>

<h3 id="1-zadÃ¡nÃ­-promptu">1. ZadÃ¡nÃ­ promptu</h3>
<p>VÅ¡e zaÄÃ­nÃ¡, kdyÅ¾ uÅ¾ivatel <a href="/ai/prompt/">zadÃ¡ textovÃ½ prompt</a> do rozhranÃ­ LLM. Tento prompt mÅ¯Å¾e bÃ½t otÃ¡zka, instrukce, nebo jakÃ½koli jinÃ½ textovÃ½ vstup. NapÅ™Ã­klad: â€œVysvÄ›tli mi, jak funguje fotosyntÃ©za u rostlin.â€</p>

<h3 id="2-tokenizace">2. Tokenizace</h3>
<p>Jakmile je prompt zadÃ¡n, prvnÃ­m krokem je tokenizace. Tokenizace rozdÄ›luje text na menÅ¡Ã­ jednotky zvanÃ© <a href="/ai/tokeny-versus-slova/">tokeny</a>. Tyto <a href="/ai/tokeny-versus-slova/">tokeny</a> mohou bÃ½t slova, ÄÃ¡sti slov, nebo dokonce jednotlivÃ© znaky, v zÃ¡vislosti na pouÅ¾itÃ©m tokenizaÄnÃ­m algoritmu. Ve <a href="/ai/tokenizace-textu/">zvlÃ¡Å¡tnÃ­m ÄlÃ¡nku vysvÄ›tluji</a>, jakÃ½ je rozdÃ­l mezi slovy a <a href="/ai/tokeny-versus-slova/">tokeny</a> a proÄ se mechanismus <a href="/ai/tokeny-versus-slova/">tokenÅ¯</a> pouÅ¾Ã­vÃ¡. 
Pro nÃ¡Å¡ pÅ™Ã­klad by tokenizace mohla vypadat takto:
[â€œVysvÄ›tliâ€, â€œmiâ€, â€œ,â€, â€œjakâ€, â€œfungujeâ€, â€œfotoâ€, â€œsyntÃ©zaâ€, â€œuâ€, â€œrostlinâ€, â€œ.â€]</p>

<p>ModernÃ­ LLM Äasto pouÅ¾Ã­vajÃ­ metody tokenizace jako <em>Byte-Pair Encoding (BPE)</em> nebo <em>SentencePiece</em>, kterÃ© umoÅ¾ÅˆujÃ­ efektivnÄ›jÅ¡Ã­ zpracovÃ¡nÃ­ mÃ©nÄ› ÄastÃ½ch slov a rÅ¯znÃ½ch jazykÅ¯. VÅ¡imnÄ›te si, Å¾e slovo â€œfotosyntÃ©zaâ€ bylo rozdÄ›leno na dva <a href="/ai/tokeny-versus-slova/">tokeny</a>.</p>

<h3 id="3-numerickÃ¡-reprezentace">3. NumerickÃ¡ reprezentace</h3>
<p>Po tokenizaci je kaÅ¾dÃ½ <a href="/ai/tokeny-versus-slova/">token</a> pÅ™eveden na numerickou reprezentaci pomocÃ­ velkÃ©ho vyhledÃ¡vacÃ­ho slovnÃ­ku, kde kaÅ¾dÃ½ <a href="/ai/tokeny-versus-slova/">token</a> mÃ¡ pÅ™iÅ™azenÃ© unikÃ¡tnÃ­ ÄÃ­slo.</p>

<h3 id="4-kontextovÃ©-okno">4. KontextovÃ© okno</h3>
<p>LLM majÃ­ omezenÃ½ poÄet <a href="/ai/tokeny-versus-slova/">tokenÅ¯</a>, kterÃ© mohou zpracovat najednou, tzv. <em>kontextovÃ© okno</em>. Toto okno mÅ¯Å¾e bÃ½t napÅ™Ã­klad 2048 nebo 4096 tokenÅ¯, u nejmodernÄ›jÅ¡Ã­ch LLM i statisÃ­ce tokenÅ¯. VÅ¡e v tomto oknÄ› slouÅ¾Ã­ jako kontext pro generovÃ¡nÃ­ odpovÄ›di, tedy informace, kterÃ¡ mÅ¯Å¾e bÃ½t relevantnÃ­ a pouÅ¾ita k upÅ™esnÄ›nÃ­ generovanÃ© odpovÄ›di.</p>

<h3 id="5-zpracovÃ¡nÃ­-neuronovou-sÃ­tÃ­">5. ZpracovÃ¡nÃ­ neuronovou sÃ­tÃ­</h3>
<p>NynÃ­ pÅ™ichÃ¡zÃ­ na Å™adu jÃ¡dro LLM - <em>masivnÃ­ <a href="/ai/neuronove-site/">neuronovÃ¡ sÃ­Å¥</a></em>, obvykle zaloÅ¾enÃ¡ na architektuÅ™e <em><a href="/ai/transformatory/">TransformÃ¡torÅ¯</a></em>. Tato sÃ­Å¥ se sklÃ¡dÃ¡ z mnoha vrstev a mÅ¯Å¾e mÃ­t miliardy parametrÅ¯.</p>

<p><strong>a) Token embeddings vrstva</strong>
PrvnÃ­ vrstva po samotnÃ© tokenizaci pÅ™evÃ¡dÃ­ numerickÃ© reprezentace tokenÅ¯ na vektory s plovoucÃ­ desetinnou ÄÃ¡rkou, kterÃ© zachycujÃ­ sÃ©mantickÃ© vlastnosti tokenÅ¯.</p>

<p>Po tokenizaci je kaÅ¾dÃ©mu tokenu pÅ™iÅ™azeno jeho ID ze slovnÃ­ku (napÅ™. token â€œkoÄkaâ€ mÃ¡ ID 587). Tato ID jsou pak pÅ™evedena na vektory s plovoucÃ­ desetinnou ÄÃ¡rkou - tzv. embeddings. Je to v podstatÄ› vyhledÃ¡vacÃ­ tabulka (lookup table), kde kaÅ¾dÃ©mu ID je pÅ™iÅ™azen vektor ÄÃ­sel (napÅ™Ã­klad vektor o dÃ©lce 768 nebo 1024 dimenzÃ­). Tyto vektory jsou nastaveny bÄ›hem trÃ©novÃ¡nÃ­ tak, aby tokeny s podobnÃ½m vÃ½znamem mÄ›ly podobnÃ© vektorovÃ© reprezentace - zachycujÃ­ tedy sÃ©mantickÃ© vlastnosti tokenÅ¯.
PÅ™Ã­klad:</p>

<ul>
  <li>Slovo â€œkoÄkaâ€ (ID 587) â†’ [0.2, -0.5, 0.1, â€¦, 0.3]</li>
  <li>Slovo â€œkocourâ€ (ID 892) â†’ [0.19, -0.48, 0.15, â€¦, 0.28]
(podobnÃ½ vektor, protoÅ¾e vÃ½znamy jsou podobnÃ©)</li>
  <li>Slovo â€œautoâ€ (ID 245) â†’ [-0.8, 0.2, -0.4, â€¦, -0.1]
(velmi odliÅ¡nÃ½ vektor, protoÅ¾e vÃ½znam je odliÅ¡nÃ½)</li>
</ul>

<p><strong>b) PozornostnÃ­ mechanismus (Self-attention)</strong>
KlÃ­Äovou souÄÃ¡stÃ­ <a href="/ai/transformatory">architektury TransformÃ¡torÅ¯</a> je mechanismus pozornosti (attention mechanism). Mechanismus pozornosti umoÅ¾Åˆuje rozhodnout se, jakÃ¡ ÄÃ¡st z jinak â€œplochÃ©hoâ€ textovÃ©ho vstupu je vÃ­ce Äi mÃ©nÄ› hodnÃ¡ pozornosti. Pro kaÅ¾dÃ½ token ve vstupnÃ­ sekvenci tento mechanismus vypoÄÃ­tÃ¡vÃ¡:</p>

<ol>
  <li>Query (Q) - dotaz: co token â€œhledÃ¡â€</li>
  <li>Key (K) - klÃ­Ä: ÄÃ­m token â€œodpovÃ­dÃ¡â€ na dotazy</li>
  <li>Value (V) - hodnota: jakou informaci token â€œposkytujeâ€</li>
</ol>

<p>Funguje to nÃ¡sledovnÄ›:</p>
<ul>
  <li>Pro kaÅ¾dÃ½ token se jeho Query porovnÃ¡ s Keys vÅ¡ech ostatnÃ­ch tokenÅ¯ (vÄetnÄ› sebe sama)</li>
  <li>Toto porovnÃ¡nÃ­ vytvoÅ™Ã­ â€œattention scoresâ€ - ÄÃ­sla urÄujÃ­cÃ­, jak moc by mÄ›l danÃ½ token vÄ›novat pozornost ostatnÃ­m tokenÅ¯m</li>
  <li>Tyto skÃ³re se normalizujÃ­ pomocÃ­ softmax funkce na pravdÄ›podobnostnÃ­ distribuci</li>
  <li>Nakonec se vypoÄtenÃ© attention scores pouÅ¾ijÃ­ jako vÃ¡hy pro Values pÅ™Ã­sluÅ¡nÃ½ch tokenÅ¯</li>
</ul>

<p>PÅ™Ã­klad:
VÄ›ta: <em>â€œKoÄka, kterÃ¡ honÃ­ myÅ¡, je ÄernÃ¡.â€</em></p>
<ul>
  <li>KdyÅ¾ model zpracovÃ¡vÃ¡ slovo â€œjeâ€, mechanismus pozornosti mu umoÅ¾nÃ­ zamÄ›Å™it se silnÄ› na slovo â€œkoÄkaâ€, protoÅ¾e to je podmÄ›t vÄ›ty</li>
  <li>PÅ™i zpracovÃ¡nÃ­ slova â€œÄernÃ¡â€ se model mÅ¯Å¾e znovu zamÄ›Å™it na â€œkoÄkaâ€, protoÅ¾e to je pÅ™edmÄ›t, kterÃ½ je popisovÃ¡n. 
TÃ­m poÄÃ­taÄ z jinak plochÃ©ho sdÄ›lenÃ­ mÅ¯Å¾e pochopit, Å¾e to, co honÃ­ myÅ¡, je koÄka a tato koÄka je ÄernÃ¡.</li>
</ul>

<p>ModernÃ­ LLM typicky pouÅ¾Ã­vajÃ­:</p>
<ul>
  <li>Multi-head attention: nÄ›kolik pozornostnÃ­ch mechanismÅ¯ bÄ›Å¾Ã­cÃ­ch paralelnÄ›</li>
  <li>Masked attention v pÅ™Ã­padÄ› generativnÃ­ch modelÅ¯: pÅ™i generovÃ¡nÃ­ mÅ¯Å¾e model vÄ›novat pozornost pouze pÅ™edchozÃ­m tokenÅ¯m, ne budoucÃ­m</li>
</ul>

<p>Tento mechanismus je klÃ­ÄovÃ½ pro schopnost LLM porozumÄ›t kontextu a dlouhodobÃ½m zÃ¡vislostem v textu.</p>

<p><strong>c) Feed-forward vrstvy (FFN)</strong>
Po tom, co attention mechanismus umoÅ¾nil obohatil tokeny o potÅ™ebnÃ½ kontext, pÅ™ichÃ¡zÃ­ feed-forward sÃ­Å¥ (FFN). Ta pÅ™edstavuje moÅ¾nost, jak pro kaÅ¾dÃ½ token zpracovat a tedy â€œpromysletâ€ vÅ¡echny informace, kterÃ© nynÃ­ dostÃ¡vÃ¡.</p>

<p>PÅ™edstavte si to jako tÅ™Ã­fÃ¡zovÃ½ proces:</p>

<ol>
  <li>RozÅ¡Ã­Å™enÃ­ informacÃ­
    <ul>
      <li>Token nejprve â€œrozbalÃ­â€ vÅ¡echny svoje informace do vÄ›tÅ¡Ã­ho prostoru</li>
      <li>Je to jako kdyÅ¾ si rozloÅ¾Ã­te puzzle na vÄ›tÅ¡Ã­ stÅ¯l a obrÃ¡tÃ­te obrÃ¡zkem nahoru, abyste lÃ©pe vidÄ›li vÅ¡echny dÃ­lky</li>
    </ul>
  </li>
  <li>ZpracovÃ¡nÃ­ informacÃ­
    <ul>
      <li>Aplikuje se aktivaÄnÃ­ funkce (GELU/ReLU), kterÃ¡ nÄ›kterÃ© informace zvÃ½raznÃ­ a jinÃ© potlaÄÃ­</li>
      <li>Je to jako kdyÅ¾ z rozloÅ¾enÃ½ch dÃ­lkÅ¯ puzzle vyberete ty, kterÃ© jsou pro vÃ¡s v danou chvÃ­li dÅ¯leÅ¾itÃ© - tÅ™eba ty okrajovÃ©</li>
    </ul>
  </li>
  <li>ShrnutÃ­
    <ul>
      <li>Nakonec se vÅ¡echny zpracovanÃ© informace opÄ›t â€œsbalÃ­â€ do pÅ¯vodnÃ­ velikosti</li>
      <li>Jako kdyÅ¾ posklÃ¡dÃ¡te vybranÃ© dÃ­lky puzzle zpÄ›t do kompaktnÃ­ho celku</li>
    </ul>
  </li>
</ol>

<p>DÅ¯leÅ¾itÃ© je, Å¾e kaÅ¾dÃ½ token tÃ­mto procesem prochÃ¡zÃ­ samostatnÄ› - je to jeho â€œosobnÃ­ Äas na pÅ™emÃ½Å¡lenÃ­â€ o vÅ¡em, co se dozvÄ›dÄ›l z attention mechanismu.</p>

<p>Proto je tento proces velmi nÃ¡roÄnÃ½ na vÃ½poÄetnÃ­ sÃ­lu a Äas a dÄ›lÃ¡ to problematickÃ½m zpracovÃ¡nÃ­ rozsÃ¡hlÃ½ch kontextÅ¯, tedy rozsÃ¡hlÃ½ch vstupÅ¯, napÅ™Ã­klad rozsÃ¡hlÃ½ch textÅ¯. KaÅ¾dÃ½ jednotlivÃ½ token musÃ­ bÃ½t propoÄÃ­tÃ¡n s ohledem na vÅ¡echny ostatnÃ­ tokeny.</p>

<p>VÃ½sledek se pak pÅ™iÄte k pÅ¯vodnÃ­m informacÃ­m tokenu (jako kdyÅ¾ si k pÅ¯vodnÃ­m poznÃ¡mkÃ¡m pÅ™idÃ¡te novÃ© postÅ™ehy) a celÃ© se to â€œuÄeÅ¡eâ€ pomocÃ­ normalizace, aby dalÅ¡Ã­ vrstvy mohly efektivnÄ› pracovat s aktualizovanÃ½mi informacemi.</p>

<h3 id="6-generovÃ¡nÃ­-vÃ½stupu">6. GenerovÃ¡nÃ­ vÃ½stupu</h3>
<p>Po zpracovÃ¡nÃ­ vstupnÃ­ho promptu neuronovou sÃ­tÃ­ zaÄÃ­nÃ¡ proces generovÃ¡nÃ­ odpovÄ›di. Tento proces je iterativnÃ­ a probÃ­hÃ¡ token po tokenu.</p>

<p><strong>a) Predikce dalÅ¡Ã­ho tokenu</strong>
Model pouÅ¾Ã­vÃ¡ svÃ© vnitÅ™nÃ­ reprezentace a nauÄenÃ© vztahy k predikci pravdÄ›podobnostnÃ­ho rozdÄ›lenÃ­ nad vÅ¡emi moÅ¾nÃ½mi nÃ¡sledujÃ­cÃ­mi tokeny.  NapÅ™Ã­klad:</p>

<p>â€œPetr Å¡el do obchodu koupitâ€¦â€</p>

<p>V tomto momentÄ› model:</p>
<ol>
  <li>Vezme vÅ¡echna moÅ¾nÃ¡ slova ze svÃ©ho slovnÃ­ku (tÅ™eba 50 000 moÅ¾nostÃ­)</li>
  <li>KaÅ¾dÃ©mu slovu pÅ™iÅ™adÃ­ pravdÄ›podobnost, Å¾e by mÄ›lo bÃ½t dalÅ¡Ã­ v poÅ™adÃ­</li>
  <li>NapÅ™Ã­klad:
    <ul>
      <li>â€œchlebaâ€ - 35% pravdÄ›podobnost</li>
      <li>â€œmlÃ©koâ€ - 25% pravdÄ›podobnost</li>
      <li>â€œjablkaâ€ - 15% pravdÄ›podobnost</li>
      <li>â€œautoâ€ - 0.001% pravdÄ›podobnost</li>
      <li>(a tak dÃ¡le pro vÅ¡echna slova)</li>
    </ul>
  </li>
</ol>

<p>Model pÅ™iÅ™azuje vyÅ¡Å¡Ã­ pravdÄ›podobnosti slovÅ¯m, kterÃ¡ dÃ¡vajÃ­ v danÃ©m kontextu nejvÄ›tÅ¡Ã­ smysl. Slova, kterÃ¡ nedÃ¡vajÃ­ smysl (jako â€œautoâ€ v naÅ¡em pÅ™Ã­kladu nÃ¡kupu), dostanou velmi nÃ­zkou pravdÄ›podobnost.</p>

<p>Tento proces se opakuje pro kaÅ¾dÃ© dalÅ¡Ã­ slovo, pÅ™iÄemÅ¾ kaÅ¾dÃ© novÄ› pÅ™idanÃ© slovo ovlivÅˆuje pravdÄ›podobnosti slov nÃ¡sledujÃ­cÃ­ch.</p>

<p><strong>b) VÃ½bÄ›r tokenu</strong>
Z tohoto pravdÄ›podobnostnÃ­ho rozdÄ›lenÃ­ je vybrÃ¡n dalÅ¡Ã­ token. LLM obvykle pouÅ¾Ã­vajÃ­ techniku zvanou sampling, kdy vybÃ­rajÃ­ z nÄ›kolika nejpravdÄ›podobnÄ›jÅ¡Ã­ch moÅ¾nostÃ­. To do jistÃ© mÃ­ry pÅ™idÃ¡vÃ¡ elementy kreativity a variability do odpovÄ›dÃ­, tzn. odpovÄ›di na stejnÃ© prompty jsou rÅ¯znÃ©. Je vÃ½znamnou souÄÃ¡stÃ­ ladÄ›nÃ­ modelu nastavenÃ­ samplingu, tedy rozpÄ›tÃ­ pravdÄ›podobnostÃ­, z nichÅ¾ model vybÃ­rÃ¡. Tedy to, jestli v naÅ¡em pÅ™Ã­padÄ› mÅ¯Å¾e vybÃ­rat ze slov chleba, mlÃ©ko, ale i jeÅ¡tÄ› jablko, nebo uÅ¾ je jablko pÅ™Ã­liÅ¡ mÃ¡lo pravdÄ›podobnÃ©. Toto nastavenÃ­ vytvÃ¡Å™Ã­ rozpÄ›tÃ­ mezi nudnÃ½mi a sprÃ¡vnÃ½mi odpovÄ›Ämi a inspirativnÃ­mi nebo Å¡Ã­lenÃ½mi na druhÃ©m pÃ³luâ€¦</p>

<p><strong>c) ZpÄ›tnÃ¡ vazba</strong>
VybranÃ½ token je pÅ™idÃ¡n k dosud vygenerovanÃ©mu vÃ½stupu a takÃ© je pouÅ¾it jako vstup pro dalÅ¡Ã­ krok generovÃ¡nÃ­. Tento proces se opakuje, dokud nenÃ­ vygenerovÃ¡n speciÃ¡lnÃ­ token oznaÄujÃ­cÃ­ konec sekvence, nebo dokud nenÃ­ dosaÅ¾eno maximÃ¡lnÃ­ dÃ©lky vÃ½stupu.</p>

<h3 id="7-post-processing">7. Post-processing</h3>
<p>Po vygenerovÃ¡nÃ­ surovÃ©ho vÃ½stupu nÃ¡sleduje detokenizace (pÅ™evod tokenÅ¯ zpÄ›t na ÄitelnÃ½ text), pÅ™Ã­padnÃ¡ filtrace obsahu a formÃ¡tovÃ¡nÃ­.</p>

<h3 id="8-zobrazenÃ­-odpovÄ›di">8. ZobrazenÃ­ odpovÄ›di</h3>
<p>KoneÄnÄ›, zpracovanÃ¡ a formÃ¡tovanÃ¡ odpovÄ›Ä je zobrazena uÅ¾ivateli.</p>

<h2 id="klÃ­ÄovÃ©-koncepty-a-pokroÄilÃ©-techniky">KlÃ­ÄovÃ© koncepty a pokroÄilÃ© techniky</h2>

<p>NynÃ­, kdyÅ¾ jsme proÅ¡li zÃ¡kladnÃ­ proces, pojÄme se podÃ­vat na nÄ›kterÃ© klÃ­ÄovÃ© koncepty a pokroÄilÃ© techniky, kterÃ© jsou dÅ¯leÅ¾itÃ© pro pochopenÃ­ fungovÃ¡nÃ­ modernÃ­ch LLM.</p>

<h3 id="1-princip-predikce-nÃ¡sledujÃ­cÃ­ho-slova">1. Princip predikce nÃ¡sledujÃ­cÃ­ho slova</h3>

<p>Je dÅ¯leÅ¾itÃ© pochopit, Å¾e jÃ¡drem fungovÃ¡nÃ­ LLM je pÅ™edpovÃ­dÃ¡nÃ­ nÃ¡sledujÃ­cÃ­ho slova (nebo tokenu) na zÃ¡kladÄ› pÅ™edchozÃ­ho kontextu. Tento jednoduchÃ½ princip je zÃ¡kladem pro vÅ¡echny sofistikovanÃ© schopnosti, kterÃ© LLM vykazujÃ­.</p>

<h3 id="2-fÃ¡ze-trÃ©novÃ¡nÃ­">2. FÃ¡ze trÃ©novÃ¡nÃ­</h3>

<p>PÅ™ed samotnÃ½m pouÅ¾itÃ­m prochÃ¡zÃ­ LLM tÅ™emi hlavnÃ­mi fÃ¡zemi trÃ©novÃ¡nÃ­:</p>

<p>a) Pre-training: Model se uÄÃ­ pÅ™edpovÃ­dat dalÅ¡Ã­ slovo na obrovskÃ©m mnoÅ¾stvÃ­ textovÃ½ch dat, zÃ­skÃ¡vajÃ­c tak Å¡irokÃ© znalosti o jazyce a svÄ›tÄ›.</p>

<p>b) Instruction fine-tuning: Model se uÄÃ­ reagovat na konkrÃ©tnÃ­ instrukce a otÃ¡zky, coÅ¾ mu pomÃ¡hÃ¡ chovat se vÃ­ce jako asistent.</p>

<p>c) Reinforcement Learning from Human Feedback (RLHF): Tato fÃ¡ze dÃ¡le vylepÅ¡uje model tak, aby jeho odpovÄ›di byly vÃ­ce v souladu s lidskÃ½mi preferencemi a hodnotami.</p>

<h3 id="3-emergentnÃ­-schopnosti">3. EmergentnÃ­ schopnosti</h3>

<p>S rostoucÃ­ velikostÃ­ modelu a mnoÅ¾stvÃ­m trÃ©ninkovÃ½ch dat se u LLM objevujÃ­ emergentnÃ­ schopnosti - schopnosti, kterÃ© nebyly explicitnÄ› natrÃ©novÃ¡ny. Mezi tyto schopnosti patÅ™Ã­ napÅ™Ã­klad:</p>

<ul>
  <li>Å˜eÅ¡enÃ­ vÃ­ceÃºrovÅˆovÃ½ch Ãºloh</li>
  <li>Few-shot learning (schopnost uÄit se z nÄ›kolika pÅ™Ã­kladÅ¯)</li>
  <li>Å˜eÅ¡enÃ­ matematickÃ½ch problÃ©mÅ¯</li>
  <li>GenerovÃ¡nÃ­ kÃ³du</li>
</ul>

<h3 id="4-techniky-pro-zlepÅ¡enÃ­-vÃ½konu">4. Techniky pro zlepÅ¡enÃ­ vÃ½konu</h3>

<p>Existuje nÄ›kolik technik, kterÃ© mohou vÃ½raznÄ› zlepÅ¡it vÃ½kon LLM:</p>

<p>a) Chain-of-thought prompting: pÅ™Ã­stup, kdy model â€œmyslÃ­ krok za krokemâ€, coÅ¾ mÅ¯Å¾e zlepÅ¡it jeho schopnost Å™eÅ¡it komplexnÃ­ Ãºlohy. Tento pÅ™Ã­stup vlastnÄ› rozklÃ¡dÃ¡ Ãºlohu na jednoduÅ¡Å¡Ã­ Ãºlohy, pÅ™iÄemÅ¾ LLM (stejnÄ› jako ÄlovÄ›k) lÃ©pe zvlÃ¡dÃ¡ vÃ­ce zÅ™etÄ›zenÃ½ch jednoduÅ¡Å¡Ã­ch Ãºloh, neÅ¾ jednu sloÅ¾itou.  Model tak dostÃ¡vÃ¡ â€œpracovnÃ­ pamÄ›Å¥â€ v podobÄ› mezivÃ½sledkÅ¯, kterÃ© jsou souÄÃ¡stÃ­ kontextu pro dalÅ¡Ã­ generovÃ¡nÃ­.</p>

<p>b) Few-shot learning: Pokud je poskytnuto v promptu nÄ›kolik pÅ™Ã­kladÅ¯, mÅ¯Å¾e to pomoci modelu lÃ©pe pochopit poÅ¾adovanÃ½ formÃ¡t nebo styl odpovÄ›di. Proto takÃ© prompty obohacujeme Å¾Ã¡dostmi, v jakÃ©m formÃ¡tu Äi stylu a odbornostÃ­ poÅ¾adujeme vÃ½stupy, aby to LLM nemusel odhadovat.</p>

<p>c) Retrieval-Augmented Generation (RAG): Kombinace LLM s vyhledÃ¡vÃ¡nÃ­m v externÃ­ databÃ¡zi znalostÃ­ mÅ¯Å¾e zlepÅ¡it pÅ™esnost a aktuÃ¡lnost odpovÄ›dÃ­. Tato technika je zvlÃ¡Å¡tÄ› uÅ¾iteÄnÃ¡ pro pÅ™ekonÃ¡nÃ­ omezenÃ­ spojenÃ½ch s cut-off datem trÃ©ninkovÃ½ch dat.</p>

<h3 id="5-omezenÃ­-a-vÃ½zvy">5. OmezenÃ­ a vÃ½zvy</h3>

<p>I pÅ™es svÃ© schopnosti majÃ­ LLM nÄ›kolik vÃ½znamnÃ½ch omezenÃ­:</p>

<p>a) <a href="/ai/halucinace-ai/">Halucinace</a>: LLM mohou nÄ›kdy generovat nepravdivÃ© nebo zavÃ¡dÄ›jÃ­cÃ­ informace, zejmÃ©na kdyÅ¾ jsou dotÃ¡zÃ¡ny na nÄ›co, co je mimo jejich trÃ©ninkovÃ½ dataset. To je ÄÃ¡steÄnÄ› zpÅ¯sobeno tÃ­m, Å¾e model je trÃ©novÃ¡n pouze na generovÃ¡nÃ­ pravdÄ›podobnÃ©ho textu, ne nutnÄ› fakticky sprÃ¡vnÃ©ho textu. <a href="/ai/halucinace-ai/">PodrobnÄ›ji probÃ­rÃ¡me zde</a>.</p>

<p>b) KontextovÃ© okno: OmezenÃ¡ velikost kontextovÃ©ho okna limituje schopnost modelu pracovat s velmi dlouhÃ½mi dokumenty nebo udrÅ¾et kontext v dlouhÃ½ch konverzacÃ­ch.</p>

<p>c) AktuÃ¡lnost informacÃ­: LLM jsou omezeny na informace, na kterÃ½ch byly natrÃ©novÃ¡ny, a nemohou pÅ™Ã­mo pÅ™istupovat k aktuÃ¡lnÃ­m informacÃ­m (pokud nejsou kombinovÃ¡ny s RAG).</p>

<p>d) EtickÃ© otÃ¡zky: PouÅ¾itÃ­ LLM vznÃ¡Å¡Ã­ Å™adu etickÃ½ch otÃ¡zek, vÄetnÄ› potenciÃ¡lnÃ­ch pÅ™edsudkÅ¯, vlivu na soukromÃ­ a autorskÃ½ch prÃ¡v.</p>

<h2 id="zÃ¡vÄ›r">ZÃ¡vÄ›r</h2>

<p>FungovÃ¡nÃ­ LLM po zadÃ¡nÃ­ promptu je proces, kterÃ½ kombinuje hlubokÃ© strojovÃ© porozumÄ›nÃ­ jazyku s pokroÄilÃ½mi algoritmy <a href="/ai/strojove-uceni-machine-learning/">strojovÃ©ho uÄenÃ­</a>. Je podle mne velmi zajÃ­mavÃ©, jak je to na jednu stranu odliÅ¡nÃ© od lidskÃ©ho postupu uvaÅ¾ovÃ¡nÃ­ (to je ta ÄÃ¡st tokenizace, tedy pÅ™evodu jazyka na ÄÃ­sla) a na stranu druhou je dalÅ¡Ã­ postup uÅ¾ velmi podobnÃ½ lidskÃ©mu pÅ™emÃ½Å¡lenÃ­ a uÄenÃ­.</p>

<p>ZÃ¡roveÅˆ je dÅ¯leÅ¾itÃ© si uvÄ›domit, Å¾e LLM stÃ¡le majÃ­ svÃ¡ omezenÃ­, zejmÃ©na v oblasti <a href="/ai/halucinace-ai/">halucinacÃ­</a>. Proto je kritickÃ© myÅ¡lenÃ­ a ovÄ›Å™ovÃ¡nÃ­ informacÃ­ stÃ¡le nezbytnÃ©.</p>

<p>S pokraÄujÃ­cÃ­m vÃ½zkumem a vÃ½vojem v oblasti AI mÅ¯Å¾eme oÄekÃ¡vat dalÅ¡Ã­ vylepÅ¡enÃ­ tÄ›chto systÃ©mÅ¯. BudoucÃ­ vÃ½voj se pravdÄ›podobnÄ› zamÄ›Å™Ã­ na pÅ™ekonÃ¡vÃ¡nÃ­ souÄasnÃ½ch omezenÃ­, jako je velikost kontextovÃ©ho okna, aktuÃ¡lnost informacÃ­ a problÃ©m <a href="/ai/halucinace-ai/">halucinacÃ­</a>. ZÃ¡roveÅˆ mÅ¯Å¾eme oÄekÃ¡vat pokrok v oblasti multimodÃ¡lnÃ­ch modelÅ¯, kterÃ© budou schopny zpracovÃ¡vat nejen text, ale i obrÃ¡zky, zvuky a videa v jednotnÃ©m rÃ¡mci.</p>

  </div>

  <div class="ai-rubric-link">
    
    <!-- PÅ™idÃ¡nÃ­ tabulky s nÃ¡hodnÄ› vybranÃ½mi obrazy pouze pro ÄlÃ¡nky z kolekce Obrazy -->
    
  </div>

      <!-- MÃ­sto pro widget -->
  <div class="posts">
    <h3>Jak se vÃ¡m lÃ­bÃ­ tento ÄlÃ¡nek?</h3>

    <div id="feedback-widget"></div>

    <!-- Widget se vloÅ¾Ã­ do #feedback-widget -->
    <script 
        src="https://top.marigold.cz/mg-feedback.js" 
        data-slug="ai-llm" 
        data-title="Jak fungujÃ­ velkÃ© jazykovÃ© modely LLM - co se dÄ›je po zadÃ¡nÃ­ promptu" 
        data-url="https://www.marigold.cz/ai/llm/"
        data-target="#feedback-widget"
    ></script>
</div>

<!-- MÃ­sto pro widget nejÄtenÄ›jÅ¡Ã­ ÄlÃ¡nky -->
<script src="//widgets.clicky.com/poppy/?site_id=101451859&sitekey=5e093ea9431d06c7b572df6b51eda89f&width=500&height=500&date=last-28-days&type=pages&limit=10&title=Nej%C4%8Dten%C4%9Bj%C5%A1%C3%AD%20%C4%8Dl%C3%A1nky&hide_title=0&hide_branding=1" type="text/javascript"></script>
<!-- Konec widget nejÄtenÄ›jÅ¡Ã­ ÄlÃ¡nky -->

  
  <div class="commentbox"></div>
  <script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script>
  <script>commentBox('5677112761516032-proj')</script>
  

  <!-- Tady zaÄÃ­nÃ¡ odkazovÃ¡nÃ­ na featured ÄlÃ¡nky -->
  
  
    
    <div class="featured-posts">
      <h3>ğŸ’¡ Co je tu dalÅ¡Ã­ho zajÃ­mavÃ©ho ke ÄtenÃ­?</h3>
      <table>
        <tbody>
          
            <tr>
              <td>
                <a href="/item/ma-nova-kniha-o-digitalni-novinarine/">ğŸ‘‰MÃ¡ novÃ¡ kniha o digitÃ¡lnÃ­ novinaÅ™inÄ›</a>
                <p class="excerpt">
                  
                    Vydal jsem novou knihu vÄ›novanou zajÃ­mavÃ©mu vÃ½seku internetu a to digitÃ¡lnÃ­ obÄanskÃ© publicistice. Jmenuje se  ZlatÃ¡ Ã©ra weblogÅ¯ a mÅ¯Å¾ete si ji stÃ¡hnout zdar...
                  
                </p>
              </td>
            </tr>
          
            <tr>
              <td>
                <a href="/item/robinhood_wallstreet_pribeh_akcii/">ğŸ‘‰PÅ™Ã­bÄ›h WallStreetBets jako RobinaHooda, kterÃ½ bohatÃ½m akcionÃ¡Å™Å¯m bralâ€¦</a>
                <p class="excerpt">
                  
                    Tenhle pÅ™Ã­bÄ›h se stane epickou sÃ¡gou o mnoha rozmÄ›rech. Å½e nÄ›kdo prostÅ™ednictvÃ­m mobilnÃ­ aplikace zbohatne, to se stÃ¡vÃ¡. VÃ½jimeÄnÄ› i v miliardovÃ½ch ÄÃ¡stkÃ¡ch....
                  
                </p>
              </td>
            </tr>
          
        </tbody>
      </table>
    </div>
    



  <div class="posts">
    <h3>Chcete tyto ÄlÃ¡nky emailem?</h3>
    <iframe src="https://zandl.substack.com/embed" width="480" height="150" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no"></iframe>
  </div>

  <div>
    <p><span class="share-box">SdÃ­lejte ÄlÃ¡nek:</span> <a href="http://twitter.com/share?text=Jak fungujÃ­ velkÃ© jazykovÃ© modely LLM - co se dÄ›je po zadÃ¡nÃ­ promptu&url=https://www.marigold.cz/ai/llm/" target="_blank">Twitter</a>, <a href="https://www.facebook.com/sharer.php?u=https://www.marigold.cz/ai/llm/" target="_blank">Facebook</a>, 

    
      <a href="https://github.com/tangero/marigold-page/blob/main/_ai/2024-07-04-llm.md" target="_blank">
        Opravit ğŸ“ƒ
      </a>
    
</p>
    <p>
    <div class="PageNavigation">
      
      
      
        <a class="next" href="/ai/tokenizace-textu/">Tokenizace textu &raquo;</a>
      
    </div>
    </p>
  </div>
</article>

<!-- Toast notifikace -->
<div class="toast" id="toast">ZkopÃ­rovÃ¡no do schrÃ¡nky!</div>

<!-- JavaScript pro funkcionalitu kopÃ­rovÃ¡nÃ­ -->
<script>
document.addEventListener('DOMContentLoaded', function() {
    // Najdi vÅ¡echny code bloky (kromÄ› Mermaid)
    const codeBlocks = document.querySelectorAll('pre:not(:has(code.language-mermaid))');
    
    codeBlocks.forEach((codeBlock, index) => {
        // VytvoÅ™ kontejner pro code block
        const container = document.createElement('div');
        container.className = 'code-block-container';
        
        // VytvoÅ™ tlaÄÃ­tko kopÃ­rovat
        const copyButton = document.createElement('button');
        copyButton.className = 'copy-button';
        copyButton.textContent = 'KopÃ­rovat';
        copyButton.setAttribute('data-index', index);
        
        // VloÅ¾ code block a tlaÄÃ­tko do kontejneru
        codeBlock.parentNode.insertBefore(container, codeBlock);
        container.appendChild(copyButton);
        container.appendChild(codeBlock);
        
        // PÅ™idej event listener na tlaÄÃ­tko
        copyButton.addEventListener('click', async () => {
            try {
                // ZÃ­skej text z code bloku
                const code = codeBlock.querySelector('code') || codeBlock;
                const text = code.textContent;
                
                // KopÃ­ruj do schrÃ¡nky
                await navigator.clipboard.writeText(text);
                
                // ZmÄ›Åˆ stav tlaÄÃ­tka
                copyButton.textContent = 'ZkopÃ­rovÃ¡no!';
                copyButton.classList.add('copied');
                
                // Zobraz toast notifikaci
                showToast();
                
                // Po 2 sekundÃ¡ch vraÅ¥ pÅ¯vodnÃ­ stav
                setTimeout(() => {
                    copyButton.textContent = 'KopÃ­rovat';
                    copyButton.classList.remove('copied');
                }, 2000);
                
            } catch (err) {
                console.error('Chyba pÅ™i kopÃ­rovÃ¡nÃ­:', err);
                copyButton.textContent = 'Chyba';
                copyButton.classList.add('copied');
                
                setTimeout(() => {
                    copyButton.textContent = 'KopÃ­rovat';
                    copyButton.classList.remove('copied');
                }, 2000);
            }
        });
    });
});

function showToast() {
    const toast = document.getElementById('toast');
    toast.style.display = 'block';
    setTimeout(() => {
        toast.style.display = 'none';
    }, 2000);
}
</script>
        </div>
      </div>

      <div class="wrapper-footer">
        <div class="container">
          <footer class="footer">
            
<a href="mailto:patrick.zandl@marigold.cz"><i class="svg-icon email"></i></a>
<a href="https://www.facebook.com/patrick.zandl"><i class="svg-icon facebook"></i></a>



<a href="https://www.linkedin.com/in/patrickzandl"><i class="svg-icon linkedin"></i></a>

<a href="/feed.xml"><i class="svg-icon rss"></i></a>
<a href="https://www.twitter.com/tangero"><i class="svg-icon twitter"></i></a>





          </footer>
        </div>
      </div>
    </div>

    <a title="GDPR-compliant Web Analytics" href="https://clicky.com/101451859"><img alt="Clicky" src="//static.getclicky.com/media/links/badge.gif" border="0" /></a>
<script async data-id="101451859" src="//static.getclicky.com/js"></script>
<noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101451859ns.gif" /></p></noscript>
 | <a href="https://github.com/tangero/marigold-page"><img src="https://img.shields.io/github/last-commit/tangero/marigold-page"></a> | <a href="https://www.kronium.eu">flashlights, headlamps Fenix & outdoor</a> | <a href="https://www.vybavenidoprirody.com/">VybavenÃ­ do pÅ™Ã­rody</a>
<!-- 100% privacy-first analytics -->
<script async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
<!-- Twitter conversion tracking base code -->
<script>
    !function(e,t,n,s,u,a){e.twq||(s=e.twq=function(){s.exe?s.exe.apply(s,arguments):s.queue.push(arguments);
    },s.version='1.1',s.queue=[],u=t.createElement(n),u.async=!0,u.src='https://static.ads-twitter.com/uwt.js',
    a=t.getElementsByTagName(n)[0],a.parentNode.insertBefore(u,a))}(window,document,'script');
    twq('config','pycs2');
</script>
    <!-- End Twitter conversion tracking base code -->


    <script>
    function toggleDetails(button) {
      const content = button.nextElementSibling;
      const isCollapsed = button.classList.contains('collapsed');
      
      if (isCollapsed) {
        button.classList.remove('collapsed');
        content.classList.add('show');
      } else {
        button.classList.add('collapsed');
        content.classList.remove('show');
      }
    }
    </script>
  </body>
</html>
