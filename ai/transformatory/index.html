<!DOCTYPE html>
<html>
  <head>
    <title>Technologie TransformÃ¡torÅ¯ - ani hraÄka, ani elektrosouÄÃ¡stka, ale neuronovÃ¡ sÃ­Å¥ | Marigold.cz - SÃ­tÄ› a Technologie</title>
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Technologie TransformÃ¡torÅ¯ - ani hraÄka, ani elektrosouÄÃ¡stka, ale neuronovÃ¡ sÃ­Å¥" />
<meta name="author" content="Patrick Zandl" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="ZÃ¡kladem LLM je architektura TransformÃ¡torÅ¯, pÅ™edstavenÃ¡ v roce 2017 v prÅ¯lomovÃ© prÃ¡ci â€œAttention Is All You Needâ€. Tato architektura pÅ™inesla nÄ›kolik klÃ­ÄovÃ½ch inovacÃ­, kterÃ© umoÅ¾nily zpracovÃ¡nÃ­ dlouhÃ½ch sekvencÃ­ textu s velkou efektivitou a pÅ™esnostÃ­. Technologie transformÃ¡torÅ¯ dnes stojÃ­ v zÃ¡kladech LLM jako jsou GPT - kde to pÃ­smeno T jsou prÃ¡vÄ› TransformÃ¡tory." />
<meta property="og:description" content="ZÃ¡kladem LLM je architektura TransformÃ¡torÅ¯, pÅ™edstavenÃ¡ v roce 2017 v prÅ¯lomovÃ© prÃ¡ci â€œAttention Is All You Needâ€. Tato architektura pÅ™inesla nÄ›kolik klÃ­ÄovÃ½ch inovacÃ­, kterÃ© umoÅ¾nily zpracovÃ¡nÃ­ dlouhÃ½ch sekvencÃ­ textu s velkou efektivitou a pÅ™esnostÃ­. Technologie transformÃ¡torÅ¯ dnes stojÃ­ v zÃ¡kladech LLM jako jsou GPT - kde to pÃ­smeno T jsou prÃ¡vÄ› TransformÃ¡tory." />
<link rel="canonical" href="https://www.marigold.cz/ai/transformatory/" />
<meta property="og:url" content="https://www.marigold.cz/ai/transformatory/" />
<meta property="og:site_name" content="Marigold.cz" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-07-05T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Technologie TransformÃ¡torÅ¯ - ani hraÄka, ani elektrosouÄÃ¡stka, ale neuronovÃ¡ sÃ­Å¥" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Patrick Zandl"},"dateModified":"2024-07-05T00:00:00+00:00","datePublished":"2024-07-05T00:00:00+00:00","description":"ZÃ¡kladem LLM je architektura TransformÃ¡torÅ¯, pÅ™edstavenÃ¡ v roce 2017 v prÅ¯lomovÃ© prÃ¡ci â€œAttention Is All You Needâ€. Tato architektura pÅ™inesla nÄ›kolik klÃ­ÄovÃ½ch inovacÃ­, kterÃ© umoÅ¾nily zpracovÃ¡nÃ­ dlouhÃ½ch sekvencÃ­ textu s velkou efektivitou a pÅ™esnostÃ­. Technologie transformÃ¡torÅ¯ dnes stojÃ­ v zÃ¡kladech LLM jako jsou GPT - kde to pÃ­smeno T jsou prÃ¡vÄ› TransformÃ¡tory.","headline":"Technologie TransformÃ¡torÅ¯ - ani hraÄka, ani elektrosouÄÃ¡stka, ale neuronovÃ¡ sÃ­Å¥","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.marigold.cz/ai/transformatory/"},"url":"https://www.marigold.cz/ai/transformatory/"}</script>
<!-- End Jekyll SEO tag -->

        <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>
    <link href="https://fed.brid.gy/" rel="alternate" type="application/activity+json">

    
    <meta property="og:description" content="ZÃ¡kladem LLM je architektura TransformÃ¡torÅ¯, pÅ™edstavenÃ¡ v roce 2017 v prÅ¯lomovÃ© prÃ¡ci â€œAttention Is All You Needâ€. Tato architektura pÅ™inesla nÄ›kolik klÃ­ÄovÃ½ch inovacÃ­, kterÃ© umoÅ¾nily zpracovÃ¡nÃ­ dlouhÃ½ch sekvencÃ­ textu s velkou efektivitou a pÅ™esnostÃ­. Technologie transformÃ¡torÅ¯ dnes stojÃ­ v zÃ¡kladech LLM jako jsou GPT - kde to pÃ­smeno T jsou prÃ¡vÄ› TransformÃ¡tory.
" />
    
    <meta name="author" content="Marigold.cz" />

    
    <meta property="og:title" content="Technologie TransformÃ¡torÅ¯ - ani hraÄka, ani elektrosouÄÃ¡stka, ale neuronovÃ¡ sÃ­Å¥" />
    <meta property="twitter:title" content="Technologie TransformÃ¡torÅ¯ - ani hraÄka, ani elektrosouÄÃ¡stka, ale neuronovÃ¡ sÃ­Å¥" />
    

    
    <!-- else -->    
    <meta property="og:image" content="https://www.marigold.cz/images/patrick-mensi.jpg"/>
    <meta property="twitter:image" content="https://www.marigold.cz/images/patrick-mensi.jpg"/>
    

    <meta property="og:site_name" content="Marigold.cz | Technologie a SpoleÄnost"/>

    


    <link rel="stylesheet" type="text/css" href="//assets/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Marigold.cz - Technologie a SvÄ›t" href="//feed.xml" />
    <link rel="canonical" href="https://www.marigold.cz/ai/transformatory/" />

    <meta name="theme-color" content="#000000">

    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="manifest" href="/images/site.webmanifest">
    <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="/images/favicon.ico">
    <meta name="msapplication-TileColor" content="#2d89ef">
    <meta name="msapplication-config" content="/images/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
    <script type="text/javascript">
      window.heapReadyCb=window.heapReadyCb||[],window.heap=window.heap||[],heap.load=function(e,t){window.heap.envId=e,window.heap.clientConfig=t=t||{},window.heap.clientConfig.shouldFetchServerConfig=!1;var a=document.createElement("script");a.type="text/javascript",a.async=!0,a.src="https://cdn.us.heap-api.com/config/"+e+"/heap_config.js";var r=document.getElementsByTagName("script")[0];r.parentNode.insertBefore(a,r);var n=["init","startTracking","stopTracking","track","resetIdentity","identify","getSessionId","getUserId","getIdentity","addUserProperties","addEventProperties","removeEventProperty","clearEventProperties","addAccountProperties","addAdapter","addTransformer","addTransformerFn","onReady","addPageviewProperties","removePageviewProperty","clearPageviewProperties","trackPageview"],i=function(e){return function(){var t=Array.prototype.slice.call(arguments,0);window.heapReadyCb.push({name:e,fn:function(){heap[e]&&heap[e].apply(heap,t)}})}};for(var p=0;p<n.length;p++)heap[n[p]]=i(n[p])};
      heap.load("2219710997");
  </script>
  </head>

  <body>
    <div id="bar"></div>
    <div class="wrapper-container">
      <div class="wrapper-masthead">
        <div class="container">
          <header class="masthead clearfix">
            <a href="//" class="site-avatar"><img src="//images/patrick-mensi.jpg" alt="" /></a>

            <div class="site-info">
              <h1 class="site-name"><a href="//">Marigold.cz</a></h1>
              <p class="site-description">Technologie a SvÄ›t</p>

            </div>

            <nav>
              <a href="/search">ğŸ” Search</a> |              
              <a href="https://www.aivefirmach.cz">Workshop AI</a> |
              <a href="/mobilnisite">ğŸ—¼ 4G/5G sÃ­tÄ›</a> | 
              <a href="/ai">ğŸ¤– AI</a> | 
              <a href="/obrazy">ğŸ–¼ï¸ Art</a>
            </nav>
          </header>
        </div>
      </div>

      <div class="wrapper-main">
        <div id="main" role="main" class="container">
          <!-- start Mermaid run code --> 
<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.0.2/+esm'
  mermaid.initialize({startOnLoad:true,theme:'neutral'})
  await mermaid.run({querySelector:'code.language-mermaid'})
</script>
<!-- end fMermaid run code --> 

<!-- start feedwind code --> 
<script type="text/javascript" src="https://feed.mikle.com/js/fw-loader.js" preloader-text="Nahr%C3%A1v%C3%A1m" data-fw-param="168257/"></script>
<style>
  code.language-mermaid {
    display: flex;
    justify-content: center;
  }
  pre:has(code.language-mermaid), code.language-mermaid {
    background-color: transparent;
  }
  .edgeLabel {
    font-size: 92%;
    opacity: .95;
    color: #111;
    padding: 0 3px;
  }
  .node rect {
    stroke: #214f78 !important;
  }
  .nodeLabel {
    color: #214f78 !important;
  }
  </style>
<!-- end feedwind code -->

<article class="post detailed">
  <h1>Technologie TransformÃ¡torÅ¯ - ani hraÄka, ani elektrosouÄÃ¡stka, ale neuronovÃ¡ sÃ­Å¥</h1>

  

  
  <!-- Zde se zobrazÃ­ obsah pro vÅ¡echny ostatnÃ­ kolekce neÅ¾ 'obrazy' -->
  <div>
    <p class="author_title">Patrick Zandl  Â·

5.
Äervenec
  
2024 
    
    </p>

    
    <div class="post-tags">
      
      
    </div>
   
  </div>

  


 



  <div class="entry">
    <p>ZÃ¡kladem LLM je architektura TransformÃ¡torÅ¯, pÅ™edstavenÃ¡ v roce 2017 v prÅ¯lomovÃ© prÃ¡ci <a href="https://arxiv.org/abs/1706.03762">â€œAttention Is All You Needâ€</a>. Tato architektura pÅ™inesla nÄ›kolik klÃ­ÄovÃ½ch inovacÃ­, kterÃ© umoÅ¾nily zpracovÃ¡nÃ­ dlouhÃ½ch sekvencÃ­ textu s velkou efektivitou a pÅ™esnostÃ­. Technologie transformÃ¡torÅ¯ dnes stojÃ­ v zÃ¡kladech LLM jako jsou GPT - kde to pÃ­smeno T jsou prÃ¡vÄ› TransformÃ¡tory.</p>

<p>JÃ¡drem transformÃ¡torovÃ© architektury je mechanismus pozornosti <em>(attention mechanism)</em>. Tento mechanismus umoÅ¾Åˆuje modelu dynamicky â€œzamÄ›Å™it seâ€ na rÅ¯znÃ© ÄÃ¡sti vstupnÃ­ho textu pÅ™i generovÃ¡nÃ­ kaÅ¾dÃ©ho vÃ½stupnÃ­ho tokenu. To je zÃ¡sadnÃ­ vylepÅ¡enÃ­ oproti pÅ™edchozÃ­m rekurentnÃ­m neuronovÃ½m sÃ­tÃ­m (RNN), kterÃ© zpracovÃ¡valy text sekvenÄnÄ›. Mechanismus pozornosti umoÅ¾Åˆuje paralelnÃ­ zpracovÃ¡nÃ­, coÅ¾ vÃ½znamnÄ› zrychluje trÃ©nink i inferenci.</p>

<p>KonkrÃ©tnÄ›, TransformÃ¡tory pouÅ¾Ã­vÃ¡ tzv. â€œself-attentionâ€, kde kaÅ¾dÃ½ token v sekvenci interaguje se vÅ¡emi ostatnÃ­mi tokeny. To se dÄ›je pomocÃ­ tÅ™Ã­ vektorÅ¯ pro kaÅ¾dÃ½ token: dotaz (query), klÃ­Ä (key) a hodnota (value). Tyto vektory jsou lineÃ¡rnÃ­mi transformacemi vstupnÃ­ho embeddigu tokenu. Pozornost se poÄÃ­tÃ¡ jako vÃ¡Å¾enÃ½ souÄet hodnot, kde vÃ¡hy jsou urÄeny skalÃ¡rnÃ­m souÄinem dotazu s klÃ­Äi.</p>

<p>LLM typicky pouÅ¾Ã­vajÃ­ tzv. <em>multi-head attention</em>, kde se nÄ›kolik mechanismÅ¯ pozornosti aplikuje paralelnÄ›. To umoÅ¾Åˆuje modelu zachytit rÅ¯znÃ© typy vztahÅ¯ mezi tokeny souÄasnÄ›.</p>

<p>DalÅ¡Ã­ klÃ­Äovou souÄÃ¡stÃ­ architektury jsou feed-forward neuronovÃ© sÃ­tÄ›. Ty se aplikujÃ­ na vÃ½stup z attention vrstev a umoÅ¾ÅˆujÃ­ modelu provÃ¡dÄ›t nelineÃ¡rnÃ­ transformace reprezentacÃ­. Typicky se sklÃ¡dajÃ­ ze dvou lineÃ¡rnÃ­ch transformacÃ­ s aktivaÄnÃ­ funkcÃ­ ReLU mezi nimi.</p>

<p>ModernÃ­ LLM jako GPT (Generative Pre-trained Transformer) pouÅ¾Ã­vajÃ­ pouze dekodÃ©rovou ÄÃ¡st pÅ¯vodnÃ­ architektury transformÃ¡torÅ¯. To znamenÃ¡, Å¾e model generuje text autoregresivnÄ›, tedy token po tokenu, pÅ™iÄemÅ¾ kaÅ¾dÃ½ novÃ½ token je generovÃ¡n na zÃ¡kladÄ› vÅ¡ech pÅ™edchozÃ­ch tokenÅ¯.</p>

<p>DÅ¯leÅ¾itou souÄÃ¡stÃ­ architektury jsou takÃ© reziduÃ¡lnÃ­ spojenÃ­ a normalizaÄnÃ­ vrstvy. ReziduÃ¡lnÃ­ spojenÃ­ umoÅ¾ÅˆujÃ­ efektivnÃ­ trÃ©nink velmi hlubokÃ½ch sÃ­tÃ­ tÃ­m, Å¾e poskytujÃ­ pÅ™Ã­mou cestu pro zpÄ›tnou propagaci gradientÅ¯. NormalizaÄnÃ­ vrstvy pak stabilizujÃ­ aktivace v sÃ­ti, coÅ¾ opÄ›t usnadÅˆuje trÃ©nink.</p>

<h2 id="trÃ©nink-llm">TrÃ©nink LLM</h2>

<p>TrÃ©nink LLM probÃ­hÃ¡ na masivnÃ­ch datovÃ½ch sadÃ¡ch, Äasto obsahujÃ­cÃ­ch stovky miliard tokenÅ¯. CÃ­lem trÃ©ninku je minimalizovat tzv. cross-entropy loss, coÅ¾ v praxi znamenÃ¡ maximalizovat pravdÄ›podobnost, Å¾e model sprÃ¡vnÄ› pÅ™edpovÃ­ dalÅ¡Ã­ token v sekvenci.</p>

<p>KlÃ­Äovou inovacÃ­ v trÃ©ninku LLM je pouÅ¾itÃ­ tzv. kauzÃ¡lnÃ­ masek v mechanismu pozornosti. Tyto masky zajiÅ¡Å¥ujÃ­, Å¾e model pÅ™i predikci dalÅ¡Ã­ho tokenu mÅ¯Å¾e pÅ™istupovat pouze k pÅ™edchozÃ­m tokenÅ¯m, nikoli k budoucÃ­m. To umoÅ¾Åˆuje efektivnÃ­ paralelnÃ­ trÃ©nink, zatÃ­mco se zachovÃ¡vÃ¡ autoregresivnÃ­ povaha modelu.</p>

<p>Velikost modernÃ­ch LLM je ohromujÃ­cÃ­. NapÅ™Ã­klad GPT-3 mÃ¡ 175 miliard parametrÅ¯. To znamenÃ¡, Å¾e model musÃ­ bÄ›hem trÃ©ninku optimalizovat 175 miliard ÄÃ­sel. TakovÃ¡ velikost pÅ™inÃ¡Å¡Ã­ vÃ½zvy v oblasti efektivnÃ­ho trÃ©ninku a inference.</p>

<p>Pro efektivnÃ­ trÃ©nink tak velkÃ½ch modelÅ¯ se pouÅ¾Ã­vajÃ­ techniky jako model parallelism a pipeline parallelism. Model parallelism rozdÄ›luje parametry modelu mezi vÃ­ce GPU, zatÃ­mco pipeline parallelism rozdÄ›luje vrstvy modelu mezi GPU. Tyto techniky umoÅ¾ÅˆujÃ­ trÃ©novat modely, kterÃ© by se jinak neveÅ¡ly do pamÄ›ti jedinÃ©ho GPU.</p>

<p>DalÅ¡Ã­m dÅ¯leÅ¾itÃ½m aspektem LLM je jejich schopnost few-shot learningu. To znamenÃ¡, Å¾e model dokÃ¡Å¾e adaptovat svÃ© chovÃ¡nÃ­ na novÃ© Ãºkoly s minimÃ¡lnÃ­m mnoÅ¾stvÃ­m pÅ™Ã­kladÅ¯. Tato schopnost emerguje s rostoucÃ­ velikostÃ­ modelu a je jednÃ­m z nejzajÃ­mavÄ›jÅ¡Ã­ch aspektÅ¯ souÄasnÃ½ch LLM.</p>

<p>Inference v LLM, tedy proces generovÃ¡nÃ­ textu, probÃ­hÃ¡ token po tokenu. V kaÅ¾dÃ©m kroku model generuje pravdÄ›podobnostnÃ­ distribuci pÅ™es celÃ½ slovnÃ­k moÅ¾nÃ½ch nÃ¡sledujÃ­cÃ­ch tokenÅ¯. Z tÃ©to distribuce se vybÃ­rÃ¡ dalÅ¡Ã­ token, Äasto s pouÅ¾itÃ­m technik jako top-k nebo nucleus sampling, kterÃ© umoÅ¾ÅˆujÃ­ kontrolovat mÃ­ru â€œkreativnostiâ€ generovanÃ©ho textu.</p>

<p>ZajÃ­mavÃ½m aspektem LLM je jejich schopnost provÃ¡dÄ›t tzv. in-context learning. To znamenÃ¡, Å¾e model dokÃ¡Å¾e adaptovat svÃ© chovÃ¡nÃ­ na zÃ¡kladÄ› kontextu poskytnutÃ©ho v promptu, aniÅ¾ by se mÄ›nily jeho parametry. Tato schopnost umoÅ¾Åˆuje pouÅ¾itÃ­ technik jako few-shot learning nebo chain-of-thought prompting, kterÃ© vÃ½znamnÄ› rozÅ¡iÅ™ujÃ­ moÅ¾nosti vyuÅ¾itÃ­ LLM.</p>

<p>NedÃ¡vnÃ© vÃ½zkumy takÃ© ukÃ¡zaly, Å¾e LLM vykazujÃ­ emergentnÃ­ schopnosti. To jsou schopnosti, kterÃ© nebyly explicitnÄ› natrÃ©novÃ¡ny, ale objevujÃ­ se s rostoucÃ­ velikostÃ­ modelu. Mezi tyto schopnosti patÅ™Ã­ napÅ™Ã­klad schopnost Å™eÅ¡it matematickÃ© Ãºlohy, programovat, nebo dokonce provÃ¡dÄ›t vÃ­ceÃºrovÅˆovÃ© uvaÅ¾ovÃ¡nÃ­.</p>

<p>Navzdory svÃ½m impozantnÃ­m schopnostem majÃ­ LLM nÄ›kolik vÃ½znamnÃ½ch omezenÃ­. JednÃ­m z nich je tendence k â€œhalucinacÃ­mâ€, tedy generovÃ¡nÃ­ informacÃ­, kterÃ© jsou vÄ›rohodnÃ©, ale fakticky nesprÃ¡vnÃ©. DalÅ¡Ã­m problÃ©mem je omezenÃ¡ velikost kontextovÃ©ho okna, coÅ¾ limituje schopnost modelu pracovat s velmi dlouhÃ½mi texty.</p>

<p>Pro adresovÃ¡nÃ­ tÄ›chto omezenÃ­ se vyvÃ­jejÃ­ rÅ¯znÃ© techniky. NapÅ™Ã­klad Retrieval-Augmented Generation (RAG) kombinuje LLM s vyhledÃ¡vÃ¡nÃ­m v externÃ­ databÃ¡zi znalostÃ­, coÅ¾ mÅ¯Å¾e zlepÅ¡it faktickou pÅ™esnost generovanÃ©ho textu. Techniky jako kNN-LM nebo Memorizing Transformers se snaÅ¾Ã­ rozÅ¡Ã­Å™it efektivnÃ­ kontext modelu.</p>

<p>ZÃ¡vÄ›rem lze Å™Ã­ci, Å¾e neuronovÃ© sÃ­tÄ› v LLM pÅ™edstavujÃ­ fascinujÃ­cÃ­ spojenÃ­ sofistikovanÃ½ch algoritmÅ¯, masivnÃ­ch datovÃ½ch sad a vÃ½konnÃ©ho hardwaru. Jejich schopnost zpracovÃ¡vat a generovat lidskÃ½ jazyk na Ãºrovni, kterÃ¡ se blÃ­Å¾Ã­ lidskÃ½m schopnostem, otevÃ­rÃ¡ novÃ© moÅ¾nosti v oblasti umÄ›lÃ© inteligence a zpracovÃ¡nÃ­ pÅ™irozenÃ©ho jazyka. ZÃ¡roveÅˆ vÅ¡ak pÅ™inÃ¡Å¡ejÃ­ novÃ© vÃ½zvy v oblasti etiky, bezpeÄnosti a interpretovatelnosti AI systÃ©mÅ¯. PokraÄujÃ­cÃ­ vÃ½zkum v tÃ©to oblasti slibuje dalÅ¡Ã­ pokroky a moÅ¾nÃ¡ i novÃ© paradigmata v tom, jak chÃ¡peme a interagujeme s umÄ›lou inteligencÃ­.</p>

  </div>

  <div class="ai-rubric-link">
    
    <!-- PÅ™idÃ¡nÃ­ tabulky s nÃ¡hodnÄ› vybranÃ½mi obrazy pouze pro ÄlÃ¡nky z kolekce Obrazy -->
    
  </div>

  
  <div class="commentbox"></div>
  <script src="https://unpkg.com/commentbox.io/dist/commentBox.min.js"></script>
  <script>commentBox('5677112761516032-proj')</script>
  

  <!-- Tady zaÄÃ­nÃ¡ odkazovÃ¡nÃ­ na featured ÄlÃ¡nky -->
  
  
    
    <div class="featured-posts">
      <h3>ğŸ’¡ Co je tu dalÅ¡Ã­ho zajÃ­mavÃ©ho ke ÄtenÃ­?</h3>
      <table>
        <tbody>
          
            <tr>
              <td>
                <a href="/item/jak-jsem-napsal-vlastni-zakon/">ğŸ‘‰Jak jsem napsal vlastnÃ­ zÃ¡kon</a>
                <p class="excerpt">
                  
                    VÃ­te co? Dneska vÃ¡m povÃ­m, jak jsem si napsal vlastnÃ­ zÃ¡kon a jak tenhle zÃ¡kon skonÄil ve SbÃ­rce zÃ¡konÅ¯. Na tenhle kus Å¾ivota jsem si vzpomnÄ›l ve stÅ™edu, kdy...
                  
                </p>
              </td>
            </tr>
          
            <tr>
              <td>
                <a href="/item/proc-je-bydleni-drahe-protoze-nezlevnilo-ze-ano/">ğŸ‘‰ProÄ je bydlenÃ­ drahÃ©? ProtoÅ¾e nezlevnilo, Å¾e anoâ€¦</a>
                <p class="excerpt">
                  
                    HodnÄ› se v poslednÃ­ dobÄ› mluvÃ­ o drahÃ½ch bytech a vÃ½stavbÄ›. Studiem na vysokÃ© Å¡kole Å¾ivota zjistÃ­te, Å¾e by staÄilo je zlevnit a bude po problÃ©mu. Ale pojÄme ...
                  
                </p>
              </td>
            </tr>
          
        </tbody>
      </table>
    </div>
    

  <div class="posts">
    <h3>Chcete tyto ÄlÃ¡nky emailem?</h3>
    <iframe src="https://zandl.substack.com/embed" width="480" height="150" style="border:1px solid #EEE; background:white;" frameborder="0" scrolling="no"></iframe>
  </div>

  <div>
    <p><span class="share-box">SdÃ­lejte ÄlÃ¡nek:</span> <a href="http://twitter.com/share?text=Technologie TransformÃ¡torÅ¯ - ani hraÄka, ani elektrosouÄÃ¡stka, ale neuronovÃ¡ sÃ­Å¥&url=https://www.marigold.cz/ai/transformatory/" target="_blank">Twitter</a>, <a href="https://www.facebook.com/sharer.php?u=https://www.marigold.cz/ai/transformatory/" target="_blank">Facebook</a>, 

    
      <a href="https://github.com/tangero/marigold-page/blob/main/_ai/2024-07-05-transformatory.md" target="_blank">
        Opravit ğŸ“ƒ
      </a>
    
</p>
    <p>
    <div class="PageNavigation">
      
        <a class="prev" href="/ai/tokenizace-textu/">&laquo; Tokenizace textu</a> |
      
      
      
        <a class="next" href="/ai/word2vec/">Word2vec aneb jak pÅ™evÃ©st slova na ÄÃ­sla &raquo;</a>
      
    </div>
    </p>
  </div>
</article>
        </div>
      </div>

      <div class="wrapper-footer">
        <div class="container">
          <footer class="footer">
            
<a href="mailto:patrick.zandl@marigold.cz"><i class="svg-icon email"></i></a>
<a href="https://www.facebook.com/patrick.zandl"><i class="svg-icon facebook"></i></a>



<a href="https://www.linkedin.com/in/patrickzandl"><i class="svg-icon linkedin"></i></a>

<a href="//feed.xml"><i class="svg-icon rss"></i></a>
<a href="https://www.twitter.com/tangero"><i class="svg-icon twitter"></i></a>





          </footer>
        </div>
      </div>
    </div>

    <a title="Web Analytics" href="https://clicky.com/101451859"><img alt="Clicky" src="//static.getclicky.com/media/links/badge.gif" border="0" /></a>
<script async data-id="101451859" src="//static.getclicky.com/js"></script>
<noscript><p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101451859ns.gif" /></p>
</noscript> | <a href="https://github.com/tangero/marigold-page"><img src="https://img.shields.io/github/last-commit/tangero/marigold-page"></a> | <a href="https://www.kronium.eu">flashlights, headlamps Fenix & outdoor</a> | <a href="https://www.vybavenidoprirody.com/">VybavenÃ­ do pÅ™Ã­rody</a>
<!-- 100% privacy-first analytics -->
<script async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
 
  </body>
</html>
